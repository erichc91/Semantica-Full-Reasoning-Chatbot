{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8ffc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ConceptNetJunkDetector:\n",
    "    \"\"\"\n",
    "    Automated system to detect and filter junk entries in ConceptNet data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, sample_size=100000):\n",
    "        \"\"\"\n",
    "        Initialize with ConceptNet dataframe\n",
    "        \n",
    "        Args:\n",
    "            df: ConceptNet dataframe with columns [relation_type, start_concept, end_concept, edge_weight]\n",
    "            sample_size: Size of sample for expensive operations\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.sample_size = min(sample_size, len(df))\n",
    "        self.junk_patterns = {}\n",
    "        self.quality_scores = {}\n",
    "        \n",
    "        print(f\"üîç Initializing Junk Detector with {len(df):,} triples\")\n",
    "        print(f\"   Working with sample size: {self.sample_size:,}\")\n",
    "    \n",
    "    def analyze_all(self, verbose=True):\n",
    "        \"\"\"Run all analysis methods and return comprehensive report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üöÄ RUNNING COMPREHENSIVE JUNK ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {\n",
    "            'length_analysis': self.analyze_concept_lengths(),\n",
    "            'character_analysis': self.analyze_character_patterns(),\n",
    "            'frequency_analysis': self.analyze_concept_frequency(),\n",
    "            'relation_analysis': self.analyze_relation_quality(),\n",
    "            'weight_analysis': self.analyze_edge_weights(),\n",
    "            'semantic_analysis': self.analyze_semantic_validity(),\n",
    "            'connectivity_analysis': self.analyze_connectivity_patterns()\n",
    "        }\n",
    "        \n",
    "        # Generate filtering recommendations\n",
    "        self.recommendations = self.generate_filtering_recommendations(results)\n",
    "        \n",
    "        if verbose:\n",
    "            self.print_analysis_report(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_concept_lengths(self):\n",
    "        \"\"\"Analyze the length distribution of concepts\"\"\"\n",
    "        print(\"\\nüìè Analyzing concept lengths...\")\n",
    "        \n",
    "        all_concepts = pd.concat([self.df['start_concept'], self.df['end_concept']])\n",
    "        concept_lengths = all_concepts.str.len()\n",
    "        \n",
    "        analysis = {\n",
    "            'single_char_concepts': list(all_concepts[concept_lengths == 1].value_counts().head(20).index),\n",
    "            'very_short_concepts': list(all_concepts[concept_lengths <= 2].value_counts().head(20).index),\n",
    "            'very_long_concepts': list(all_concepts[concept_lengths > 50].value_counts().head(10).index),\n",
    "            'length_distribution': {\n",
    "                '1_char': (concept_lengths == 1).sum(),\n",
    "                '2-3_chars': ((concept_lengths >= 2) & (concept_lengths <= 3)).sum(),\n",
    "                '4-10_chars': ((concept_lengths >= 4) & (concept_lengths <= 10)).sum(),\n",
    "                '11-20_chars': ((concept_lengths >= 11) & (concept_lengths <= 20)).sum(),\n",
    "                '21-50_chars': ((concept_lengths >= 21) & (concept_lengths <= 50)).sum(),\n",
    "                '>50_chars': (concept_lengths > 50).sum()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Flag suspicious patterns\n",
    "        analysis['suspicious_single_chars'] = [c for c in analysis['single_char_concepts'] \n",
    "                                               if not c.isalpha() or c.lower() not in ['a', 'i']]\n",
    "        \n",
    "        print(f\"   Found {len(analysis['suspicious_single_chars'])} suspicious single-char concepts\")\n",
    "        print(f\"   Examples: {analysis['suspicious_single_chars'][:10]}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_character_patterns(self):\n",
    "        \"\"\"Detect non-standard character patterns\"\"\"\n",
    "        print(\"\\nüî§ Analyzing character patterns...\")\n",
    "        \n",
    "        # Sample for performance\n",
    "        sample = self.df.sample(n=self.sample_size, random_state=42)\n",
    "        all_concepts = pd.concat([sample['start_concept'], sample['end_concept']])\n",
    "        \n",
    "        patterns = {\n",
    "            'only_numbers': [],\n",
    "            'only_punctuation': [],\n",
    "            'mixed_alphanumeric': [],\n",
    "            'contains_underscore': [],\n",
    "            'contains_dots': [],\n",
    "            'non_ascii': [],\n",
    "            'looks_like_code': [],\n",
    "            'url_like': []\n",
    "        }\n",
    "        \n",
    "        for concept in tqdm(all_concepts.unique(), desc=\"Scanning concepts\"):\n",
    "            if re.match(r'^\\d+$', concept):\n",
    "                patterns['only_numbers'].append(concept)\n",
    "            elif re.match(r'^[^a-zA-Z0-9]+$', concept):\n",
    "                patterns['only_punctuation'].append(concept)\n",
    "            elif '_' in concept:\n",
    "                patterns['contains_underscore'].append(concept)\n",
    "            elif '.' in concept and not concept.replace('.', '').isalpha():\n",
    "                patterns['contains_dots'].append(concept)\n",
    "            elif re.search(r'[^\\x00-\\x7F]', concept):\n",
    "                patterns['non_ascii'].append(concept)\n",
    "            elif re.match(r'^[a-z0-9_]+\\(\\)$|^[A-Z_]+$|^\\$\\w+$', concept):\n",
    "                patterns['looks_like_code'].append(concept)\n",
    "            elif re.match(r'^(http|www|\\.com|\\.org)', concept):\n",
    "                patterns['url_like'].append(concept)\n",
    "        \n",
    "        # Limit lists and get counts\n",
    "        pattern_keys = list(patterns.keys())  # Create a copy of keys\n",
    "        for key in pattern_keys:\n",
    "            patterns[f'{key}_count'] = len(patterns[key])\n",
    "            patterns[key] = patterns[key][:20]  # Keep only examples\n",
    "        \n",
    "        print(f\"   Found {patterns['only_numbers_count']} pure number concepts\")\n",
    "        print(f\"   Found {patterns['looks_like_code_count']} code-like concepts\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def analyze_concept_frequency(self):\n",
    "        \"\"\"Analyze frequency distribution to find outliers\"\"\"\n",
    "        print(\"\\nüìä Analyzing concept frequency...\")\n",
    "        \n",
    "        all_concepts = pd.concat([self.df['start_concept'], self.df['end_concept']])\n",
    "        concept_freq = all_concepts.value_counts()\n",
    "        \n",
    "        analysis = {\n",
    "            'top_20_concepts': concept_freq.head(20).to_dict(),\n",
    "            'single_occurrence': (concept_freq == 1).sum(),\n",
    "            'rare_concepts': (concept_freq <= 3).sum(),\n",
    "            'frequency_stats': {\n",
    "                'mean': concept_freq.mean(),\n",
    "                'median': concept_freq.median(),\n",
    "                'std': concept_freq.std(),\n",
    "                'max': concept_freq.max()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Identify suspiciously frequent short concepts\n",
    "        short_frequent = []\n",
    "        for concept, freq in concept_freq.head(100).items():\n",
    "            if len(concept) <= 2 and freq > 1000:\n",
    "                short_frequent.append((concept, freq))\n",
    "        \n",
    "        analysis['suspicious_frequent'] = short_frequent[:20]\n",
    "        \n",
    "        print(f\"   Total unique concepts: {len(concept_freq):,}\")\n",
    "        print(f\"   Concepts appearing only once: {analysis['single_occurrence']:,}\")\n",
    "        print(f\"   Top concept: '{concept_freq.index[0]}' appears {concept_freq.iloc[0]:,} times\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_relation_quality(self):\n",
    "        \"\"\"Analyze relation types and their quality\"\"\"\n",
    "        print(\"\\nüîó Analyzing relation quality...\")\n",
    "        \n",
    "        relation_stats = self.df.groupby('relation_type').agg({\n",
    "            'edge_weight': ['mean', 'std', 'count']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Sample some triples for each relation to check validity\n",
    "        relation_examples = {}\n",
    "        suspicious_relations = []\n",
    "        \n",
    "        for relation in self.df['relation_type'].unique():\n",
    "            rel_df = self.df[self.df['relation_type'] == relation]\n",
    "            examples = rel_df.sample(n=min(5, len(rel_df))).values.tolist()\n",
    "            relation_examples[relation] = examples\n",
    "            \n",
    "            # Check for suspicious patterns\n",
    "            avg_weight = rel_df['edge_weight'].mean()\n",
    "            if avg_weight == 1.0 and len(rel_df) > 100:\n",
    "                suspicious_relations.append(relation)\n",
    "        \n",
    "        analysis = {\n",
    "            'relation_stats': relation_stats.to_dict(),\n",
    "            'total_relation_types': self.df['relation_type'].nunique(),\n",
    "            'suspicious_relations': suspicious_relations,\n",
    "            'relation_examples': relation_examples\n",
    "        }\n",
    "        \n",
    "        print(f\"   Found {len(suspicious_relations)} suspicious relation types\")\n",
    "        print(f\"   Total relation types: {analysis['total_relation_types']}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_edge_weights(self):\n",
    "        \"\"\"Analyze edge weight distribution\"\"\"\n",
    "        print(\"\\n‚öñÔ∏è Analyzing edge weights...\")\n",
    "        \n",
    "        weight_dist = self.df['edge_weight'].value_counts().sort_index()\n",
    "        \n",
    "        analysis = {\n",
    "            'weight_distribution': weight_dist.to_dict(),\n",
    "            'unique_weights': len(weight_dist),\n",
    "            'all_same_weight': len(weight_dist) == 1,\n",
    "            'mostly_default': (self.df['edge_weight'] == 1.0).sum() / len(self.df),\n",
    "            'weight_stats': {\n",
    "                'mean': self.df['edge_weight'].mean(),\n",
    "                'std': self.df['edge_weight'].std(),\n",
    "                'min': self.df['edge_weight'].min(),\n",
    "                'max': self.df['edge_weight'].max()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"   Unique weight values: {analysis['unique_weights']}\")\n",
    "        print(f\"   Percentage with weight=1.0: {analysis['mostly_default']*100:.1f}%\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_semantic_validity(self):\n",
    "        \"\"\"Check semantic validity of relations\"\"\"\n",
    "        print(\"\\nüß† Analyzing semantic validity...\")\n",
    "        \n",
    "        # Sample for performance\n",
    "        sample = self.df.sample(n=min(10000, len(self.df)), random_state=42)\n",
    "        \n",
    "        invalid_patterns = []\n",
    "        \n",
    "        # Check for nonsensical relations\n",
    "        for _, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Checking semantics\"):\n",
    "            start, rel, end = row['start_concept'], row['relation_type'], row['end_concept']\n",
    "            \n",
    "            # Single letter/number antonyms\n",
    "            if rel == 'Antonym' and (len(start) == 1 or len(end) == 1):\n",
    "                if not (start.isalpha() and end.isalpha()):\n",
    "                    invalid_patterns.append(('single_char_antonym', start, rel, end))\n",
    "            \n",
    "            # Numbers with non-numeric relations\n",
    "            elif start.isdigit() or end.isdigit():\n",
    "                if rel not in ['GreaterThan', 'LessThan', 'EqualTo', 'RelatedTo']:\n",
    "                    invalid_patterns.append(('number_invalid_relation', start, rel, end))\n",
    "            \n",
    "            # Same concept relations\n",
    "            elif start == end and rel not in ['RelatedTo', 'SimilarTo']:\n",
    "                invalid_patterns.append(('self_relation', start, rel, end))\n",
    "        \n",
    "        # Group by pattern type\n",
    "        pattern_counts = Counter([p[0] for p in invalid_patterns])\n",
    "        \n",
    "        analysis = {\n",
    "            'invalid_pattern_counts': dict(pattern_counts),\n",
    "            'invalid_examples': invalid_patterns[:50],\n",
    "            'estimated_invalid_percentage': len(invalid_patterns) / len(sample) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"   Estimated invalid triples: {analysis['estimated_invalid_percentage']:.1f}%\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def analyze_connectivity_patterns(self):\n",
    "        \"\"\"Analyze graph connectivity patterns to find isolated junk\"\"\"\n",
    "        print(\"\\nüï∏Ô∏è Analyzing connectivity patterns...\")\n",
    "        \n",
    "        # Find concepts that only appear with low-weight edges\n",
    "        concept_avg_weights = {}\n",
    "        \n",
    "        for concept in tqdm(pd.concat([self.df['start_concept'], self.df['end_concept']]).unique()[:10000], \n",
    "                           desc=\"Analyzing connectivity\"):\n",
    "            mask = (self.df['start_concept'] == concept) | (self.df['end_concept'] == concept)\n",
    "            if mask.any():\n",
    "                avg_weight = self.df[mask]['edge_weight'].mean()\n",
    "                connection_count = mask.sum()\n",
    "                concept_avg_weights[concept] = (avg_weight, connection_count)\n",
    "        \n",
    "        # Find poorly connected concepts\n",
    "        poorly_connected = [(c, w, n) for c, (w, n) in concept_avg_weights.items() \n",
    "                           if w < 0.3 or n == 1]\n",
    "        \n",
    "        analysis = {\n",
    "            'poorly_connected_concepts': poorly_connected[:50],\n",
    "            'single_connection_concepts': len([c for c, (w, n) in concept_avg_weights.items() if n == 1]),\n",
    "            'low_weight_concepts': len([c for c, (w, n) in concept_avg_weights.items() if w < 0.5])\n",
    "        }\n",
    "        \n",
    "        print(f\"   Concepts with single connection: {analysis['single_connection_concepts']:,}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_filtering_recommendations(self, results):\n",
    "        \"\"\"Generate specific filtering recommendations based on analysis\"\"\"\n",
    "        print(\"\\nüéØ Generating filtering recommendations...\")\n",
    "        \n",
    "        recommendations = {\n",
    "            'remove_concepts': set(),\n",
    "            'remove_relations': set(),\n",
    "            'remove_patterns': [],\n",
    "            'weight_threshold': 0.5,\n",
    "            'estimated_reduction': 0\n",
    "        }\n",
    "        \n",
    "        # Add junk concepts\n",
    "        recommendations['remove_concepts'].update(results['length_analysis']['suspicious_single_chars'])\n",
    "        recommendations['remove_concepts'].update([c for c, _ in results['frequency_analysis']['suspicious_frequent']])\n",
    "        recommendations['remove_concepts'].update(results['character_analysis']['only_numbers'][:50])\n",
    "        recommendations['remove_concepts'].update(results['character_analysis']['only_punctuation'])\n",
    "        \n",
    "        # Add patterns\n",
    "        if results['character_analysis']['only_numbers_count'] > 100:\n",
    "            recommendations['remove_patterns'].append(r'^\\d+$')  # Pure numbers\n",
    "        \n",
    "        if results['character_analysis']['looks_like_code_count'] > 50:\n",
    "            recommendations['remove_patterns'].append(r'^[a-z0-9_]+\\(\\)$')  # Function calls\n",
    "        \n",
    "        # Estimate reduction\n",
    "        mask = (self.df['start_concept'].isin(recommendations['remove_concepts']) | \n",
    "                self.df['end_concept'].isin(recommendations['remove_concepts']))\n",
    "        recommendations['estimated_reduction'] = mask.sum()\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def apply_smart_filter(self, aggressive=False):\n",
    "        \"\"\"Apply recommended filters and return cleaned dataframe\"\"\"\n",
    "        print(\"\\nüßπ Applying smart filters...\")\n",
    "        \n",
    "        original_size = len(self.df)\n",
    "        filtered_df = self.df.copy()\n",
    "        \n",
    "        # Remove identified junk concepts\n",
    "        junk_concepts = list(self.recommendations['remove_concepts'])\n",
    "        filtered_df = filtered_df[~filtered_df['start_concept'].isin(junk_concepts)]\n",
    "        filtered_df = filtered_df[~filtered_df['end_concept'].isin(junk_concepts)]\n",
    "        \n",
    "        print(f\"   Removed {original_size - len(filtered_df):,} triples with junk concepts\")\n",
    "        \n",
    "        # Apply regex patterns\n",
    "        for pattern in self.recommendations['remove_patterns']:\n",
    "            mask = (filtered_df['start_concept'].str.match(pattern) | \n",
    "                   filtered_df['end_concept'].str.match(pattern))\n",
    "            filtered_df = filtered_df[~mask]\n",
    "        \n",
    "        print(f\"   Removed {original_size - len(filtered_df):,} total triples after pattern matching\")\n",
    "        \n",
    "        if aggressive:\n",
    "            # Remove low-weight edges\n",
    "            filtered_df = filtered_df[filtered_df['edge_weight'] >= 0.5]\n",
    "            \n",
    "            # Remove single-character concepts except 'a' and 'i'\n",
    "            single_char_mask = ((filtered_df['start_concept'].str.len() == 1) | \n",
    "                               (filtered_df['end_concept'].str.len() == 1))\n",
    "            valid_single = ['a', 'i', 'A', 'I']\n",
    "            valid_mask = (filtered_df['start_concept'].isin(valid_single) | \n",
    "                         filtered_df['end_concept'].isin(valid_single))\n",
    "            filtered_df = filtered_df[~(single_char_mask & ~valid_mask)]\n",
    "            \n",
    "            # Remove very short concepts with numbers\n",
    "            short_with_numbers = (\n",
    "                ((filtered_df['start_concept'].str.len() <= 3) & \n",
    "                 filtered_df['start_concept'].str.contains(r'\\d', regex=True)) |\n",
    "                ((filtered_df['end_concept'].str.len() <= 3) & \n",
    "                 filtered_df['end_concept'].str.contains(r'\\d', regex=True))\n",
    "            )\n",
    "            filtered_df = filtered_df[~short_with_numbers]\n",
    "        \n",
    "        final_size = len(filtered_df)\n",
    "        print(f\"\\n‚úÖ Filtering complete!\")\n",
    "        print(f\"   Original size: {original_size:,}\")\n",
    "        print(f\"   Final size: {final_size:,}\")\n",
    "        print(f\"   Reduction: {(1 - final_size/original_size)*100:.1f}%\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def print_analysis_report(self, results):\n",
    "        \"\"\"Print a comprehensive analysis report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã JUNK DETECTION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüö® TOP JUNK INDICATORS:\")\n",
    "        print(f\"   ‚Ä¢ Single char concepts: {results['length_analysis']['length_distribution']['1_char']:,}\")\n",
    "        print(f\"   ‚Ä¢ Pure number concepts: {results['character_analysis']['only_numbers_count']:,}\")\n",
    "        print(f\"   ‚Ä¢ Code-like concepts: {results['character_analysis']['looks_like_code_count']:,}\")\n",
    "        print(f\"   ‚Ä¢ Invalid semantic patterns: ~{results['semantic_analysis']['estimated_invalid_percentage']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìä FILTERING RECOMMENDATIONS:\")\n",
    "        print(f\"   ‚Ä¢ Remove {len(self.recommendations['remove_concepts']):,} specific junk concepts\")\n",
    "        print(f\"   ‚Ä¢ Apply {len(self.recommendations['remove_patterns'])} regex filters\")\n",
    "        print(f\"   ‚Ä¢ Estimated reduction: {self.recommendations['estimated_reduction']:,} triples\")\n",
    "        \n",
    "        print(f\"\\nüí° SUGGESTED NEXT STEPS:\")\n",
    "        print(f\"   1. Run: cleaned_df = detector.apply_smart_filter()\")\n",
    "        print(f\"   2. For aggressive cleaning: cleaned_df = detector.apply_smart_filter(aggressive=True)\")\n",
    "        print(f\"   3. Test on small sample first: test_df = cleaned_df.head(50000)\")\n",
    "    \n",
    "    def visualize_analysis(self, results=None):\n",
    "        \"\"\"Create visualization of the analysis results\"\"\"\n",
    "        if results is None:\n",
    "            results = self.results if hasattr(self, 'results') else self.analyze_all(verbose=False)\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Concept length distribution\n",
    "        lengths = pd.concat([self.df['start_concept'].str.len(), \n",
    "                           self.df['end_concept'].str.len()])\n",
    "        axes[0,0].hist(lengths[lengths <= 20], bins=20, edgecolor='black')\n",
    "        axes[0,0].set_title('Concept Length Distribution (‚â§20 chars)')\n",
    "        axes[0,0].set_xlabel('Length')\n",
    "        axes[0,0].set_ylabel('Count')\n",
    "        \n",
    "        # 2. Edge weight distribution\n",
    "        self.df['edge_weight'].value_counts().sort_index().plot(kind='bar', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Edge Weight Distribution')\n",
    "        axes[0,1].set_xlabel('Weight')\n",
    "        axes[0,1].set_ylabel('Count')\n",
    "        \n",
    "        # 3. Top 20 relations\n",
    "        self.df['relation_type'].value_counts().head(20).plot(kind='barh', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Top 20 Relation Types')\n",
    "        axes[1,0].set_xlabel('Count')\n",
    "        \n",
    "        # 4. Junk categories pie chart\n",
    "        if hasattr(self, 'recommendations'):\n",
    "            junk_categories = {\n",
    "                'Single chars': results['length_analysis']['length_distribution']['1_char'],\n",
    "                'Pure numbers': results['character_analysis']['only_numbers_count'],\n",
    "                'Code-like': results['character_analysis']['looks_like_code_count'],\n",
    "                'Valid': len(self.df) - self.recommendations['estimated_reduction']\n",
    "            }\n",
    "            axes[1,1].pie(junk_categories.values(), labels=junk_categories.keys(), autopct='%1.1f%%')\n",
    "            axes[1,1].set_title('Data Quality Breakdown')\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'Run analyze_all() first', ha='center', va='center')\n",
    "            axes[1,1].set_title('Data Quality Breakdown')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def analyze_and_clean_conceptnet(df, visualize=True):\n",
    "    \"\"\"\n",
    "    One-stop function to analyze and clean ConceptNet data\n",
    "    \"\"\"\n",
    "    # Initialize detector\n",
    "    detector = ConceptNetJunkDetector(df)\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    results = detector.analyze_all()\n",
    "    \n",
    "    # Visualize if requested\n",
    "    if visualize:\n",
    "        detector.visualize_analysis(results)\n",
    "    \n",
    "    # Apply filtering\n",
    "    print(\"\\nü§ñ Applying recommended filters...\")\n",
    "    cleaned_df = detector.apply_smart_filter(aggressive=False)\n",
    "    \n",
    "    # Show before/after stats\n",
    "    print(f\"\\nüìä CLEANING SUMMARY:\")\n",
    "    print(f\"   Original unique concepts: {pd.concat([df['start_concept'], df['end_concept']]).nunique():,}\")\n",
    "    print(f\"   Cleaned unique concepts: {pd.concat([cleaned_df['start_concept'], cleaned_df['end_concept']]).nunique():,}\")\n",
    "    print(f\"   Original relations: {df['relation_type'].nunique()}\")\n",
    "    print(f\"   Cleaned relations: {cleaned_df['relation_type'].nunique()}\")\n",
    "    \n",
    "    return cleaned_df, detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2217681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "EN_PATH = '../Data/Input/conceptnet_en_full.csv'\n",
    "your_conceptnet_df = pd.read_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_cleaned.parquet.gzip')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcbf8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Initializing Junk Detector with 1,655,522 triples\n",
      "   Working with sample size: 100,000\n",
      "\n",
      "============================================================\n",
      "üöÄ RUNNING COMPREHENSIVE JUNK ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìè Analyzing concept lengths...\n",
      "   Found 18 suspicious single-char concepts\n",
      "   Examples: ['n', 'v', 'r', 'o', 't', 's', 'y', 'c', 'e', 'u']\n",
      "\n",
      "üî§ Analyzing character patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning concepts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99821/99821 [00:00<00:00, 641135.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 43 pure number concepts\n",
      "   Found 0 code-like concepts\n",
      "\n",
      "üìä Analyzing concept frequency...\n",
      "   Total unique concepts: 754,380\n",
      "   Concepts appearing only once: 426,993\n",
      "   Top concept: 'n' appears 560,635 times\n",
      "\n",
      "üîó Analyzing relation quality...\n",
      "   Found 0 suspicious relation types\n",
      "   Total relation types: 47\n",
      "\n",
      "‚öñÔ∏è Analyzing edge weights...\n",
      "   Unique weight values: 4912\n",
      "   Percentage with weight=1.0: 79.0%\n",
      "\n",
      "üß† Analyzing semantic validity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking semantics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 56991.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Estimated invalid triples: 2.5%\n",
      "\n",
      "üï∏Ô∏è Analyzing connectivity patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing connectivity:   2%|‚ñè         | 182/10000 [00:22<20:27,  8.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick analysis and cleaning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cleaned_df, detector = \u001b[43manalyze_and_clean_conceptnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43myour_conceptnet_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Or for more control:\u001b[39;00m\n\u001b[32m      5\u001b[39m detector = ConceptNetJunkDetector(your_conceptnet_df, sample_size=\u001b[32m100000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 448\u001b[39m, in \u001b[36manalyze_and_clean_conceptnet\u001b[39m\u001b[34m(df, visualize)\u001b[39m\n\u001b[32m    445\u001b[39m detector = ConceptNetJunkDetector(df)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Run comprehensive analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m results = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Visualize if requested\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mConceptNetJunkDetector.analyze_all\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ RUNNING COMPREHENSIVE JUNK ANALYSIS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     36\u001b[39m results = {\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlength_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_concept_lengths(),\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcharacter_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_character_patterns(),\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfrequency_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_concept_frequency(),\n\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelation_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_relation_quality(),\n\u001b[32m     41\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mweight_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_edge_weights(),\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msemantic_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.analyze_semantic_validity(),\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mconnectivity_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manalyze_connectivity_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m }\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Generate filtering recommendations\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.recommendations = \u001b[38;5;28mself\u001b[39m.generate_filtering_recommendations(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mConceptNetJunkDetector.analyze_connectivity_patterns\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m concept_avg_weights = {}\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m tqdm(pd.concat([\u001b[38;5;28mself\u001b[39m.df[\u001b[33m'\u001b[39m\u001b[33mstart_concept\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mself\u001b[39m.df[\u001b[33m'\u001b[39m\u001b[33mend_concept\u001b[39m\u001b[33m'\u001b[39m]]).unique()[:\u001b[32m10000\u001b[39m], \n\u001b[32m    269\u001b[39m                    desc=\u001b[33m\"\u001b[39m\u001b[33mAnalyzing connectivity\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     mask = (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstart_concept\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcept\u001b[49m) | (\u001b[38;5;28mself\u001b[39m.df[\u001b[33m'\u001b[39m\u001b[33mend_concept\u001b[39m\u001b[33m'\u001b[39m] == concept)\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m    272\u001b[39m         avg_weight = \u001b[38;5;28mself\u001b[39m.df[mask][\u001b[33m'\u001b[39m\u001b[33medge_weight\u001b[39m\u001b[33m'\u001b[39m].mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6116\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6117\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6119\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lvalues.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     res_values = \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:130\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    129\u001b[39m     result = libops.scalar_compare(x.ravel(), y, op)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Quick analysis and cleaning\n",
    "cleaned_df, detector = analyze_and_clean_conceptnet(your_conceptnet_df)\n",
    "\n",
    "# Or for more control:\n",
    "detector = ConceptNetJunkDetector(your_conceptnet_df, sample_size=100000)\n",
    "results = detector.analyze_all()\n",
    "\n",
    "# Review the recommendations\n",
    "print(detector.recommendations)\n",
    "\n",
    "# Apply conservative cleaning\n",
    "cleaned_df = detector.apply_smart_filter(aggressive=False)\n",
    "\n",
    "# Or aggressive cleaning\n",
    "cleaned_df = detector.apply_smart_filter(aggressive=True)\n",
    "\n",
    "# Test your training loop on a small cleaned sample first\n",
    "test_sample = cleaned_df.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d6c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
