{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0664277",
   "metadata": {},
   "source": [
    "# Semantic Knowledge Graph Exploration\n",
    "\n",
    "This notebook is dedicated to exploring and analyzing the trained semantic knowledge graph stored in `nx_semantic_final.graphml`. The graph has been enriched through hyper-training with semantic features, centrality measures, and relationship confidence scores.\n",
    "\n",
    "## Key Features to Explore:\n",
    "- Graph structure and statistics\n",
    "- Semantic relationships and patterns\n",
    "- Centrality analysis and influential concepts\n",
    "- Relationship types and their distributions\n",
    "- Concept clustering and communities\n",
    "- Advanced semantic queries and path finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17e2d",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcde0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained semantic graph\n",
    "graph_path = r\"c:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\Data\\Output\\nx_semantic_final.graphml\"\n",
    "\n",
    "print(\"Loading semantic knowledge graph...\")\n",
    "try:\n",
    "    G = nx.read_graphml(graph_path)\n",
    "    print(f\"‚úÖ Graph loaded successfully!\")\n",
    "    print(f\"üìä Nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"üìä Edges: {G.number_of_edges():,}\")\n",
    "    print(f\"üìä Graph type: {type(G).__name__}\")\n",
    "    print(f\"üìä Is directed: {G.is_directed()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading graph: {e}\")\n",
    "    G = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b7b92",
   "metadata": {},
   "source": [
    "## 2. Graph Overview and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph_structure(G):\n",
    "    \"\"\"Comprehensive analysis of graph structure\"\"\"\n",
    "    if G is None:\n",
    "        print(\"Graph not loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== GRAPH STRUCTURE ANALYSIS ===\")\n",
    "    print(f\"Nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"Edges: {G.number_of_edges():,}\")\n",
    "    print(f\"Density: {nx.density(G):.6f}\")\n",
    "    print(f\"Is Connected: {nx.is_connected(G) if not G.is_directed() else 'N/A (directed)'}\")\n",
    "    \n",
    "    if G.is_directed():\n",
    "        print(f\"Is Weakly Connected: {nx.is_weakly_connected(G)}\")\n",
    "        print(f\"Number of SCCs: {nx.number_strongly_connected_components(G)}\")\n",
    "        print(f\"Number of WCCs: {nx.number_weakly_connected_components(G)}\")\n",
    "    \n",
    "    # Degree statistics\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    print(f\"\\n=== DEGREE STATISTICS ===\")\n",
    "    print(f\"Average degree: {np.mean(degrees):.2f}\")\n",
    "    print(f\"Median degree: {np.median(degrees):.2f}\")\n",
    "    print(f\"Max degree: {np.max(degrees)}\")\n",
    "    print(f\"Min degree: {np.min(degrees)}\")\n",
    "    \n",
    "    # Node and edge attributes\n",
    "    if G.nodes():\n",
    "        sample_node = list(G.nodes())[0]\n",
    "        print(f\"\\n=== NODE ATTRIBUTES ===\")\n",
    "        print(f\"Sample node: {sample_node}\")\n",
    "        print(f\"Node attributes: {list(G.nodes[sample_node].keys())}\")\n",
    "    \n",
    "    if G.edges():\n",
    "        sample_edge = list(G.edges())[0]\n",
    "        print(f\"\\n=== EDGE ATTRIBUTES ===\")\n",
    "        print(f\"Sample edge: {sample_edge}\")\n",
    "        print(f\"Edge attributes: {list(G.edges[sample_edge].keys())}\")\n",
    "\n",
    "analyze_graph_structure(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degree distribution\n",
    "def plot_degree_distribution(G):\n",
    "    \"\"\"Plot degree distribution with both linear and log scales\"\"\"\n",
    "    if G is None:\n",
    "        return\n",
    "    \n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    degree_counts = Counter(degrees)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Linear scale\n",
    "    ax1.hist(degrees, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Degree')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Degree Distribution (Linear Scale)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale\n",
    "    degrees_vals, counts = zip(*sorted(degree_counts.items()))\n",
    "    ax2.loglog(degrees_vals, counts, 'o-', alpha=0.7, color='coral')\n",
    "    ax2.set_xlabel('Degree (log scale)')\n",
    "    ax2.set_ylabel('Frequency (log scale)')\n",
    "    ax2.set_title('Degree Distribution (Log-Log Scale)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top nodes by degree\n",
    "    top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\n=== TOP 10 NODES BY DEGREE ===\")\n",
    "    for node, degree in top_nodes:\n",
    "        print(f\"{node}: {degree}\")\n",
    "\n",
    "plot_degree_distribution(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81358f",
   "metadata": {},
   "source": [
    "## 3. Semantic Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relationship_types(G):\n",
    "    \"\"\"Analyze the distribution of relationship types in the graph\"\"\"\n",
    "    if G is None:\n",
    "        return\n",
    "    \n",
    "    # Extract relationship types\n",
    "    relations = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if 'relation' in data:\n",
    "            relations.append(data['relation'])\n",
    "        elif 'rel' in data:\n",
    "            relations.append(data['rel'])\n",
    "    \n",
    "    if not relations:\n",
    "        print(\"No relation attributes found in edges.\")\n",
    "        return\n",
    "    \n",
    "    relation_counts = Counter(relations)\n",
    "    \n",
    "    print(f\"=== RELATIONSHIP TYPE ANALYSIS ===\")\n",
    "    print(f\"Total unique relation types: {len(relation_counts)}\")\n",
    "    print(f\"Total relationships: {len(relations):,}\")\n",
    "    \n",
    "    # Top 20 most common relations\n",
    "    print(\"\\n=== TOP 20 RELATIONSHIP TYPES ===\")\n",
    "    for rel, count in relation_counts.most_common(20):\n",
    "        percentage = (count / len(relations)) * 100\n",
    "        print(f\"{rel}: {count:,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    return relation_counts\n",
    "\n",
    "relation_counts = analyze_relationship_types(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a400588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship distribution\n",
    "def plot_relationship_distribution(relation_counts, top_n=15):\n",
    "    \"\"\"Plot the distribution of relationship types\"\"\"\n",
    "    if relation_counts is None:\n",
    "        return\n",
    "    \n",
    "    # Get top N relationships\n",
    "    top_relations = relation_counts.most_common(top_n)\n",
    "    relations, counts = zip(*top_relations)\n",
    "    \n",
    "    # Create interactive plot with Plotly\n",
    "    fig = px.bar(\n",
    "        x=list(relations), \n",
    "        y=list(counts),\n",
    "        title=f'Top {top_n} Relationship Types Distribution',\n",
    "        labels={'x': 'Relationship Type', 'y': 'Count'},\n",
    "        color=list(counts),\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create a pie chart for proportions\n",
    "    fig_pie = px.pie(\n",
    "        values=list(counts), \n",
    "        names=list(relations),\n",
    "        title=f'Top {top_n} Relationship Types Distribution (Proportions)'\n",
    "    )\n",
    "    \n",
    "    fig_pie.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    fig_pie.update_layout(height=700)\n",
    "    fig_pie.show()\n",
    "\n",
    "if relation_counts:\n",
    "    plot_relationship_distribution(relation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab8b1f",
   "metadata": {},
   "source": [
    "## 4. Centrality Analysis and Important Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_centrality_measures(G, sample_size=1000):\n",
    "    \"\"\"Analyze centrality measures for important concepts\"\"\"\n",
    "    if G is None:\n",
    "        return\n",
    "    \n",
    "    print(\"=== CENTRALITY ANALYSIS ===\")\n",
    "    \n",
    "    # For large graphs, sample nodes for expensive centrality measures\n",
    "    if G.number_of_nodes() > sample_size:\n",
    "        print(f\"Sampling {sample_size} nodes for centrality analysis...\")\n",
    "        sample_nodes = np.random.choice(list(G.nodes()), sample_size, replace=False)\n",
    "        G_sample = G.subgraph(sample_nodes)\n",
    "    else:\n",
    "        G_sample = G\n",
    "    \n",
    "    centrality_measures = {}\n",
    "    \n",
    "    # Degree centrality (fast)\n",
    "    print(\"Computing degree centrality...\")\n",
    "    centrality_measures['degree'] = nx.degree_centrality(G)\n",
    "    \n",
    "    # Betweenness centrality (expensive, use sample)\n",
    "    print(\"Computing betweenness centrality...\")\n",
    "    centrality_measures['betweenness'] = nx.betweenness_centrality(G_sample)\n",
    "    \n",
    "    # Closeness centrality (expensive, use sample)\n",
    "    print(\"Computing closeness centrality...\")\n",
    "    centrality_measures['closeness'] = nx.closeness_centrality(G_sample)\n",
    "    \n",
    "    # Eigenvector centrality (can be expensive)\n",
    "    try:\n",
    "        print(\"Computing eigenvector centrality...\")\n",
    "        centrality_measures['eigenvector'] = nx.eigenvector_centrality(G_sample, max_iter=100)\n",
    "    except:\n",
    "        print(\"Eigenvector centrality failed - graph may not be connected\")\n",
    "    \n",
    "    # Print top concepts for each centrality measure\n",
    "    for measure, values in centrality_measures.items():\n",
    "        print(f\"\\n=== TOP 10 CONCEPTS BY {measure.upper()} CENTRALITY ===\")\n",
    "        top_concepts = sorted(values.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for concept, score in top_concepts:\n",
    "            print(f\"{concept}: {score:.6f}\")\n",
    "    \n",
    "    return centrality_measures\n",
    "\n",
    "centrality_measures = analyze_centrality_measures(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize centrality distributions\n",
    "def plot_centrality_distributions(centrality_measures):\n",
    "    \"\"\"Plot distributions of centrality measures\"\"\"\n",
    "    if not centrality_measures:\n",
    "        return\n",
    "    \n",
    "    n_measures = len(centrality_measures)\n",
    "    fig, axes = plt.subplots(1, n_measures, figsize=(5*n_measures, 5))\n",
    "    \n",
    "    if n_measures == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (measure, values) in enumerate(centrality_measures.items()):\n",
    "        centrality_scores = list(values.values())\n",
    "        \n",
    "        axes[i].hist(centrality_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_xlabel(f'{measure.title()} Centrality')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].set_title(f'{measure.title()} Centrality Distribution')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = np.mean(centrality_scores)\n",
    "        median_val = np.median(centrality_scores)\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.4f}')\n",
    "        axes[i].axvline(median_val, color='orange', linestyle='--', label=f'Median: {median_val:.4f}')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_centrality_distributions(centrality_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1c0d8",
   "metadata": {},
   "source": [
    "## 5. Semantic Path Finding and Concept Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b73903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_semantic_paths(G, source, target, max_paths=5):\n",
    "    \"\"\"Find semantic paths between two concepts\"\"\"\n",
    "    if G is None:\n",
    "        return []\n",
    "    \n",
    "    if source not in G.nodes() or target not in G.nodes():\n",
    "        print(f\"Error: '{source}' or '{target}' not found in graph\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Find shortest paths\n",
    "        paths = list(nx.all_shortest_paths(G, source, target))\n",
    "        paths = paths[:max_paths]  # Limit number of paths\n",
    "        \n",
    "        print(f\"=== SEMANTIC PATHS FROM '{source}' TO '{target}' ===\")\n",
    "        print(f\"Found {len(paths)} shortest path(s) of length {len(paths[0])-1 if paths else 0}\")\n",
    "        \n",
    "        for i, path in enumerate(paths, 1):\n",
    "            print(f\"\\nPath {i}:\")\n",
    "            for j in range(len(path)-1):\n",
    "                u, v = path[j], path[j+1]\n",
    "                edge_data = G.edges[u, v] if G.has_edge(u, v) else {}\n",
    "                relation = edge_data.get('relation', edge_data.get('rel', 'unknown'))\n",
    "                weight = edge_data.get('weight', 1.0)\n",
    "                print(f\"  {u} --[{relation}|{weight:.3f}]--> {v}\")\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path found between '{source}' and '{target}'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding paths: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example: Find paths between related concepts\n",
    "example_paths = [\n",
    "    (\"/c/en/dog\", \"/c/en/animal\"),\n",
    "    (\"/c/en/happy\", \"/c/en/sad\"),\n",
    "    (\"/c/en/car\", \"/c/en/transportation\"),\n",
    "    (\"/c/en/book\", \"/c/en/knowledge\")\n",
    "]\n",
    "\n",
    "for source, target in example_paths:\n",
    "    paths = find_semantic_paths(G, source, target)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e299f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_concept_neighborhood(G, concept, max_neighbors=20):\n",
    "    \"\"\"Explore the semantic neighborhood of a concept\"\"\"\n",
    "    if G is None or concept not in G.nodes():\n",
    "        print(f\"Concept '{concept}' not found in graph\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== EXPLORING CONCEPT: '{concept}' ===\")\n",
    "    \n",
    "    # Get neighbors\n",
    "    neighbors = list(G.neighbors(concept))\n",
    "    print(f\"Total neighbors: {len(neighbors)}\")\n",
    "    \n",
    "    # Analyze relationships by type\n",
    "    relationship_groups = defaultdict(list)\n",
    "    \n",
    "    for neighbor in neighbors:\n",
    "        edge_data = G.edges[concept, neighbor] if G.has_edge(concept, neighbor) else G.edges[neighbor, concept]\n",
    "        relation = edge_data.get('relation', edge_data.get('rel', 'unknown'))\n",
    "        weight = edge_data.get('weight', 1.0)\n",
    "        relationship_groups[relation].append((neighbor, weight))\n",
    "    \n",
    "    # Display relationships grouped by type\n",
    "    for relation, neighbors_list in sorted(relationship_groups.items()):\n",
    "        print(f\"\\n--- {relation.upper()} ({len(neighbors_list)} connections) ---\")\n",
    "        # Sort by weight (confidence) and show top ones\n",
    "        neighbors_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        for neighbor, weight in neighbors_list[:10]:  # Show top 10\n",
    "            print(f\"  {neighbor} (weight: {weight:.3f})\")\n",
    "        if len(neighbors_list) > 10:\n",
    "            print(f\"  ... and {len(neighbors_list) - 10} more\")\n",
    "    \n",
    "    return relationship_groups\n",
    "\n",
    "# Explore some interesting concepts\n",
    "interesting_concepts = [\"/c/en/love\", \"/c/en/intelligence\", \"/c/en/technology\", \"/c/en/nature\"]\n",
    "\n",
    "for concept in interesting_concepts:\n",
    "    if G and concept in G.nodes():\n",
    "        explore_concept_neighborhood(G, concept)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"Concept '{concept}' not found in graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d9807",
   "metadata": {},
   "source": [
    "## 6. Graph Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_graph_quality(G):\n",
    "    \"\"\"Assess the quality of semantic enrichments in the graph\"\"\"\n",
    "    if G is None:\n",
    "        return\n",
    "    \n",
    "    print(\"=== GRAPH QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Check for enriched attributes\n",
    "    enriched_edges = 0\n",
    "    high_confidence_edges = 0\n",
    "    transitivity_edges = 0\n",
    "    centrality_enriched = 0\n",
    "    \n",
    "    weight_values = []\n",
    "    confidence_values = []\n",
    "    \n",
    "    print(\"Analyzing edge attributes...\")\n",
    "    for u, v, data in tqdm(G.edges(data=True), desc=\"Processing edges\"):\n",
    "        if 'weight' in data:\n",
    "            weight_values.append(data['weight'])\n",
    "        \n",
    "        if 'high_confidence' in data and data['high_confidence']:\n",
    "            high_confidence_edges += 1\n",
    "        \n",
    "        if 'transitivity_inferred' in data and data['transitivity_inferred']:\n",
    "            transitivity_edges += 1\n",
    "        \n",
    "        if 'centrality_boosted' in data and data['centrality_boosted']:\n",
    "            centrality_enriched += 1\n",
    "        \n",
    "        # Count edges with multiple enrichments\n",
    "        enrichment_count = sum([\n",
    "            'high_confidence' in data,\n",
    "            'transitivity_inferred' in data,\n",
    "            'centrality_boosted' in data\n",
    "        ])\n",
    "        if enrichment_count > 0:\n",
    "            enriched_edges += 1\n",
    "    \n",
    "    total_edges = G.number_of_edges()\n",
    "    \n",
    "    print(f\"\\n=== ENRICHMENT STATISTICS ===\")\n",
    "    print(f\"Total edges: {total_edges:,}\")\n",
    "    print(f\"Enriched edges: {enriched_edges:,} ({enriched_edges/total_edges*100:.2f}%)\")\n",
    "    print(f\"High confidence edges: {high_confidence_edges:,} ({high_confidence_edges/total_edges*100:.2f}%)\")\n",
    "    print(f\"Transitivity inferred: {transitivity_edges:,} ({transitivity_edges/total_edges*100:.2f}%)\")\n",
    "    print(f\"Centrality boosted: {centrality_enriched:,} ({centrality_enriched/total_edges*100:.2f}%)\")\n",
    "    \n",
    "    if weight_values:\n",
    "        print(f\"\\n=== WEIGHT STATISTICS ===\")\n",
    "        print(f\"Mean weight: {np.mean(weight_values):.4f}\")\n",
    "        print(f\"Median weight: {np.median(weight_values):.4f}\")\n",
    "        print(f\"Weight std: {np.std(weight_values):.4f}\")\n",
    "        print(f\"Min weight: {np.min(weight_values):.4f}\")\n",
    "        print(f\"Max weight: {np.max(weight_values):.4f}\")\n",
    "        \n",
    "        # Plot weight distribution\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(weight_values, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Edge Weight')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Edge Weight Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(weight_values)\n",
    "        plt.ylabel('Edge Weight')\n",
    "        plt.title('Edge Weight Box Plot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_edges': total_edges,\n",
    "        'enriched_edges': enriched_edges,\n",
    "        'high_confidence_edges': high_confidence_edges,\n",
    "        'transitivity_edges': transitivity_edges,\n",
    "        'centrality_enriched': centrality_enriched,\n",
    "        'weight_stats': {\n",
    "            'mean': np.mean(weight_values) if weight_values else None,\n",
    "            'median': np.median(weight_values) if weight_values else None,\n",
    "            'std': np.std(weight_values) if weight_values else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "quality_stats = assess_graph_quality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185b8b0",
   "metadata": {},
   "source": [
    "## 7. Advanced Semantic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_query_builder(G):\n",
    "    \"\"\"Advanced semantic query functions\"\"\"\n",
    "    \n",
    "    def find_concepts_by_relation(relation_type, top_n=20):\n",
    "        \"\"\"Find most connected concepts for a specific relation type\"\"\"\n",
    "        concept_counts = defaultdict(int)\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            edge_relation = data.get('relation', data.get('rel', ''))\n",
    "            if relation_type.lower() in edge_relation.lower():\n",
    "                concept_counts[u] += 1\n",
    "                concept_counts[v] += 1\n",
    "        \n",
    "        top_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return top_concepts\n",
    "    \n",
    "    def find_bridge_concepts(concept1, concept2, max_hops=3):\n",
    "        \"\"\"Find concepts that bridge two other concepts\"\"\"\n",
    "        if concept1 not in G.nodes() or concept2 not in G.nodes():\n",
    "            return []\n",
    "        \n",
    "        bridge_concepts = []\n",
    "        \n",
    "        # Find all paths up to max_hops\n",
    "        try:\n",
    "            for path in nx.all_simple_paths(G, concept1, concept2, cutoff=max_hops):\n",
    "                if len(path) > 2:  # Has intermediate concepts\n",
    "                    bridge_concepts.extend(path[1:-1])  # Exclude source and target\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        \n",
    "        # Count frequency of bridge concepts\n",
    "        bridge_counts = Counter(bridge_concepts)\n",
    "        return bridge_counts.most_common(10)\n",
    "    \n",
    "    def find_concept_clusters(relation_type, min_cluster_size=5):\n",
    "        \"\"\"Find clusters of concepts connected by specific relation types\"\"\"\n",
    "        # Create subgraph with only specified relation type\n",
    "        edges_to_include = []\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            edge_relation = data.get('relation', data.get('rel', ''))\n",
    "            if relation_type.lower() in edge_relation.lower():\n",
    "                edges_to_include.append((u, v))\n",
    "        \n",
    "        if not edges_to_include:\n",
    "            return []\n",
    "        \n",
    "        subgraph = G.edge_subgraph(edges_to_include)\n",
    "        \n",
    "        # Find connected components\n",
    "        if G.is_directed():\n",
    "            components = list(nx.weakly_connected_components(subgraph))\n",
    "        else:\n",
    "            components = list(nx.connected_components(subgraph))\n",
    "        \n",
    "        # Filter by minimum size\n",
    "        large_components = [comp for comp in components if len(comp) >= min_cluster_size]\n",
    "        \n",
    "        return sorted(large_components, key=len, reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'find_concepts_by_relation': find_concepts_by_relation,\n",
    "        'find_bridge_concepts': find_bridge_concepts,\n",
    "        'find_concept_clusters': find_concept_clusters\n",
    "    }\n",
    "\n",
    "if G:\n",
    "    query_functions = semantic_query_builder(G)\n",
    "    print(\"Semantic query functions created successfully!\")\n",
    "    print(\"Available functions:\")\n",
    "    for func_name in query_functions.keys():\n",
    "        print(f\"  - {func_name}\")\n",
    "else:\n",
    "    print(\"Graph not loaded - cannot create query functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16236a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example semantic queries\n",
    "if G and 'query_functions' in locals():\n",
    "    # Query 1: Find concepts most associated with \"IsA\" relations\n",
    "    print(\"=== CONCEPTS MOST CONNECTED BY 'IsA' RELATIONS ===\")\n",
    "    isa_concepts = query_functions['find_concepts_by_relation']('isa', top_n=15)\n",
    "    for concept, count in isa_concepts:\n",
    "        print(f\"{concept}: {count} connections\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Query 2: Find bridge concepts between related concepts\n",
    "    print(\"=== BRIDGE CONCEPTS BETWEEN 'HAPPINESS' AND 'SUCCESS' ===\")\n",
    "    bridges = query_functions['find_bridge_concepts']('/c/en/happiness', '/c/en/success')\n",
    "    for bridge, count in bridges:\n",
    "        print(f\"{bridge}: appears in {count} bridging paths\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Query 3: Find concept clusters\n",
    "    print(\"=== CONCEPT CLUSTERS CONNECTED BY 'RelatedTo' RELATIONS ===\")\n",
    "    clusters = query_functions['find_concept_clusters']('relatedto', min_cluster_size=10)\n",
    "    for i, cluster in enumerate(clusters[:5], 1):  # Show top 5 clusters\n",
    "        print(f\"\\nCluster {i} ({len(cluster)} concepts):\")\n",
    "        sample_concepts = list(cluster)[:10]  # Show first 10 concepts\n",
    "        print(f\"  {', '.join(sample_concepts)}\")\n",
    "        if len(cluster) > 10:\n",
    "            print(f\"  ... and {len(cluster) - 10} more concepts\")\n",
    "else:\n",
    "    print(\"Cannot run semantic queries - graph not loaded or query functions not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d53ebd",
   "metadata": {},
   "source": [
    "## 8. Interactive Concept Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_concept_explorer(G):\n",
    "    \"\"\"Interactive function to explore concepts\"\"\"\n",
    "    if G is None:\n",
    "        print(\"Graph not loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç INTERACTIVE CONCEPT EXPLORER\")\n",
    "    print(\"Enter a concept to explore (e.g., '/c/en/love' or just 'love')\")\n",
    "    print(\"Type 'quit' to exit, 'help' for commands\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter concept: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif user_input.lower() == 'help':\n",
    "            print(\"\\nAvailable commands:\")\n",
    "            print(\"  - Enter concept name to explore its neighborhood\")\n",
    "            print(\"  - 'search <term>' to find concepts containing term\")\n",
    "            print(\"  - 'path <concept1> <concept2>' to find paths between concepts\")\n",
    "            print(\"  - 'stats' to show graph statistics\")\n",
    "            print(\"  - 'quit' to exit\")\n",
    "            continue\n",
    "        elif user_input.lower().startswith('search '):\n",
    "            search_term = user_input[7:].strip()\n",
    "            matching_concepts = [node for node in G.nodes() if search_term.lower() in node.lower()]\n",
    "            print(f\"\\nFound {len(matching_concepts)} concepts containing '{search_term}':\")\n",
    "            for concept in matching_concepts[:20]:  # Show first 20\n",
    "                print(f\"  {concept}\")\n",
    "            if len(matching_concepts) > 20:\n",
    "                print(f\"  ... and {len(matching_concepts) - 20} more\")\n",
    "            continue\n",
    "        elif user_input.lower().startswith('path '):\n",
    "            parts = user_input[5:].strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                concept1, concept2 = parts[0], parts[1]\n",
    "                # Auto-format if needed\n",
    "                if not concept1.startswith('/c/en/'):\n",
    "                    concept1 = f'/c/en/{concept1}'\n",
    "                if not concept2.startswith('/c/en/'):\n",
    "                    concept2 = f'/c/en/{concept2}'\n",
    "                find_semantic_paths(G, concept1, concept2, max_paths=3)\n",
    "            else:\n",
    "                print(\"Please provide two concepts: path <concept1> <concept2>\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'stats':\n",
    "            print(f\"\\nGraph Statistics:\")\n",
    "            print(f\"  Nodes: {G.number_of_nodes():,}\")\n",
    "            print(f\"  Edges: {G.number_of_edges():,}\")\n",
    "            print(f\"  Density: {nx.density(G):.6f}\")\n",
    "            continue\n",
    "        \n",
    "        # Regular concept exploration\n",
    "        concept = user_input\n",
    "        \n",
    "        # Auto-format concept if needed\n",
    "        if not concept.startswith('/c/en/') and not concept.startswith('/c/'):\n",
    "            concept = f'/c/en/{concept}'\n",
    "        \n",
    "        if concept in G.nodes():\n",
    "            explore_concept_neighborhood(G, concept, max_neighbors=15)\n",
    "        else:\n",
    "            print(f\"\\nConcept '{concept}' not found in graph.\")\n",
    "            # Suggest similar concepts\n",
    "            similar = [node for node in G.nodes() if user_input.lower() in node.lower()]\n",
    "            if similar:\n",
    "                print(\"Did you mean one of these?\")\n",
    "                for suggestion in similar[:10]:\n",
    "                    print(f\"  {suggestion}\")\n",
    "\n",
    "# Note: This function requires user input, so it's better suited for interactive use\n",
    "print(\"Interactive concept explorer function defined.\")\n",
    "print(\"Call interactive_concept_explorer(G) to start exploring!\")\n",
    "print(\"\\nExample concepts to try:\")\n",
    "print(\"  - love, happiness, intelligence, technology\")\n",
    "print(\"  - dog, animal, car, book, music\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7aa7c",
   "metadata": {},
   "source": [
    "## 9. Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results(G, centrality_measures, quality_stats, relation_counts):\n",
    "    \"\"\"Export analysis results to files\"\"\"\n",
    "    output_dir = r\"c:\\Users\\erich\\OneDrive\\Documents\\Python Projects\\Semantica-Full-Reasoning-Chatbot\\Data\\Output\"\n",
    "    \n",
    "    # Create analysis summary\n",
    "    analysis_summary = {\n",
    "        'graph_stats': {\n",
    "            'nodes': G.number_of_nodes() if G else 0,\n",
    "            'edges': G.number_of_edges() if G else 0,\n",
    "            'density': nx.density(G) if G else 0,\n",
    "            'is_directed': G.is_directed() if G else False\n",
    "        },\n",
    "        'quality_stats': quality_stats,\n",
    "        'relation_distribution': dict(relation_counts.most_common(50)) if relation_counts else {},\n",
    "        'top_concepts_by_centrality': {}\n",
    "    }\n",
    "    \n",
    "    # Add top concepts for each centrality measure\n",
    "    if centrality_measures:\n",
    "        for measure, values in centrality_measures.items():\n",
    "            top_concepts = sorted(values.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "            analysis_summary['top_concepts_by_centrality'][measure] = dict(top_concepts)\n",
    "    \n",
    "    # Save analysis summary\n",
    "    summary_file = f\"{output_dir}/semantic_graph_analysis_summary.json\"\n",
    "    try:\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis_summary, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Analysis summary saved to: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving analysis summary: {e}\")\n",
    "    \n",
    "    # Export top concepts by centrality as CSV\n",
    "    if centrality_measures:\n",
    "        centrality_df_data = []\n",
    "        for measure, values in centrality_measures.items():\n",
    "            for concept, score in values.items():\n",
    "                centrality_df_data.append({\n",
    "                    'concept': concept,\n",
    "                    'centrality_measure': measure,\n",
    "                    'score': score\n",
    "                })\n",
    "        \n",
    "        centrality_df = pd.DataFrame(centrality_df_data)\n",
    "        centrality_file = f\"{output_dir}/concept_centrality_scores.csv\"\n",
    "        try:\n",
    "            centrality_df.to_csv(centrality_file, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Centrality scores saved to: {centrality_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving centrality scores: {e}\")\n",
    "    \n",
    "    # Export relationship distribution as CSV\n",
    "    if relation_counts:\n",
    "        relation_df = pd.DataFrame([\n",
    "            {'relation': rel, 'count': count, 'percentage': count/sum(relation_counts.values())*100}\n",
    "            for rel, count in relation_counts.most_common()\n",
    "        ])\n",
    "        relation_file = f\"{output_dir}/relationship_distribution.csv\"\n",
    "        try:\n",
    "            relation_df.to_csv(relation_file, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Relationship distribution saved to: {relation_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving relationship distribution: {e}\")\n",
    "    \n",
    "    print(\"\\nüìä Export completed!\")\n",
    "\n",
    "# Export results\n",
    "if G:\n",
    "    export_analysis_results(G, centrality_measures, quality_stats, relation_counts)\n",
    "else:\n",
    "    print(\"Cannot export - graph not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(G, centrality_measures, quality_stats, relation_counts):\n",
    "    \"\"\"Generate a comprehensive final summary\"\"\"\n",
    "    print(\"üéØ SEMANTIC KNOWLEDGE GRAPH EXPLORATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if G is None:\n",
    "        print(\"‚ùå Graph could not be loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä GRAPH OVERVIEW:\")\n",
    "    print(f\"   ‚Ä¢ Total concepts (nodes): {G.number_of_nodes():,}\")\n",
    "    print(f\"   ‚Ä¢ Total relationships (edges): {G.number_of_edges():,}\")\n",
    "    print(f\"   ‚Ä¢ Graph density: {nx.density(G):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Graph type: {'Directed' if G.is_directed() else 'Undirected'}\")\n",
    "    \n",
    "    if quality_stats:\n",
    "        print(f\"\\nüîß SEMANTIC ENRICHMENT QUALITY:\")\n",
    "        total_edges = quality_stats['total_edges']\n",
    "        print(f\"   ‚Ä¢ Enriched edges: {quality_stats['enriched_edges']:,} ({quality_stats['enriched_edges']/total_edges*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ High confidence: {quality_stats['high_confidence_edges']:,} ({quality_stats['high_confidence_edges']/total_edges*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Transitivity inferred: {quality_stats['transitivity_edges']:,} ({quality_stats['transitivity_edges']/total_edges*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Centrality boosted: {quality_stats['centrality_enriched']:,} ({quality_stats['centrality_enriched']/total_edges*100:.1f}%)\")\n",
    "        \n",
    "        if quality_stats['weight_stats']['mean']:\n",
    "            print(f\"   ‚Ä¢ Average edge weight: {quality_stats['weight_stats']['mean']:.4f}\")\n",
    "    \n",
    "    if relation_counts:\n",
    "        print(f\"\\nüîó RELATIONSHIP TYPES:\")\n",
    "        print(f\"   ‚Ä¢ Unique relation types: {len(relation_counts)}\")\n",
    "        print(f\"   ‚Ä¢ Top 5 most common relations:\")\n",
    "        total_relations = sum(relation_counts.values())\n",
    "        for rel, count in relation_counts.most_common(5):\n",
    "            percentage = count / total_relations * 100\n",
    "            print(f\"     - {rel}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if centrality_measures:\n",
    "        print(f\"\\n‚≠ê MOST INFLUENTIAL CONCEPTS:\")\n",
    "        for measure, values in centrality_measures.items():\n",
    "            top_concept = max(values.items(), key=lambda x: x[1])\n",
    "            print(f\"   ‚Ä¢ By {measure}: {top_concept[0]} (score: {top_concept[1]:.4f})\")\n",
    "    \n",
    "    print(f\"\\nüéâ EXPLORATION CAPABILITIES AVAILABLE:\")\n",
    "    print(f\"   ‚Ä¢ Semantic path finding between concepts\")\n",
    "    print(f\"   ‚Ä¢ Concept neighborhood exploration\")\n",
    "    print(f\"   ‚Ä¢ Advanced semantic queries and clustering\")\n",
    "    print(f\"   ‚Ä¢ Centrality analysis for concept importance\")\n",
    "    print(f\"   ‚Ä¢ Interactive concept exploration tools\")\n",
    "    \n",
    "    print(f\"\\nüíæ EXPORTED FILES:\")\n",
    "    print(f\"   ‚Ä¢ semantic_graph_analysis_summary.json\")\n",
    "    print(f\"   ‚Ä¢ concept_centrality_scores.csv\")\n",
    "    print(f\"   ‚Ä¢ relationship_distribution.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üöÄ Semantic knowledge graph exploration complete!\")\n",
    "    print(\"   Your graph is ready for advanced semantic reasoning tasks.\")\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary(G, centrality_measures, quality_stats, relation_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
