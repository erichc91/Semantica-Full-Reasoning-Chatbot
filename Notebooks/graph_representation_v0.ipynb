{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d047a09",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "384872fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae455181",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8d48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_PATH = '../Data/Input/conceptnet_en_full.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395bdfc",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "***\n",
    "\n",
    "Allow an agent to interact with the full relation universe and create a full knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5c234",
   "metadata": {},
   "source": [
    "### Phase 1. \n",
    "***\n",
    "Load the english triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da087dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "uri",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "relation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weight",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sources",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "surfaceText",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "license",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "context",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ae7658a9-e231-437a-bce8-5af78e2f55eb",
       "rows": [
        [
         "0",
         "/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]",
         "/r/Antonym",
         "/c/en/0/n",
         "/c/en/1",
         "{\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/fr\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/24_hour_clock/]",
         "/r/Antonym",
         "/c/en/12_hour_clock/n",
         "/c/en/24_hour_clock",
         "{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/en\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/12_hour_clock/]",
         "/r/Antonym",
         "/c/en/24_hour_clock/n",
         "/c/en/12_hour_clock",
         "{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/en\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]",
         "/r/Antonym",
         "/c/en/5/n",
         "/c/en/3",
         "{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/en\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]",
         "/r/Antonym",
         "/c/en/a.c/n",
         "/c/en/d.c",
         "{\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/fr\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}",
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "      <th>relation</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>weight</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sources</th>\n",
       "      <th>surfaceText</th>\n",
       "      <th>license</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]</td>\n",
       "      <td>/r/Antonym</td>\n",
       "      <td>/c/en/0/n</td>\n",
       "      <td>/c/en/1</td>\n",
       "      <td>{\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/2...</td>\n",
       "      <td>/r/Antonym</td>\n",
       "      <td>/c/en/12_hour_clock/n</td>\n",
       "      <td>/c/en/24_hour_clock</td>\n",
       "      <td>{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/1...</td>\n",
       "      <td>/r/Antonym</td>\n",
       "      <td>/c/en/24_hour_clock/n</td>\n",
       "      <td>/c/en/12_hour_clock</td>\n",
       "      <td>{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]</td>\n",
       "      <td>/r/Antonym</td>\n",
       "      <td>/c/en/5/n</td>\n",
       "      <td>/c/en/3</td>\n",
       "      <td>{\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]</td>\n",
       "      <td>/r/Antonym</td>\n",
       "      <td>/c/en/a.c/n</td>\n",
       "      <td>/c/en/d.c</td>\n",
       "      <td>{\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 uri    relation  \\\n",
       "0               /a/[/r/Antonym/,/c/en/0/n/,/c/en/1/]  /r/Antonym   \n",
       "1  /a/[/r/Antonym/,/c/en/12_hour_clock/n/,/c/en/2...  /r/Antonym   \n",
       "2  /a/[/r/Antonym/,/c/en/24_hour_clock/n/,/c/en/1...  /r/Antonym   \n",
       "3               /a/[/r/Antonym/,/c/en/5/n/,/c/en/3/]  /r/Antonym   \n",
       "4           /a/[/r/Antonym/,/c/en/a.c/n/,/c/en/d.c/]  /r/Antonym   \n",
       "\n",
       "                   start                  end  \\\n",
       "0              /c/en/0/n              /c/en/1   \n",
       "1  /c/en/12_hour_clock/n  /c/en/24_hour_clock   \n",
       "2  /c/en/24_hour_clock/n  /c/en/12_hour_clock   \n",
       "3              /c/en/5/n              /c/en/3   \n",
       "4            /c/en/a.c/n            /c/en/d.c   \n",
       "\n",
       "                                              weight  dataset  sources  \\\n",
       "0  {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...      NaN      NaN   \n",
       "1  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...      NaN      NaN   \n",
       "2  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...      NaN      NaN   \n",
       "3  {\"dataset\": \"/d/wiktionary/en\", \"license\": \"cc...      NaN      NaN   \n",
       "4  {\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc...      NaN      NaN   \n",
       "\n",
       "   surfaceText  license  context  \n",
       "0          NaN      NaN      NaN  \n",
       "1          NaN      NaN      NaN  \n",
       "2          NaN      NaN      NaN  \n",
       "3          NaN      NaN      NaN  \n",
       "4          NaN      NaN      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_triples = pd.read_csv(EN_PATH)\n",
    "english_triples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff424404",
   "metadata": {},
   "source": [
    "#### Phase 1.A \n",
    "***\n",
    "Preprocess and prepare the concept data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc48b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"dataset\": \"/d/wiktionary/fr\", \"license\": \"cc:by-sa/4.0\", \"sources\": [{\"contributor\": \"/s/resource/wiktionary/fr\", \"process\": \"/s/process/wikiparsec/2\"}], \"weight\": 1.0}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_triples['weight'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551364de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "relation_cleaned",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_cleaned",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end_cleaned",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weight_cleaned",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0fcce3cc-9de7-44e0-995b-670ce906138d",
       "rows": [
        [
         "0",
         "Antonym",
         "n",
         "1",
         "1.0"
        ],
        [
         "1",
         "Antonym",
         "n",
         "24_hour_clock",
         "1.0"
        ],
        [
         "2",
         "Antonym",
         "n",
         "12_hour_clock",
         "1.0"
        ],
        [
         "3",
         "Antonym",
         "n",
         "3",
         "1.0"
        ],
        [
         "4",
         "Antonym",
         "n",
         "d.c",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation_cleaned</th>\n",
       "      <th>start_cleaned</th>\n",
       "      <th>end_cleaned</th>\n",
       "      <th>weight_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>24_hour_clock</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>12_hour_clock</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>d.c</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relation_cleaned start_cleaned    end_cleaned  weight_cleaned\n",
       "0          Antonym             n              1             1.0\n",
       "1          Antonym             n  24_hour_clock             1.0\n",
       "2          Antonym             n  12_hour_clock             1.0\n",
       "3          Antonym             n              3             1.0\n",
       "4          Antonym             n            d.c             1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_column(df, col):\n",
    "    col_string = df[col].astype(str)\n",
    "    col_string = col_string.str.split('/')\n",
    "    col_string = col_string.str[-1]\n",
    "    df[f'{col}_cleaned'] = col_string\n",
    "    return df\n",
    "\n",
    "def extract_weight(df):\n",
    "    df['weight_cleaned'] = df['weight'].apply(lambda x: json.loads(x)['weight'])\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df_copy = df.copy()\n",
    "    for col in ['relation', 'start', 'end']:\n",
    "        df_copy = clean_column(df_copy, col)\n",
    "    df_copy = extract_weight(df_copy)\n",
    "    return df_copy\n",
    "\n",
    "cleaned_cols = ['relation', 'start', 'end', 'weight']\n",
    "cleaned_cols = [f'{col}_cleaned' for col in cleaned_cols]\n",
    "\n",
    "reprocess_triples = False\n",
    "\n",
    "if reprocess_triples:\n",
    "    cleaned_english_triples = preprocess_data(english_triples)[cleaned_cols].drop_duplicates()\n",
    "    cleaned_english_triples[cleaned_cols].head()\n",
    "    # Save as a parquet file\n",
    "    cleaned_english_triples.to_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_cleaned.parquet.gzip'),\n",
    "        index=False,\n",
    "        compression='gzip'\n",
    "    )\n",
    "else:\n",
    "    # Load the cleaned data\n",
    "    cleaned_english_triples = pd.read_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_cleaned.parquet.gzip')\n",
    "    )\n",
    "cleaned_english_triples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d88c8a",
   "metadata": {},
   "source": [
    "# Phase 2. Seed the agent with random english relations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41eded98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_weighted_sample(df, sample_size=5000, min_weight_threshold=0.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Create a stratified sample maintaining relation distribution while prioritizing higher weights\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['start_concept', 'end_concept', 'relation_type', 'edge_weight']\n",
    "    - sample_size: Target number of triples in sample\n",
    "    - min_weight_threshold: Minimum weight to consider (filters low-quality relations)\n",
    "    - verbose: Print detailed progress\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Stratified sample\n",
    "    \"\"\"\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.rename(columns={\n",
    "        'relation_cleaned': 'relation_type',\n",
    "        'start_cleaned': 'start_concept',\n",
    "        'end_cleaned': 'end_concept',\n",
    "        'weight_cleaned': 'edge_weight'\n",
    "    })\n",
    "    \n",
    "    print(f\"🎯 Creating stratified weighted sample of {sample_size:,} triples...\")\n",
    "    \n",
    "    # Step 1: Filter by minimum weight threshold\n",
    "    if verbose:\n",
    "        print(f\"   Filtering triples with weight >= {min_weight_threshold}\")\n",
    "    \n",
    "    initial_count = len(df_copy)\n",
    "    filtered_df = df_copy[df_copy['edge_weight'] >= min_weight_threshold].copy()\n",
    "    filtered_count = len(filtered_df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Kept {filtered_count:,} of {initial_count:,} triples ({filtered_count/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # Step 2: Calculate current relation distribution\n",
    "    relation_counts = filtered_df['relation_type'].value_counts()\n",
    "    relation_proportions = relation_counts / len(filtered_df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n📊 Original relation distribution (top 10):\")\n",
    "        for relation, prop in relation_proportions.head(10).items():\n",
    "            count = relation_counts[relation]\n",
    "            print(f\"   {relation}: {count:,} ({prop*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Sort by weight within each relation (highest first)\n",
    "    if verbose:\n",
    "        print(f\"\\n⚖️  Sorting by weight within each relation...\")\n",
    "    \n",
    "    filtered_df = filtered_df.sort_values(['relation_type', 'edge_weight'], \n",
    "                                        ascending=[True, False])\n",
    "    \n",
    "    # Step 4: Calculate target samples per relation\n",
    "    target_samples_per_relation = {}\n",
    "    for relation in relation_proportions.index:\n",
    "        target_count = int(sample_size * relation_proportions[relation])\n",
    "        # Ensure at least 1 sample for each relation if possible\n",
    "        target_count = max(1, target_count)\n",
    "        target_samples_per_relation[relation] = target_count\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n🎯 Target samples per relation:\")\n",
    "        total_targeted = sum(target_samples_per_relation.values())\n",
    "        for relation, target in sorted(target_samples_per_relation.items(), \n",
    "                                     key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"   {relation}: {target:,}\")\n",
    "        print(f\"   Total targeted: {total_targeted:,}\")\n",
    "    \n",
    "    # Step 5: Sample from each relation group\n",
    "    sampled_dfs = []\n",
    "    actual_samples = {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n🔄 Sampling from each relation group...\")\n",
    "    \n",
    "    for relation, target_count in tqdm(target_samples_per_relation.items(), \n",
    "                                     desc=\"Sampling relations\"):\n",
    "        relation_data = filtered_df[filtered_df['relation_type'] == relation]\n",
    "        \n",
    "        # Take top weighted samples up to target count\n",
    "        actual_count = min(target_count, len(relation_data))\n",
    "        sampled_data = relation_data.head(actual_count)\n",
    "        \n",
    "        sampled_dfs.append(sampled_data)\n",
    "        actual_samples[relation] = actual_count\n",
    "    \n",
    "    # Step 6: Combine all samples\n",
    "    stratified_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Step 7: If we're short, fill with highest-weight remaining samples\n",
    "    current_size = len(stratified_sample)\n",
    "    if current_size < sample_size:\n",
    "        shortage = sample_size - current_size\n",
    "        if verbose:\n",
    "            print(f\"   Short by {shortage:,} samples, filling with highest-weight remaining...\")\n",
    "        \n",
    "        # Get samples not already included\n",
    "        used_indices = set(stratified_sample.index) if hasattr(stratified_sample, 'index') else set()\n",
    "        remaining_df = filtered_df[~filtered_df.index.isin(used_indices)]\n",
    "        \n",
    "        if len(remaining_df) > 0:\n",
    "            # Sort by weight and take top samples\n",
    "            top_remaining = remaining_df.nlargest(shortage, 'edge_weight')\n",
    "            stratified_sample = pd.concat([stratified_sample, top_remaining], ignore_index=True)\n",
    "    \n",
    "    # Step 8: Final shuffle to mix relations\n",
    "    stratified_sample = stratified_sample.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Step 9: Validation and statistics\n",
    "    final_size = len(stratified_sample)\n",
    "    final_relation_counts = stratified_sample['relation_type'].value_counts()\n",
    "    final_relation_proportions = final_relation_counts / final_size\n",
    "    \n",
    "    print(f\"\\n✅ Stratified sample created!\")\n",
    "    print(f\"   Final size: {final_size:,} triples\")\n",
    "    print(f\"   Weight range: {stratified_sample['edge_weight'].min():.3f} - {stratified_sample['edge_weight'].max():.3f}\")\n",
    "    print(f\"   Mean weight: {stratified_sample['edge_weight'].mean():.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n📊 Final relation distribution (top 10):\")\n",
    "        for relation, prop in final_relation_proportions.head(10).items():\n",
    "            count = final_relation_counts[relation]\n",
    "            original_prop = relation_proportions.get(relation, 0)\n",
    "            print(f\"   {relation}: {count:,} ({prop*100:.1f}% vs {original_prop*100:.1f}% orig)\")\n",
    "    \n",
    "    return stratified_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44b913",
   "metadata": {},
   "source": [
    "#### Phase 2A. \n",
    "***\n",
    "Created stratified sampling for agent seeded nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca7b854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 Creating stratified weighted sample for agent initialization...\n",
      "🎯 Creating stratified weighted sample of 5,000 triples...\n",
      "   Filtering triples with weight >= 0.5\n",
      "   Kept 1,477,248 of 1,655,522 triples (89.2%)\n",
      "\n",
      "📊 Original relation distribution (top 10):\n",
      "   RelatedTo: 417,772 (28.3%)\n",
      "   DerivedFrom: 324,167 (21.9%)\n",
      "   FormOf: 294,073 (19.9%)\n",
      "   Synonym: 107,359 (7.3%)\n",
      "   IsA: 65,328 (4.4%)\n",
      "   UsedFor: 39,470 (2.7%)\n",
      "   AtLocation: 27,708 (1.9%)\n",
      "   HasSubevent: 25,238 (1.7%)\n",
      "   HasPrerequisite: 22,710 (1.5%)\n",
      "   CapableOf: 22,677 (1.5%)\n",
      "\n",
      "⚖️  Sorting by weight within each relation...\n",
      "\n",
      "🎯 Target samples per relation:\n",
      "   RelatedTo: 1,414\n",
      "   DerivedFrom: 1,097\n",
      "   FormOf: 995\n",
      "   Synonym: 363\n",
      "   IsA: 221\n",
      "   UsedFor: 133\n",
      "   AtLocation: 93\n",
      "   HasSubevent: 85\n",
      "   HasPrerequisite: 76\n",
      "   CapableOf: 76\n",
      "   Total targeted: 4,983\n",
      "\n",
      "🔄 Sampling from each relation group...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling relations: 100%|██████████| 47/47 [00:01<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Short by 17 samples, filling with highest-weight remaining...\n",
      "\n",
      "✅ Stratified sample created!\n",
      "   Final size: 5,000 triples\n",
      "   Weight range: 0.500 - 22.891\n",
      "   Mean weight: 4.063\n",
      "\n",
      "📊 Final relation distribution (top 10):\n",
      "   RelatedTo: 1,424 (28.5% vs 28.3% orig)\n",
      "   DerivedFrom: 1,097 (21.9% vs 21.9% orig)\n",
      "   FormOf: 995 (19.9% vs 19.9% orig)\n",
      "   Synonym: 363 (7.3% vs 7.3% orig)\n",
      "   IsA: 226 (4.5% vs 4.4% orig)\n",
      "   UsedFor: 133 (2.7% vs 2.7% orig)\n",
      "   AtLocation: 93 (1.9% vs 1.9% orig)\n",
      "   HasSubevent: 85 (1.7% vs 1.7% orig)\n",
      "   CapableOf: 77 (1.5% vs 1.5% orig)\n",
      "   HasPrerequisite: 76 (1.5% vs 1.5% orig)\n",
      "\n",
      "🔍 Sample characteristics:\n",
      "Weight distribution:\n",
      "count    5000.000000\n",
      "mean        4.062885\n",
      "std         2.422774\n",
      "min         0.500000\n",
      "25%         2.000000\n",
      "50%         2.828000\n",
      "75%         5.759000\n",
      "max        22.891000\n",
      "Name: edge_weight, dtype: float64\n",
      "\n",
      "Top concept pairs by weight:\n",
      "   baseball --IsA--> sport (weight: 22.891)\n",
      "   baseball --IsA--> sport (weight: 22.891)\n",
      "   yo_yo --IsA--> toy (weight: 19.391)\n",
      "   yo_yo --IsA--> toy (weight: 19.391)\n",
      "   dog --CapableOf--> bark (weight: 16.000)\n",
      "\n",
      "💾 Stratified seed data saved to: ../Data/Input\\conceptnet_en_stratified_seed_5k.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "resample_data = True\n",
    "\n",
    "if resample_data:\n",
    "    # Apply the stratified sampling to your cleaned data\n",
    "    print(\"🎲 Creating stratified weighted sample for agent initialization...\")\n",
    "\n",
    "    stratified_seed_data = create_stratified_weighted_sample(\n",
    "        df=cleaned_english_triples,\n",
    "        sample_size=5000,\n",
    "        min_weight_threshold=0.5,  # Only include relations with decent confidence\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Show sample characteristics\n",
    "    print(f\"\\n🔍 Sample characteristics:\")\n",
    "    print(f\"Weight distribution:\")\n",
    "    print(stratified_seed_data['edge_weight'].describe())\n",
    "\n",
    "    print(f\"\\nTop concept pairs by weight:\")\n",
    "    top_weighted = stratified_seed_data.nlargest(5, 'edge_weight')\n",
    "    for _, row in top_weighted.iterrows():\n",
    "        print(f\"   {row['start_concept']} --{row['relation_type']}--> {row['end_concept']} (weight: {row['edge_weight']:.3f})\")\n",
    "    # Save the stratified sample\n",
    "    stratified_output_path = os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_stratified_seed_5k.parquet.gzip')\n",
    "    stratified_seed_data.to_parquet(stratified_output_path, index=False, compression='gzip')\n",
    "    print(f\"\\n💾 Stratified seed data saved to: {stratified_output_path}\")\n",
    "else:\n",
    "    # Load the stratified sample\n",
    "    stratified_seed_data = pd.read_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_stratified.parquet.gzip')\n",
    "    )\n",
    "    print(f\"Loaded {len(stratified_seed_data):,} triples from stratified sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b64083",
   "metadata": {},
   "source": [
    "## Phase 3\n",
    "***\n",
    "Agent memory graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25789d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2.B - Initialize Agent with Stratified Seed Data\n",
    "\n",
    "print(\"🧠 Initializing Knowledge Graph Agent with stratified seed...\")\n",
    "agent = KnowledgeGraphAgent(validate_on_add=True, verbose=False)\n",
    "\n",
    "# Load the stratified sample\n",
    "print(f\"📊 Loading {len(stratified_seed_data):,} stratified triples...\")\n",
    "agent.bulk_load_triples(stratified_seed_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 TESTING AGENT WITH STRATIFIED SEED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Show relation diversity\n",
    "print(\"\\n📊 Relation diversity in loaded graph:\")\n",
    "relation_stats = {}\n",
    "for _, _, data in agent.graph.edges(data=True):\n",
    "    relation = data.get('relation', 'Unknown')\n",
    "    if relation not in relation_stats:\n",
    "        relation_stats[relation] = 0\n",
    "    relation_stats[relation] += 1\n",
    "\n",
    "# Show top relations\n",
    "sorted_relations = sorted(relation_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 relations in graph:\")\n",
    "for relation, count in sorted_relations[:10]:\n",
    "    print(f\"   {relation}: {count:,}\")\n",
    "\n",
    "# Test 2: Quality check - show high-weight concepts\n",
    "print(f\"\\n⚖️  High-quality relationships (weight > 0.8):\")\n",
    "high_quality_count = 0\n",
    "for start, end, data in agent.graph.edges(data=True):\n",
    "    if data.get('weight', 0) > 0.8:\n",
    "        relation = data.get('relation', 'Unknown')\n",
    "        weight = data.get('weight', 0)\n",
    "        print(f\"   {start} --{relation}--> {end} (weight: {weight:.3f})\")\n",
    "        high_quality_count += 1\n",
    "        if high_quality_count >= 10:  # Limit output\n",
    "            break\n",
    "\n",
    "print(f\"Total high-quality relationships: {high_quality_count:,}\")\n",
    "\n",
    "# Test 3: Concept connectivity analysis\n",
    "print(f\"\\n🕸️  Connectivity analysis:\")\n",
    "node_degrees = dict(agent.graph.degree())\n",
    "top_connected = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Most connected concepts:\")\n",
    "for concept, degree in top_connected:\n",
    "    print(f\"   {concept}: {degree} connections\")\n",
    "\n",
    "# Test 4: Sample queries on well-connected concepts\n",
    "print(f\"\\n🔍 Testing queries on top concepts:\")\n",
    "for concept, degree in top_connected[:3]:\n",
    "    print(f\"\\n--- Relationships for '{concept}' (degree: {degree}) ---\")\n",
    "    agent.query_concept(concept, max_results=5)\n",
    "\n",
    "print(f\"\\n🎉 Agent successfully initialized with high-quality stratified seed!\")\n",
    "print(f\"   Ready for knowledge graph reasoning and expansion!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
