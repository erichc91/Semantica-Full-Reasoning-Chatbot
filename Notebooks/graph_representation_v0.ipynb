{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d047a09",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "384872fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae455181",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_PATH = '../Data/Input/conceptnet_en_full.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395bdfc",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "***\n",
    "\n",
    "Allow an agent to interact with the full relation universe and create a full knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5c234",
   "metadata": {},
   "source": [
    "### Phase 1. \n",
    "***\n",
    "Load the english triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da087dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_triples = pd.read_csv(EN_PATH)\n",
    "english_triples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff424404",
   "metadata": {},
   "source": [
    "#### Phase 1.A \n",
    "***\n",
    "Preprocess and prepare the concept data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_triples['weight'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551364de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column(df, col):\n",
    "    col_string = df[col].astype(str)\n",
    "    col_string = col_string.str.split('/')\n",
    "    col_string = col_string.str[-1]\n",
    "    df[f'{col}_cleaned'] = col_string\n",
    "    return df\n",
    "\n",
    "def extract_weight(df):\n",
    "    df['weight_cleaned'] = df['weight'].apply(lambda x: json.loads(x)['weight'])\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df_copy = df.copy()\n",
    "    for col in ['relation', 'start', 'end']:\n",
    "        df_copy = clean_column(df_copy, col)\n",
    "    df_copy = extract_weight(df_copy)\n",
    "    return df_copy\n",
    "\n",
    "cleaned_cols = ['relation', 'start', 'end', 'weight']\n",
    "cleaned_cols = [f'{col}_cleaned' for col in cleaned_cols]\n",
    "\n",
    "reprocess_triples = False\n",
    "\n",
    "if reprocess_triples:\n",
    "    cleaned_english_triples = preprocess_data(english_triples)[cleaned_cols].drop_duplicates()\n",
    "    cleaned_english_triples[cleaned_cols].head()\n",
    "    # Save as a parquet file\n",
    "    cleaned_english_triples.to_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_cleaned.parquet.gzip'),\n",
    "        index=False,\n",
    "        compression='gzip'\n",
    "    )\n",
    "else:\n",
    "    # Load the cleaned data\n",
    "    cleaned_english_triples = pd.read_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_cleaned.parquet.gzip')\n",
    "    )\n",
    "cleaned_english_triples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d88c8a",
   "metadata": {},
   "source": [
    "# Phase 2. Seed the agent with random english relations\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41eded98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_weighted_sample(df, sample_size=5000, min_weight_threshold=0.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Create a stratified sample maintaining relation distribution while prioritizing higher weights\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['start_concept', 'end_concept', 'relation_type', 'edge_weight']\n",
    "    - sample_size: Target number of triples in sample\n",
    "    - min_weight_threshold: Minimum weight to consider (filters low-quality relations)\n",
    "    - verbose: Print detailed progress\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Stratified sample\n",
    "    \"\"\"\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.rename(columns={\n",
    "        'relation_cleaned': 'relation_type',\n",
    "        'start_cleaned': 'start_concept',\n",
    "        'end_cleaned': 'end_concept',\n",
    "        'weight_cleaned': 'edge_weight'\n",
    "    })\n",
    "    \n",
    "    print(f\"üéØ Creating stratified weighted sample of {sample_size:,} triples...\")\n",
    "    \n",
    "    # Step 1: Filter by minimum weight threshold\n",
    "    if verbose:\n",
    "        print(f\"   Filtering triples with weight >= {min_weight_threshold}\")\n",
    "    \n",
    "    initial_count = len(df_copy)\n",
    "    filtered_df = df_copy[df_copy['edge_weight'] >= min_weight_threshold].copy()\n",
    "    filtered_count = len(filtered_df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Kept {filtered_count:,} of {initial_count:,} triples ({filtered_count/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    # Step 2: Calculate current relation distribution\n",
    "    relation_counts = filtered_df['relation_type'].value_counts()\n",
    "    relation_proportions = relation_counts / len(filtered_df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìä Original relation distribution (top 10):\")\n",
    "        for relation, prop in relation_proportions.head(10).items():\n",
    "            count = relation_counts[relation]\n",
    "            print(f\"   {relation}: {count:,} ({prop*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Sort by weight within each relation (highest first)\n",
    "    if verbose:\n",
    "        print(f\"\\n‚öñÔ∏è  Sorting by weight within each relation...\")\n",
    "    \n",
    "    filtered_df = filtered_df.sort_values(['relation_type', 'edge_weight'], \n",
    "                                        ascending=[True, False])\n",
    "    \n",
    "    # Step 4: Calculate target samples per relation\n",
    "    target_samples_per_relation = {}\n",
    "    for relation in relation_proportions.index:\n",
    "        target_count = int(sample_size * relation_proportions[relation])\n",
    "        # Ensure at least 1 sample for each relation if possible\n",
    "        target_count = max(1, target_count)\n",
    "        target_samples_per_relation[relation] = target_count\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéØ Target samples per relation:\")\n",
    "        total_targeted = sum(target_samples_per_relation.values())\n",
    "        for relation, target in sorted(target_samples_per_relation.items(), \n",
    "                                     key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"   {relation}: {target:,}\")\n",
    "        print(f\"   Total targeted: {total_targeted:,}\")\n",
    "    \n",
    "    # Step 5: Sample from each relation group\n",
    "    sampled_dfs = []\n",
    "    actual_samples = {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîÑ Sampling from each relation group...\")\n",
    "    \n",
    "    for relation, target_count in tqdm(target_samples_per_relation.items(), \n",
    "                                     desc=\"Sampling relations\"):\n",
    "        relation_data = filtered_df[filtered_df['relation_type'] == relation]\n",
    "        \n",
    "        # Take top weighted samples up to target count\n",
    "        actual_count = min(target_count, len(relation_data))\n",
    "        sampled_data = relation_data.head(actual_count)\n",
    "        \n",
    "        sampled_dfs.append(sampled_data)\n",
    "        actual_samples[relation] = actual_count\n",
    "    \n",
    "    # Step 6: Combine all samples\n",
    "    stratified_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Step 7: If we're short, fill with highest-weight remaining samples\n",
    "    current_size = len(stratified_sample)\n",
    "    if current_size < sample_size:\n",
    "        shortage = sample_size - current_size\n",
    "        if verbose:\n",
    "            print(f\"   Short by {shortage:,} samples, filling with highest-weight remaining...\")\n",
    "        \n",
    "        # Get samples not already included\n",
    "        used_indices = set(stratified_sample.index) if hasattr(stratified_sample, 'index') else set()\n",
    "        remaining_df = filtered_df[~filtered_df.index.isin(used_indices)]\n",
    "        \n",
    "        if len(remaining_df) > 0:\n",
    "            # Sort by weight and take top samples\n",
    "            top_remaining = remaining_df.nlargest(shortage, 'edge_weight')\n",
    "            stratified_sample = pd.concat([stratified_sample, top_remaining], ignore_index=True)\n",
    "    \n",
    "    # Step 8: Final shuffle to mix relations\n",
    "    stratified_sample = stratified_sample.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Step 9: Validation and statistics\n",
    "    final_size = len(stratified_sample)\n",
    "    final_relation_counts = stratified_sample['relation_type'].value_counts()\n",
    "    final_relation_proportions = final_relation_counts / final_size\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stratified sample created!\")\n",
    "    print(f\"   Final size: {final_size:,} triples\")\n",
    "    print(f\"   Weight range: {stratified_sample['edge_weight'].min():.3f} - {stratified_sample['edge_weight'].max():.3f}\")\n",
    "    print(f\"   Mean weight: {stratified_sample['edge_weight'].mean():.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìä Final relation distribution (top 10):\")\n",
    "        for relation, prop in final_relation_proportions.head(10).items():\n",
    "            count = final_relation_counts[relation]\n",
    "            original_prop = relation_proportions.get(relation, 0)\n",
    "            print(f\"   {relation}: {count:,} ({prop*100:.1f}% vs {original_prop*100:.1f}% orig)\")\n",
    "    \n",
    "    return stratified_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44b913",
   "metadata": {},
   "source": [
    "#### Phase 2A. \n",
    "***\n",
    "Created stratified sampling for agent seeded nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca7b854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Creating stratified weighted sample for agent initialization...\n",
      "üéØ Creating stratified weighted sample of 5,000 triples...\n",
      "   Filtering triples with weight >= 0.5\n",
      "   Kept 1,477,248 of 1,655,522 triples (89.2%)\n",
      "\n",
      "üìä Original relation distribution (top 10):\n",
      "   RelatedTo: 417,772 (28.3%)\n",
      "   DerivedFrom: 324,167 (21.9%)\n",
      "   FormOf: 294,073 (19.9%)\n",
      "   Synonym: 107,359 (7.3%)\n",
      "   IsA: 65,328 (4.4%)\n",
      "   UsedFor: 39,470 (2.7%)\n",
      "   AtLocation: 27,708 (1.9%)\n",
      "   HasSubevent: 25,238 (1.7%)\n",
      "   HasPrerequisite: 22,710 (1.5%)\n",
      "   CapableOf: 22,677 (1.5%)\n",
      "\n",
      "‚öñÔ∏è  Sorting by weight within each relation...\n",
      "\n",
      "üéØ Target samples per relation:\n",
      "   RelatedTo: 1,414\n",
      "   DerivedFrom: 1,097\n",
      "   FormOf: 995\n",
      "   Synonym: 363\n",
      "   IsA: 221\n",
      "   UsedFor: 133\n",
      "   AtLocation: 93\n",
      "   HasSubevent: 85\n",
      "   HasPrerequisite: 76\n",
      "   CapableOf: 76\n",
      "   Total targeted: 4,983\n",
      "\n",
      "üîÑ Sampling from each relation group...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling relations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:02<00:00, 22.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Short by 17 samples, filling with highest-weight remaining...\n",
      "\n",
      "‚úÖ Stratified sample created!\n",
      "   Final size: 5,000 triples\n",
      "   Weight range: 0.500 - 22.891\n",
      "   Mean weight: 4.063\n",
      "\n",
      "üìä Final relation distribution (top 10):\n",
      "   RelatedTo: 1,424 (28.5% vs 28.3% orig)\n",
      "   DerivedFrom: 1,097 (21.9% vs 21.9% orig)\n",
      "   FormOf: 995 (19.9% vs 19.9% orig)\n",
      "   Synonym: 363 (7.3% vs 7.3% orig)\n",
      "   IsA: 226 (4.5% vs 4.4% orig)\n",
      "   UsedFor: 133 (2.7% vs 2.7% orig)\n",
      "   AtLocation: 93 (1.9% vs 1.9% orig)\n",
      "   HasSubevent: 85 (1.7% vs 1.7% orig)\n",
      "   CapableOf: 77 (1.5% vs 1.5% orig)\n",
      "   HasPrerequisite: 76 (1.5% vs 1.5% orig)\n",
      "\n",
      "üîç Sample characteristics:\n",
      "Weight distribution:\n",
      "count    5000.000000\n",
      "mean        4.062885\n",
      "std         2.422774\n",
      "min         0.500000\n",
      "25%         2.000000\n",
      "50%         2.828000\n",
      "75%         5.759000\n",
      "max        22.891000\n",
      "Name: edge_weight, dtype: float64\n",
      "\n",
      "Top concept pairs by weight:\n",
      "   baseball --IsA--> sport (weight: 22.891)\n",
      "   baseball --IsA--> sport (weight: 22.891)\n",
      "   yo_yo --IsA--> toy (weight: 19.391)\n",
      "   yo_yo --IsA--> toy (weight: 19.391)\n",
      "   dog --CapableOf--> bark (weight: 16.000)\n",
      "\n",
      "üíæ Stratified seed data saved to: ../Data/Input\\conceptnet_en_stratified_seed_5k.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "resample_data = True\n",
    "\n",
    "if resample_data:\n",
    "    # Apply the stratified sampling to your cleaned data\n",
    "    print(\"üé≤ Creating stratified weighted sample for agent initialization...\")\n",
    "\n",
    "    stratified_seed_data = create_stratified_weighted_sample(\n",
    "        df=cleaned_english_triples,\n",
    "        sample_size=5000,\n",
    "        min_weight_threshold=0.5,  # Only include relations with decent confidence\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Show sample characteristics\n",
    "    print(f\"\\nüîç Sample characteristics:\")\n",
    "    print(f\"Weight distribution:\")\n",
    "    print(stratified_seed_data['edge_weight'].describe())\n",
    "\n",
    "    print(f\"\\nTop concept pairs by weight:\")\n",
    "    top_weighted = stratified_seed_data.nlargest(5, 'edge_weight')\n",
    "    for _, row in top_weighted.iterrows():\n",
    "        print(f\"   {row['start_concept']} --{row['relation_type']}--> {row['end_concept']} (weight: {row['edge_weight']:.3f})\")\n",
    "    # Save the stratified sample\n",
    "    stratified_output_path = os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_stratified_seed_5k.parquet.gzip')\n",
    "    stratified_seed_data.to_parquet(stratified_output_path, index=False, compression='gzip')\n",
    "    print(f\"\\nüíæ Stratified seed data saved to: {stratified_output_path}\")\n",
    "else:\n",
    "    # Load the stratified sample\n",
    "    stratified_seed_data = pd.read_parquet(\n",
    "        os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_full_stratified.parquet.gzip')\n",
    "    )\n",
    "    print(f\"Loaded {len(stratified_seed_data):,} triples from stratified sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06f2c4",
   "metadata": {},
   "source": [
    "# Phase 3 \n",
    "***\n",
    "Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08abfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphAgent:\n",
    "    \"\"\"\n",
    "    MVP Knowledge Graph Agent with self-validation and extensible structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, validate_on_add=True, verbose=True):\n",
    "        self.graph = nx.MultiDiGraph()  # Allows multiple edges between same nodes\n",
    "        self.validate_on_add = validate_on_add\n",
    "        self.verbose = verbose\n",
    "        self.validation_stats = {\n",
    "            'total_attempted': 0,\n",
    "            'successful_adds': 0,\n",
    "            'duplicates_rejected': 0,\n",
    "            'contradictions_found': 0,\n",
    "            'validation_errors': 0\n",
    "        }\n",
    "        \n",
    "        # Contradiction rules - relations that shouldn't coexist\n",
    "        self.contradiction_rules = {\n",
    "            'Antonym': ['Synonym', 'RelatedTo'],\n",
    "            'Synonym': ['Antonym'],\n",
    "            'Causes': ['Prevents'],\n",
    "            'Prevents': ['Causes']\n",
    "        }\n",
    "        \n",
    "        print(\"üß† Knowledge Graph Agent initialized!\")\n",
    "        print(f\"   Validation: {'ON' if validate_on_add else 'OFF'}\")\n",
    "        print(f\"   Verbose mode: {'ON' if verbose else 'OFF'}\")\n",
    "    \n",
    "    def clean_conceptnet_data(self, df):\n",
    "        \"\"\"\n",
    "        Properly clean ConceptNet data - fixes the string splitting issue\n",
    "        \"\"\"\n",
    "        print(\"üßπ Cleaning ConceptNet data...\")\n",
    "        \n",
    "        def extract_concept(concept_string):\n",
    "            \"\"\"Extract clean concept from ConceptNet URI format\"\"\"\n",
    "            if pd.isna(concept_string):\n",
    "                return None\n",
    "            \n",
    "            # ConceptNet format: /c/en/concept_name/part_of_speech\n",
    "            # We want the concept_name part\n",
    "            parts = str(concept_string).split('/')\n",
    "            if len(parts) >= 4 and parts[1] == 'c' and parts[2] == 'en':\n",
    "                concept = parts[3]\n",
    "                # Handle underscores and clean up\n",
    "                concept = concept.replace('_', ' ')\n",
    "                return concept\n",
    "            return concept_string\n",
    "        \n",
    "        def extract_relation(relation_string):\n",
    "            \"\"\"Extract relation type from ConceptNet URI\"\"\"\n",
    "            if pd.isna(relation_string):\n",
    "                return None\n",
    "            parts = str(relation_string).split('/')\n",
    "            if len(parts) >= 3 and parts[1] == 'r':\n",
    "                return parts[2]\n",
    "            return relation_string\n",
    "        \n",
    "        def extract_weight(weight_string):\n",
    "            \"\"\"Extract numerical weight from JSON string\"\"\"\n",
    "            try:\n",
    "                weight_data = json.loads(weight_string)\n",
    "                return float(weight_data.get('weight', 1.0))\n",
    "            except:\n",
    "                return 1.0\n",
    "        \n",
    "        # Apply cleaning functions\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        print(\"   Extracting concepts and relations...\")\n",
    "        cleaned_df['start_concept'] = df['start'].apply(extract_concept)\n",
    "        cleaned_df['end_concept'] = df['end'].apply(extract_concept)\n",
    "        cleaned_df['relation_type'] = df['relation'].apply(extract_relation)\n",
    "        cleaned_df['edge_weight'] = df['weight'].apply(extract_weight)\n",
    "        \n",
    "        # Filter out invalid entries\n",
    "        initial_count = len(cleaned_df)\n",
    "        cleaned_df = cleaned_df.dropna(subset=['start_concept', 'end_concept', 'relation_type'])\n",
    "        final_count = len(cleaned_df)\n",
    "        \n",
    "        print(f\"   Filtered {initial_count - final_count:,} invalid entries\")\n",
    "        print(f\"   Clean dataset: {final_count:,} triples\")\n",
    "        \n",
    "        return cleaned_df[['start_concept', 'end_concept', 'relation_type', 'edge_weight']]\n",
    "    \n",
    "    def validate_triple(self, start, relation, end, weight=1.0):\n",
    "        \"\"\"\n",
    "        Validate a triple before adding to graph\n",
    "        Returns: (is_valid, reason)\n",
    "        \"\"\"\n",
    "        # Check for duplicates\n",
    "        if self.graph.has_edge(start, end):\n",
    "            existing_edges = self.graph[start][end]\n",
    "            for edge_data in existing_edges.values():\n",
    "                if edge_data.get('relation') == relation:\n",
    "                    return False, f\"Duplicate: {start} --{relation}--> {end}\"\n",
    "        \n",
    "        # Check for contradictions\n",
    "        if relation in self.contradiction_rules:\n",
    "            contradictory_relations = self.contradiction_rules[relation]\n",
    "            \n",
    "            if self.graph.has_edge(start, end):\n",
    "                for edge_data in self.graph[start][end].values():\n",
    "                    if edge_data.get('relation') in contradictory_relations:\n",
    "                        return False, f\"Contradiction: {start} already has {edge_data.get('relation')} with {end}\"\n",
    "        \n",
    "        # Passed all validation checks\n",
    "        return True, \"Valid\"\n",
    "    \n",
    "    def add_triple(self, start, relation, end, weight=1.0, force=False):\n",
    "        \"\"\"\n",
    "        Add a validated triple to the knowledge graph\n",
    "        \"\"\"\n",
    "        self.validation_stats['total_attempted'] += 1\n",
    "        \n",
    "        if not force and self.validate_on_add:\n",
    "            is_valid, reason = self.validate_triple(start, relation, end, weight)\n",
    "            \n",
    "            if not is_valid:\n",
    "                if \"Duplicate\" in reason:\n",
    "                    self.validation_stats['duplicates_rejected'] += 1\n",
    "                elif \"Contradiction\" in reason:\n",
    "                    self.validation_stats['contradictions_found'] += 1\n",
    "                else:\n",
    "                    self.validation_stats['validation_errors'] += 1\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"‚ùå Rejected: {reason}\")\n",
    "                return False\n",
    "        \n",
    "        # Add the triple to graph\n",
    "        self.graph.add_edge(start, end, relation=relation, weight=weight)\n",
    "        self.validation_stats['successful_adds'] += 1\n",
    "        \n",
    "        if self.verbose and self.validation_stats['total_attempted'] % 1000 == 0:\n",
    "            print(f\"‚úÖ Added {self.validation_stats['successful_adds']:,} triples so far...\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def bulk_load_triples(self, df, max_triples=None):\n",
    "        \"\"\"\n",
    "        Efficiently load multiple triples with progress tracking\n",
    "        \"\"\"\n",
    "        print(f\"üìä Loading triples into knowledge graph...\")\n",
    "        \n",
    "        if max_triples:\n",
    "            df = df.head(max_triples)\n",
    "            print(f\"   Limited to first {max_triples:,} triples\")\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        print(f\"   Processing {total_rows:,} triples...\")\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        for idx, row in tqdm(df.iterrows(), total=total_rows, desc=\"Loading triples\"):\n",
    "            self.add_triple(\n",
    "                start=str(row['start_concept']),\n",
    "                relation=str(row['relation_type']),\n",
    "                end=str(row['end_concept']),\n",
    "                weight=float(row['edge_weight'])\n",
    "            )\n",
    "        \n",
    "        self.print_stats()\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print comprehensive statistics about the knowledge graph\"\"\"\n",
    "        print(\"\\nüìà Knowledge Graph Statistics:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"üî¢ Total nodes: {self.graph.number_of_nodes():,}\")\n",
    "        print(f\"üîó Total edges: {self.graph.number_of_edges():,}\")\n",
    "        print(f\"üìä Average degree: {np.mean([d for n, d in self.graph.degree()]):,.2f}\")\n",
    "        \n",
    "        print(f\"\\nüîç Validation Results:\")\n",
    "        print(f\"   Attempted additions: {self.validation_stats['total_attempted']:,}\")\n",
    "        print(f\"   ‚úÖ Successful: {self.validation_stats['successful_adds']:,}\")\n",
    "        print(f\"   üîÑ Duplicates rejected: {self.validation_stats['duplicates_rejected']:,}\")\n",
    "        print(f\"   ‚ö° Contradictions found: {self.validation_stats['contradictions_found']:,}\")\n",
    "        print(f\"   ‚ùå Other errors: {self.validation_stats['validation_errors']:,}\")\n",
    "        \n",
    "        success_rate = (self.validation_stats['successful_adds'] / max(self.validation_stats['total_attempted'], 1)) * 100\n",
    "        print(f\"   üìä Success rate: {success_rate:.2f}%\")\n",
    "        \n",
    "        # Relation type distribution\n",
    "        relation_counts = Counter()\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            relation_counts[data.get('relation', 'Unknown')] += 1\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è  Top 10 Relation Types:\")\n",
    "        for relation, count in relation_counts.most_common(10):\n",
    "            print(f\"   {relation}: {count:,}\")\n",
    "    \n",
    "    def query_concept(self, concept, max_results=10):\n",
    "        \"\"\"\n",
    "        Query all relations for a given concept\n",
    "        \"\"\"\n",
    "        if concept not in self.graph:\n",
    "            print(f\"‚ùì Concept '{concept}' not found in graph\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"üîç Relations for '{concept}':\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Outgoing relations\n",
    "        for neighbor in list(self.graph.neighbors(concept))[:max_results//2]:\n",
    "            edge_data = self.graph[concept][neighbor]\n",
    "            for edge in edge_data.values():\n",
    "                relation = edge.get('relation', 'Unknown')\n",
    "                weight = edge.get('weight', 1.0)\n",
    "                results.append((concept, relation, neighbor, weight, 'outgoing'))\n",
    "                print(f\"   {concept} --{relation}--> {neighbor} (weight: {weight:.2f})\")\n",
    "        \n",
    "        # Incoming relations\n",
    "        for predecessor in list(self.graph.predecessors(concept))[:max_results//2]:\n",
    "            edge_data = self.graph[predecessor][concept]\n",
    "            for edge in edge_data.values():\n",
    "                relation = edge.get('relation', 'Unknown')\n",
    "                weight = edge.get('weight', 1.0)\n",
    "                results.append((predecessor, relation, concept, weight, 'incoming'))\n",
    "                print(f\"   {predecessor} --{relation}--> {concept} (weight: {weight:.2f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_path(self, start_concept, end_concept, max_length=3):\n",
    "        \"\"\"\n",
    "        Find connection paths between two concepts\n",
    "        \"\"\"\n",
    "        if start_concept not in self.graph or end_concept not in self.graph:\n",
    "            print(f\"‚ùì One or both concepts not found in graph\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Find shortest path\n",
    "            path = nx.shortest_path(self.graph, start_concept, end_concept, weight=None)\n",
    "            \n",
    "            print(f\"üõ§Ô∏è  Path from '{start_concept}' to '{end_concept}':\")\n",
    "            \n",
    "            # Print the path with relations\n",
    "            for i in range(len(path) - 1):\n",
    "                current = path[i]\n",
    "                next_node = path[i + 1]\n",
    "                \n",
    "                if self.graph.has_edge(current, next_node):\n",
    "                    edge_data = list(self.graph[current][next_node].values())[0]\n",
    "                    relation = edge_data.get('relation', 'Unknown')\n",
    "                    print(f\"   {current} --{relation}--> {next_node}\")\n",
    "                \n",
    "            return path\n",
    "            \n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"‚ùå No path found between '{start_concept}' and '{end_concept}'\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e63991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Knowledge Graph Agent initialized!\n",
      "   Validation: ON\n",
      "   Verbose mode: OFF\n",
      "üìù Loading sample data...\n",
      "üìä Loading triples into knowledge graph...\n",
      "   Processing 5 triples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading triples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 5002.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Knowledge Graph Statistics:\n",
      "==================================================\n",
      "üî¢ Total nodes: 6\n",
      "üîó Total edges: 5\n",
      "üìä Average degree: 1.67\n",
      "\n",
      "üîç Validation Results:\n",
      "   Attempted additions: 5\n",
      "   ‚úÖ Successful: 5\n",
      "   üîÑ Duplicates rejected: 0\n",
      "   ‚ö° Contradictions found: 0\n",
      "   ‚ùå Other errors: 0\n",
      "   üìä Success rate: 100.00%\n",
      "\n",
      "üè∑Ô∏è  Top 10 Relation Types:\n",
      "   IsA: 4\n",
      "   RelatedTo: 1\n",
      "\n",
      "üîç Testing queries...\n",
      "üîç Relations for 'dog':\n",
      "   dog --IsA--> animal (weight: 1.00)\n",
      "   dog --RelatedTo--> cat (weight: 0.80)\n",
      "\n",
      "üõ§Ô∏è  Testing path finding...\n",
      "‚ùå No path found between 'dog' and 'emotion'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize agent\n",
    "agent = KnowledgeGraphAgent(validate_on_add=True, verbose=False)\n",
    "\n",
    "# Test with sample data (replace with your actual cleaned data)\n",
    "sample_data = pd.DataFrame({\n",
    "    'start_concept': ['dog', 'cat', 'dog', 'happy', 'sad'],\n",
    "    'end_concept': ['animal', 'animal', 'cat', 'emotion', 'emotion'],\n",
    "    'relation_type': ['IsA', 'IsA', 'RelatedTo', 'IsA', 'IsA'],\n",
    "    'edge_weight': [1.0, 1.0, 0.8, 1.0, 1.0]\n",
    "})\n",
    "\n",
    "print(\"üìù Loading sample data...\")\n",
    "agent.bulk_load_triples(sample_data)\n",
    "\n",
    "print(\"\\nüîç Testing queries...\")\n",
    "agent.query_concept('dog')\n",
    "\n",
    "print(\"\\nüõ§Ô∏è  Testing path finding...\")\n",
    "agent.find_path('dog', 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0c673",
   "metadata": {},
   "source": [
    "#### Phase 3.A \n",
    "***\n",
    "Testing agent interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f26ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ TESTING THE AGENT\n",
      "============================================================\n",
      "\n",
      "üîç Test 1: Querying concept relationships\n",
      "\n",
      "--- Relationships for 'dog' ---\n",
      "üîç Relations for 'dog':\n",
      "   dog --IsA--> animal (weight: 1.00)\n",
      "   dog --RelatedTo--> cat (weight: 0.80)\n",
      "\n",
      "üõ§Ô∏è  Test 2: Finding concept paths\n",
      "\n",
      "Trying to find path from 'dog' to 'animal':\n",
      "üõ§Ô∏è  Path from 'dog' to 'animal':\n",
      "   dog --IsA--> animal\n",
      "\n",
      "üî¨ Test 3: Adding duplicate to test validation\n",
      "Attempting to add duplicate: dog --IsA--> animal\n",
      "Duplicate addition was rejected (as expected)\n",
      "\n",
      "üíæ Saving processed data...\n",
      "   Saved to: ../Data/Input\\conceptnet_en_processed_for_graph.parquet.gzip\n",
      "\n",
      "üéâ Phase 2 Complete!\n",
      "   Graph loaded with 5 validated triples\n",
      "   Ready for agent interactions and queries!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ TESTING THE AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Query specific concepts\n",
    "print(\"\\nüîç Test 1: Querying concept relationships\")\n",
    "test_concepts = ['dog', 'cat', 'animal', 'happy', 'food']\n",
    "\n",
    "for concept in test_concepts:\n",
    "    if concept in [str(c).lower() for c in sample_data['start_concept'].unique()]:\n",
    "        print(f\"\\n--- Relationships for '{concept}' ---\")\n",
    "        agent.query_concept(concept, max_results=5)\n",
    "        break\n",
    "\n",
    "# Test 2: Find paths between concepts\n",
    "print(\"\\nüõ§Ô∏è  Test 2: Finding concept paths\")\n",
    "# Try to find a path between two concepts\n",
    "start_concepts = sample_data['start_concept'].value_counts().head(5).index.tolist()\n",
    "end_concepts = sample_data['end_concept'].value_counts().head(5).index.tolist()\n",
    "\n",
    "if len(start_concepts) > 0 and len(end_concepts) > 0:\n",
    "    start_test = str(start_concepts[0])\n",
    "    end_test = str(end_concepts[0])\n",
    "    print(f\"\\nTrying to find path from '{start_test}' to '{end_test}':\")\n",
    "    agent.find_path(start_test, end_test)\n",
    "\n",
    "# Test 3: Validation effectiveness\n",
    "print(\"\\nüî¨ Test 3: Adding duplicate to test validation\")\n",
    "if len(sample_data) > 0:\n",
    "    first_row = sample_data.iloc[0]\n",
    "    print(f\"Attempting to add duplicate: {first_row['start_concept']} --{first_row['relation_type']}--> {first_row['end_concept']}\")\n",
    "    \n",
    "    success = agent.add_triple(\n",
    "        start=str(first_row['start_concept']),\n",
    "        relation=str(first_row['relation_type']),\n",
    "        end=str(first_row['end_concept']),\n",
    "        weight=float(first_row['edge_weight'])\n",
    "    )\n",
    "    \n",
    "    print(f\"Duplicate addition {'succeeded' if success else 'was rejected (as expected)'}\")\n",
    "\n",
    "# Save the processed data for future use\n",
    "print(f\"\\nüíæ Saving processed data...\")\n",
    "output_path = os.path.join(os.path.dirname(EN_PATH), 'conceptnet_en_processed_for_graph.parquet.gzip')\n",
    "cleaned_english_triples.to_parquet(output_path, index=False, compression='gzip')\n",
    "print(f\"   Saved to: {output_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Phase 2 Complete!\")\n",
    "print(f\"   Graph loaded with {agent.validation_stats['successful_adds']:,} validated triples\")\n",
    "print(f\"   Ready for agent interactions and queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7d64e",
   "metadata": {},
   "source": [
    "# Phase 4 \n",
    "***\n",
    "Testing if the agent successfully integrates the stratidfied seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38489d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing Knowledge Graph Agent with stratified seed...\n",
      "üß† Knowledge Graph Agent initialized!\n",
      "   Validation: ON\n",
      "   Verbose mode: OFF\n",
      "üìä Loading 5,000 stratified triples...\n",
      "üìä Loading triples into knowledge graph...\n",
      "   Processing 5,000 triples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading triples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 44993.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Knowledge Graph Statistics:\n",
      "==================================================\n",
      "üî¢ Total nodes: 4,640\n",
      "üîó Total edges: 4,956\n",
      "üìä Average degree: 2.14\n",
      "\n",
      "üîç Validation Results:\n",
      "   Attempted additions: 5,000\n",
      "   ‚úÖ Successful: 4,956\n",
      "   üîÑ Duplicates rejected: 42\n",
      "   ‚ö° Contradictions found: 2\n",
      "   ‚ùå Other errors: 0\n",
      "   üìä Success rate: 99.12%\n",
      "\n",
      "üè∑Ô∏è  Top 10 Relation Types:\n",
      "   RelatedTo: 1,414\n",
      "   DerivedFrom: 1,097\n",
      "   FormOf: 991\n",
      "   Synonym: 345\n",
      "   IsA: 221\n",
      "   UsedFor: 133\n",
      "   AtLocation: 93\n",
      "   HasSubevent: 85\n",
      "   HasPrerequisite: 76\n",
      "   CapableOf: 76\n",
      "\n",
      "============================================================\n",
      "üéØ TESTING AGENT WITH STRATIFIED SEED\n",
      "============================================================\n",
      "\n",
      "üìä Relation diversity in loaded graph:\n",
      "Top 10 relations in graph:\n",
      "   RelatedTo: 1,414\n",
      "   DerivedFrom: 1,097\n",
      "   FormOf: 991\n",
      "   Synonym: 345\n",
      "   IsA: 221\n",
      "   UsedFor: 133\n",
      "   AtLocation: 93\n",
      "   HasSubevent: 85\n",
      "   HasPrerequisite: 76\n",
      "   CapableOf: 76\n",
      "\n",
      "‚öñÔ∏è  High-quality relationships (weight > 0.8):\n",
      "   antichrist --DerivedFrom--> christ (weight: 2.000)\n",
      "   n --FormOf--> dixie_cup (weight: 2.828)\n",
      "   n --FormOf--> pop (weight: 2.828)\n",
      "   n --FormOf--> babysitting (weight: 2.000)\n",
      "   n --Synonym--> blackboard (weight: 2.828)\n",
      "   n --FormOf--> body_part (weight: 2.000)\n",
      "   n --FormOf--> bafta (weight: 2.000)\n",
      "   n --FormOf--> spidey_sense (weight: 4.472)\n",
      "   n --Synonym--> heaven (weight: 2.828)\n",
      "   n --Synonym--> advert (weight: 2.000)\n",
      "Total high-quality relationships: 10\n",
      "\n",
      "üï∏Ô∏è  Connectivity analysis:\n",
      "Most connected concepts:\n",
      "   n: 1523 connections\n",
      "   v: 264 connections\n",
      "   a: 232 connections\n",
      "   water: 32 connections\n",
      "   person: 32 connections\n",
      "   food: 28 connections\n",
      "   animal: 24 connections\n",
      "   r: 24 connections\n",
      "   en_1: 23 connections\n",
      "   wn: 22 connections\n",
      "\n",
      "üîç Testing queries on top concepts:\n",
      "\n",
      "--- Relationships for 'n' (degree: 1523) ---\n",
      "üîç Relations for 'n':\n",
      "   n --FormOf--> dixie_cup (weight: 2.83)\n",
      "   n --FormOf--> pop (weight: 2.83)\n",
      "   pork_pie --DerivedFrom--> n (weight: 2.00)\n",
      "   bladelet --DerivedFrom--> n (weight: 2.00)\n",
      "\n",
      "--- Relationships for 'v' (degree: 264) ---\n",
      "üîç Relations for 'v':\n",
      "   v --Synonym--> expect (weight: 2.00)\n",
      "   v --Synonym--> bear (weight: 2.83)\n",
      "   re_emerge --DerivedFrom--> v (weight: 2.00)\n",
      "   inhabitable --DerivedFrom--> v (weight: 2.00)\n",
      "\n",
      "--- Relationships for 'a' (degree: 232) ---\n",
      "üîç Relations for 'a':\n",
      "   a --Synonym--> compulsory (weight: 2.83)\n",
      "   a --Synonym--> adaptive (weight: 2.00)\n",
      "   realtimely --DerivedFrom--> a (weight: 2.00)\n",
      "   cytostatically --DerivedFrom--> a (weight: 2.00)\n",
      "\n",
      "üéâ Agent successfully initialized with high-quality stratified seed!\n",
      "   Ready for knowledge graph reasoning and expansion!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üß† Initializing Knowledge Graph Agent with stratified seed...\")\n",
    "agent = KnowledgeGraphAgent(validate_on_add=True, verbose=False)\n",
    "\n",
    "# Load the stratified sample\n",
    "print(f\"üìä Loading {len(stratified_seed_data):,} stratified triples...\")\n",
    "agent.bulk_load_triples(stratified_seed_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ TESTING AGENT WITH STRATIFIED SEED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Show relation diversity\n",
    "print(\"\\nüìä Relation diversity in loaded graph:\")\n",
    "relation_stats = {}\n",
    "for _, _, data in agent.graph.edges(data=True):\n",
    "    relation = data.get('relation', 'Unknown')\n",
    "    if relation not in relation_stats:\n",
    "        relation_stats[relation] = 0\n",
    "    relation_stats[relation] += 1\n",
    "\n",
    "# Show top relations\n",
    "sorted_relations = sorted(relation_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 relations in graph:\")\n",
    "for relation, count in sorted_relations[:10]:\n",
    "    print(f\"   {relation}: {count:,}\")\n",
    "\n",
    "# Test 2: Quality check - show high-weight concepts\n",
    "print(f\"\\n‚öñÔ∏è  High-quality relationships (weight > 0.8):\")\n",
    "high_quality_count = 0\n",
    "for start, end, data in agent.graph.edges(data=True):\n",
    "    if data.get('weight', 0) > 0.8:\n",
    "        relation = data.get('relation', 'Unknown')\n",
    "        weight = data.get('weight', 0)\n",
    "        print(f\"   {start} --{relation}--> {end} (weight: {weight:.3f})\")\n",
    "        high_quality_count += 1\n",
    "        if high_quality_count >= 10:  # Limit output\n",
    "            break\n",
    "\n",
    "print(f\"Total high-quality relationships: {high_quality_count:,}\")\n",
    "\n",
    "# Test 3: Concept connectivity analysis\n",
    "print(f\"\\nüï∏Ô∏è  Connectivity analysis:\")\n",
    "node_degrees = dict(agent.graph.degree())\n",
    "top_connected = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Most connected concepts:\")\n",
    "for concept, degree in top_connected:\n",
    "    print(f\"   {concept}: {degree} connections\")\n",
    "\n",
    "# Test 4: Sample queries on well-connected concepts\n",
    "print(f\"\\nüîç Testing queries on top concepts:\")\n",
    "for concept, degree in top_connected[:3]:\n",
    "    print(f\"\\n--- Relationships for '{concept}' (degree: {degree}) ---\")\n",
    "    agent.query_concept(concept, max_results=5)\n",
    "\n",
    "print(f\"\\nüéâ Agent successfully initialized with high-quality stratified seed!\")\n",
    "print(f\"   Ready for knowledge graph reasoning and expansion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5a59f",
   "metadata": {},
   "source": [
    "# Phase 5 \n",
    "***\n",
    "Test if the Agent memory graph is now interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab0445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing MVP Knowledge Agent...\n",
      "üöÄ MVP Knowledge Agent initialized!\n",
      "   Base knowledge: 4,640 concepts\n",
      "   Relationships: 4,956 edges\n",
      "\n",
      "============================================================\n",
      "üéØ MVP AGENT READY - TESTING CORE FEATURES\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Testing Concept Exploration:\n",
      "üîç EXPLORING: 'dog' (depth: 2)\n",
      "==================================================\n",
      "\n",
      "üìç DIRECT RELATIONSHIPS:\n",
      "   dog --IsA--> canine (w: 4.90)\n",
      "   dog --CapableOf--> guard_house (w: 10.39)\n",
      "   dog --CapableOf--> bark (w: 16.00)\n",
      "   dog --AtLocation--> kennel (w: 9.38)\n",
      "   dog --Antonym--> cat (w: 3.69)\n",
      "   dog --Desires--> petted (w: 4.90)\n",
      "   dog --IsA--> mammal (w: 5.29)\n",
      "   dog --CapableOf--> run (w: 6.00)\n",
      "   dog --IsA--> loyal_friend (w: 6.63)\n",
      "   dog --IsA--> pet (w: 6.00)\n",
      "\n",
      "üîÑ EXTENDED RELATIONSHIPS (depth 2):\n",
      "   dog --CapableOf--> bark --PartOf--> tree\n",
      "   dog --Antonym--> cat --CapableOf--> hunt_mice\n",
      "   dog --Antonym--> cat --RelatedTo--> feline\n",
      "   dog --Antonym--> cat --AtLocation--> lap\n",
      "\n",
      "2Ô∏è‚É£ Testing Relationship Reasoning:\n",
      "üß† REASONING: Why might 'dog' and 'animal' be related?\n",
      "============================================================\n",
      "‚úÖ DIRECT CONNECTION FOUND:\n",
      "   dog --RelatedTo--> animal (weight: 9.410)\n",
      "\n",
      "üõ§Ô∏è  REASONING PATHS FOUND (3):\n",
      "\n",
      "   Path 1 (strength: 4.784):\n",
      "      dog --Antonym--> cat\n",
      "      cat --RelatedTo--> animal\n",
      "\n",
      "   Path 2 (strength: 4.784):\n",
      "      dog --Antonym--> cat\n",
      "      cat --RelatedTo--> animal\n",
      "\n",
      "   Path 3 (strength: 9.410):\n",
      "      dog --RelatedTo--> animal\n",
      "\n",
      "ü§ù SHARED CONNECTIONS (2):\n",
      "   Both relate to: 'animal'\n",
      "   Both relate to: 'cat'\n",
      "\n",
      "üìä RELATIONSHIP STRENGTH: STRONG (9.410)\n",
      "\n",
      "3Ô∏è‚É£ Testing Knowledge Validation:\n",
      "üî¨ VALIDATING: dog --IsA--> mammal\n",
      "==================================================\n",
      "‚úÖ Know about 'dog'\n",
      "‚úÖ Know about 'mammal'\n",
      "‚úÖ 'dog' commonly uses 'IsA' relation\n",
      "\n",
      "üìä VALIDATION SCORE: 0.970\n",
      "‚úÖ RECOMMENDATION: ACCEPT\n",
      "\n",
      "4Ô∏è‚É£ Testing Learning:\n",
      "üß† LEARNING: dog --CapableOf--> barking\n",
      "üî¨ VALIDATING: dog --CapableOf--> barking\n",
      "==================================================\n",
      "‚úÖ Know about 'dog'\n",
      "‚ùì Unknown concept: 'barking'\n",
      "‚úÖ 'dog' commonly uses 'CapableOf' relation\n",
      "\n",
      "üìä VALIDATION SCORE: 0.835\n",
      "‚úÖ RECOMMENDATION: ACCEPT\n",
      "‚úÖ LEARNED: Successfully integrated new knowledge\n",
      "   Knowledge base now has 4,957 relationships\n",
      "\n",
      "üéâ MVP Agent is ready! All core features working.\n",
      "üìä Current knowledge: 4,641 concepts, 4,957 relationships\n",
      "üß† Learning events: 1\n"
     ]
    }
   ],
   "source": [
    "class MVPKnowledgeAgent:\n",
    "    \"\"\"\n",
    "    Minimum Viable Product - Interactive Knowledge Graph Agent\n",
    "    Core features: Explore, Reason, Validate, Learn\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_agent):\n",
    "        self.graph = base_agent.graph\n",
    "        self.base_agent = base_agent\n",
    "        self.reasoning_cache = {}  # Cache for expensive operations\n",
    "        self.learning_log = []     # Track what the agent learns\n",
    "        \n",
    "        print(\"üöÄ MVP Knowledge Agent initialized!\")\n",
    "        print(f\"   Base knowledge: {self.graph.number_of_nodes():,} concepts\")\n",
    "        print(f\"   Relationships: {self.graph.number_of_edges():,} edges\")\n",
    "    \n",
    "    def explore_concept(self, concept, depth=2, max_results=20):\n",
    "        \"\"\"\n",
    "        Core MVP Feature 1: Deep concept exploration\n",
    "        Shows not just direct relationships, but relationships of relationships\n",
    "        \"\"\"\n",
    "        print(f\"üîç EXPLORING: '{concept}' (depth: {depth})\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if concept not in self.graph:\n",
    "            # Try fuzzy matching\n",
    "            similar = self._find_similar_concepts(concept)\n",
    "            if similar:\n",
    "                print(f\"‚ùì '{concept}' not found. Did you mean: {', '.join(similar[:3])}?\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"‚ùå '{concept}' not found in knowledge base\")\n",
    "                return None\n",
    "        \n",
    "        exploration_results = {\n",
    "            'target_concept': concept,\n",
    "            'direct_relations': [],\n",
    "            'extended_relations': [],\n",
    "            'concept_clusters': [],\n",
    "            'reasoning_paths': []\n",
    "        }\n",
    "        \n",
    "        # Level 1: Direct relationships\n",
    "        print(f\"\\nüìç DIRECT RELATIONSHIPS:\")\n",
    "        direct_count = 0\n",
    "        for neighbor in self.graph.neighbors(concept):\n",
    "            if direct_count >= max_results // 2:\n",
    "                break\n",
    "                \n",
    "            edge_data = list(self.graph[concept][neighbor].values())[0]\n",
    "            relation = edge_data.get('relation', 'Unknown')\n",
    "            weight = edge_data.get('weight', 1.0)\n",
    "            \n",
    "            exploration_results['direct_relations'].append({\n",
    "                'from': concept,\n",
    "                'relation': relation,\n",
    "                'to': neighbor,\n",
    "                'weight': weight,\n",
    "                'type': 'outgoing'\n",
    "            })\n",
    "            \n",
    "            print(f\"   {concept} --{relation}--> {neighbor} (w: {weight:.2f})\")\n",
    "            direct_count += 1\n",
    "        \n",
    "        # Include incoming relationships\n",
    "        for predecessor in self.graph.predecessors(concept):\n",
    "            if direct_count >= max_results // 2:\n",
    "                break\n",
    "                \n",
    "            edge_data = list(self.graph[predecessor][concept].values())[0]\n",
    "            relation = edge_data.get('relation', 'Unknown')\n",
    "            weight = edge_data.get('weight', 1.0)\n",
    "            \n",
    "            exploration_results['direct_relations'].append({\n",
    "                'from': predecessor,\n",
    "                'relation': relation,\n",
    "                'to': concept,\n",
    "                'weight': weight,\n",
    "                'type': 'incoming'\n",
    "            })\n",
    "            \n",
    "            print(f\"   {predecessor} --{relation}--> {concept} (w: {weight:.2f})\")\n",
    "            direct_count += 1\n",
    "        \n",
    "        # Level 2: Extended exploration (relationships of relationships)\n",
    "        if depth > 1:\n",
    "            print(f\"\\nüîÑ EXTENDED RELATIONSHIPS (depth 2):\")\n",
    "            extended_concepts = set()\n",
    "            \n",
    "            # Get neighbors of neighbors\n",
    "            for neighbor in list(self.graph.neighbors(concept))[:5]:  # Limit to prevent explosion\n",
    "                for second_neighbor in list(self.graph.neighbors(neighbor))[:3]:\n",
    "                    if second_neighbor != concept and second_neighbor not in extended_concepts:\n",
    "                        extended_concepts.add(second_neighbor)\n",
    "                        \n",
    "                        # Get the relation chain\n",
    "                        edge1 = list(self.graph[concept][neighbor].values())[0]\n",
    "                        edge2 = list(self.graph[neighbor][second_neighbor].values())[0]\n",
    "                        \n",
    "                        relation1 = edge1.get('relation', 'Unknown')\n",
    "                        relation2 = edge2.get('relation', 'Unknown')\n",
    "                        \n",
    "                        exploration_results['extended_relations'].append({\n",
    "                            'path': [concept, neighbor, second_neighbor],\n",
    "                            'relations': [relation1, relation2],\n",
    "                            'reasoning': f\"{concept} --{relation1}--> {neighbor} --{relation2}--> {second_neighbor}\"\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"   {concept} --{relation1}--> {neighbor} --{relation2}--> {second_neighbor}\")\n",
    "                        \n",
    "                        if len(extended_concepts) >= max_results // 4:\n",
    "                            break\n",
    "                if len(extended_concepts) >= max_results // 4:\n",
    "                    break\n",
    "        \n",
    "        return exploration_results\n",
    "    \n",
    "    def reason_about_relationship(self, concept1, concept2, max_paths=3):\n",
    "        \"\"\"\n",
    "        Core MVP Feature 2: Reasoning about why two concepts might be related\n",
    "        \"\"\"\n",
    "        print(f\"üß† REASONING: Why might '{concept1}' and '{concept2}' be related?\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if concept1 not in self.graph or concept2 not in self.graph:\n",
    "            missing = [c for c in [concept1, concept2] if c not in self.graph]\n",
    "            print(f\"‚ùå Concepts not found: {missing}\")\n",
    "            return None\n",
    "        \n",
    "        reasoning_results = {\n",
    "            'concept1': concept1,\n",
    "            'concept2': concept2,\n",
    "            'direct_connection': None,\n",
    "            'reasoning_paths': [],\n",
    "            'shared_concepts': [],\n",
    "            'relationship_strength': 0.0\n",
    "        }\n",
    "        \n",
    "        # Check for direct connection\n",
    "        if self.graph.has_edge(concept1, concept2):\n",
    "            edge_data = list(self.graph[concept1][concept2].values())[0]\n",
    "            relation = edge_data.get('relation', 'Unknown')\n",
    "            weight = edge_data.get('weight', 1.0)\n",
    "            \n",
    "            reasoning_results['direct_connection'] = {\n",
    "                'relation': relation,\n",
    "                'weight': weight\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ DIRECT CONNECTION FOUND:\")\n",
    "            print(f\"   {concept1} --{relation}--> {concept2} (weight: {weight:.3f})\")\n",
    "            reasoning_results['relationship_strength'] = weight\n",
    "        \n",
    "        # Find indirect reasoning paths\n",
    "        try:\n",
    "            paths = list(nx.all_simple_paths(self.graph, concept1, concept2, cutoff=3))[:max_paths]\n",
    "            \n",
    "            if paths:\n",
    "                print(f\"\\nüõ§Ô∏è  REASONING PATHS FOUND ({len(paths)}):\")\n",
    "                \n",
    "                for i, path in enumerate(paths, 1):\n",
    "                    path_relations = []\n",
    "                    path_weights = []\n",
    "                    path_description = []\n",
    "                    \n",
    "                    for j in range(len(path) - 1):\n",
    "                        current = path[j]\n",
    "                        next_node = path[j + 1]\n",
    "                        \n",
    "                        if self.graph.has_edge(current, next_node):\n",
    "                            edge_data = list(self.graph[current][next_node].values())[0]\n",
    "                            relation = edge_data.get('relation', 'Unknown')\n",
    "                            weight = edge_data.get('weight', 1.0)\n",
    "                            \n",
    "                            path_relations.append(relation)\n",
    "                            path_weights.append(weight)\n",
    "                            path_description.append(f\"{current} --{relation}--> {next_node}\")\n",
    "                    \n",
    "                    avg_weight = np.mean(path_weights) if path_weights else 0.0\n",
    "                    \n",
    "                    reasoning_results['reasoning_paths'].append({\n",
    "                        'path': path,\n",
    "                        'relations': path_relations,\n",
    "                        'avg_weight': avg_weight,\n",
    "                        'description': ' ‚Üí '.join(path_description)\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"\\n   Path {i} (strength: {avg_weight:.3f}):\")\n",
    "                    for desc in path_description:\n",
    "                        print(f\"      {desc}\")\n",
    "                    \n",
    "                    # Update relationship strength\n",
    "                    if avg_weight > reasoning_results['relationship_strength']:\n",
    "                        reasoning_results['relationship_strength'] = avg_weight\n",
    "        \n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"‚ùå No reasoning paths found between '{concept1}' and '{concept2}'\")\n",
    "        \n",
    "        # Find shared connections (concepts both are related to)\n",
    "        concept1_neighbors = set(self.graph.neighbors(concept1)) | set(self.graph.predecessors(concept1))\n",
    "        concept2_neighbors = set(self.graph.neighbors(concept2)) | set(self.graph.predecessors(concept2))\n",
    "        shared = concept1_neighbors & concept2_neighbors\n",
    "        \n",
    "        if shared:\n",
    "            print(f\"\\nü§ù SHARED CONNECTIONS ({len(shared)}):\")\n",
    "            for shared_concept in list(shared)[:5]:  # Limit output\n",
    "                reasoning_results['shared_concepts'].append(shared_concept)\n",
    "                print(f\"   Both relate to: '{shared_concept}'\")\n",
    "        \n",
    "        # Generate reasoning strength score\n",
    "        strength_score = reasoning_results['relationship_strength']\n",
    "        if strength_score > 0.8:\n",
    "            strength_desc = \"STRONG\"\n",
    "        elif strength_score > 0.5:\n",
    "            strength_desc = \"MODERATE\"\n",
    "        elif strength_score > 0.2:\n",
    "            strength_desc = \"WEAK\"\n",
    "        else:\n",
    "            strength_desc = \"MINIMAL\"\n",
    "        \n",
    "        print(f\"\\nüìä RELATIONSHIP STRENGTH: {strength_desc} ({strength_score:.3f})\")\n",
    "        \n",
    "        return reasoning_results\n",
    "    \n",
    "    def validate_new_knowledge(self, start_concept, relation, end_concept, confidence=1.0):\n",
    "        \"\"\"\n",
    "        Core MVP Feature 3: Validate if new knowledge makes sense\n",
    "        \"\"\"\n",
    "        print(f\"üî¨ VALIDATING: {start_concept} --{relation}--> {end_concept}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        validation_result = {\n",
    "            'proposed_triple': (start_concept, relation, end_concept),\n",
    "            'confidence': confidence,\n",
    "            'validation_score': 0.0,\n",
    "            'supporting_evidence': [],\n",
    "            'contradictions': [],\n",
    "            'recommendation': 'REJECT'\n",
    "        }\n",
    "        \n",
    "        # Check 1: Do the concepts exist in our knowledge base?\n",
    "        concept_familiarity = 0\n",
    "        if start_concept in self.graph:\n",
    "            concept_familiarity += 0.5\n",
    "            print(f\"‚úÖ Know about '{start_concept}'\")\n",
    "        else:\n",
    "            print(f\"‚ùì Unknown concept: '{start_concept}'\")\n",
    "        \n",
    "        if end_concept in self.graph:\n",
    "            concept_familiarity += 0.5\n",
    "            print(f\"‚úÖ Know about '{end_concept}'\")\n",
    "        else:\n",
    "            print(f\"‚ùì Unknown concept: '{end_concept}'\")\n",
    "        \n",
    "        # Check 2: Look for supporting evidence\n",
    "        if start_concept in self.graph:\n",
    "            start_relations = [data.get('relation') for _, _, data in self.graph.edges(start_concept, data=True)]\n",
    "            if relation in start_relations:\n",
    "                validation_result['supporting_evidence'].append(f\"'{start_concept}' commonly uses '{relation}' relation\")\n",
    "                print(f\"‚úÖ '{start_concept}' commonly uses '{relation}' relation\")\n",
    "        \n",
    "        # Check 3: Look for contradictions\n",
    "        if self.graph.has_edge(start_concept, end_concept):\n",
    "            existing_relations = [data.get('relation') for data in self.graph[start_concept][end_concept].values()]\n",
    "            contradictory_relations = self.base_agent.contradiction_rules.get(relation, [])\n",
    "            \n",
    "            for existing_rel in existing_relations:\n",
    "                if existing_rel in contradictory_relations:\n",
    "                    validation_result['contradictions'].append(f\"Contradicts existing '{existing_rel}' relation\")\n",
    "                    print(f\"‚ö†Ô∏è  Contradicts existing '{existing_rel}' relation\")\n",
    "        \n",
    "        # Calculate validation score\n",
    "        score = 0.0\n",
    "        score += concept_familiarity * 0.3  # 30% for concept familiarity\n",
    "        score += len(validation_result['supporting_evidence']) * 0.4  # 40% for supporting evidence\n",
    "        score -= len(validation_result['contradictions']) * 0.5  # Penalty for contradictions\n",
    "        score += confidence * 0.3  # 30% for stated confidence\n",
    "        \n",
    "        validation_result['validation_score'] = max(0.0, min(1.0, score))  # Clamp to [0,1]\n",
    "        \n",
    "        # Make recommendation\n",
    "        if validation_result['validation_score'] > 0.7:\n",
    "            validation_result['recommendation'] = 'ACCEPT'\n",
    "            recommendation_color = \"‚úÖ\"\n",
    "        elif validation_result['validation_score'] > 0.4:\n",
    "            validation_result['recommendation'] = 'REVIEW'\n",
    "            recommendation_color = \"‚ö†Ô∏è \"\n",
    "        else:\n",
    "            validation_result['recommendation'] = 'REJECT'\n",
    "            recommendation_color = \"‚ùå\"\n",
    "        \n",
    "        print(f\"\\nüìä VALIDATION SCORE: {validation_result['validation_score']:.3f}\")\n",
    "        print(f\"{recommendation_color} RECOMMENDATION: {validation_result['recommendation']}\")\n",
    "        \n",
    "        return validation_result\n",
    "    \n",
    "    def learn_new_triple(self, start_concept, relation, end_concept, confidence=1.0, force=False):\n",
    "        \"\"\"\n",
    "        Core MVP Feature 4: Learn and integrate new knowledge\n",
    "        \"\"\"\n",
    "        print(f\"üß† LEARNING: {start_concept} --{relation}--> {end_concept}\")\n",
    "        \n",
    "        # First validate the knowledge\n",
    "        validation = self.validate_new_knowledge(start_concept, relation, end_concept, confidence)\n",
    "        \n",
    "        should_learn = force or validation['recommendation'] in ['ACCEPT', 'REVIEW']\n",
    "        \n",
    "        if should_learn:\n",
    "            # Add to graph\n",
    "            success = self.base_agent.add_triple(start_concept, relation, end_concept, confidence)\n",
    "            \n",
    "            if success:\n",
    "                # Log the learning event\n",
    "                learning_event = {\n",
    "                    'timestamp': pd.Timestamp.now(),\n",
    "                    'triple': (start_concept, relation, end_concept),\n",
    "                    'confidence': confidence,\n",
    "                    'validation_score': validation['validation_score'],\n",
    "                    'method': 'forced' if force else 'validated'\n",
    "                }\n",
    "                self.learning_log.append(learning_event)\n",
    "                \n",
    "                print(f\"‚úÖ LEARNED: Successfully integrated new knowledge\")\n",
    "                print(f\"   Knowledge base now has {self.graph.number_of_edges():,} relationships\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: Could not integrate (duplicate or error)\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå REJECTED: Validation score too low ({validation['validation_score']:.3f})\")\n",
    "            return False\n",
    "    \n",
    "    def _find_similar_concepts(self, concept, threshold=0.7):\n",
    "        \"\"\"Helper: Find concepts similar to the input (simple string matching)\"\"\"\n",
    "        concept_lower = concept.lower()\n",
    "        similar = []\n",
    "        \n",
    "        for node in self.graph.nodes():\n",
    "            node_lower = str(node).lower()\n",
    "            if concept_lower in node_lower or node_lower in concept_lower:\n",
    "                similar.append(str(node))\n",
    "            if len(similar) >= 5:\n",
    "                break\n",
    "        \n",
    "        return similar\n",
    "    \n",
    "    def interactive_session(self):\n",
    "        \"\"\"\n",
    "        Core MVP Feature 5: Interactive exploration session\n",
    "        \"\"\"\n",
    "        print(\"üéÆ Starting Interactive Knowledge Exploration Session!\")\n",
    "        print(\"Commands: explore <concept>, reason <concept1> <concept2>, validate <start> <relation> <end>, learn <start> <relation> <end>, quit\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nü§ñ mvp_agent> \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"üëã Goodbye! Thanks for exploring knowledge with me!\")\n",
    "                    break\n",
    "                \n",
    "                parts = user_input.split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                \n",
    "                command = parts[0].lower()\n",
    "                \n",
    "                if command == 'explore' and len(parts) >= 2:\n",
    "                    concept = ' '.join(parts[1:])\n",
    "                    self.explore_concept(concept)\n",
    "                \n",
    "                elif command == 'reason' and len(parts) >= 3:\n",
    "                    concept1 = parts[1]\n",
    "                    concept2 = ' '.join(parts[2:])\n",
    "                    self.reason_about_relationship(concept1, concept2)\n",
    "                \n",
    "                elif command == 'validate' and len(parts) >= 4:\n",
    "                    start = parts[1]\n",
    "                    relation = parts[2]\n",
    "                    end = ' '.join(parts[3:])\n",
    "                    self.validate_new_knowledge(start, relation, end)\n",
    "                \n",
    "                elif command == 'learn' and len(parts) >= 4:\n",
    "                    start = parts[1]\n",
    "                    relation = parts[2]\n",
    "                    end = ' '.join(parts[3:])\n",
    "                    self.learn_new_triple(start, relation, end)\n",
    "                \n",
    "                else:\n",
    "                    print(\"‚ùì Unknown command. Try: explore <concept>, reason <concept1> <concept2>, validate/learn <start> <relation> <end>, quit\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Session interrupted. Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Initialize the MVP Agent\n",
    "print(\"üöÄ Initializing MVP Knowledge Agent...\")\n",
    "mvp_agent = MVPKnowledgeAgent(agent)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ MVP AGENT READY - TESTING CORE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test the core features\n",
    "print(\"\\n1Ô∏è‚É£ Testing Concept Exploration:\")\n",
    "mvp_agent.explore_concept('dog', depth=2)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Testing Relationship Reasoning:\")\n",
    "mvp_agent.reason_about_relationship('dog', 'animal')\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Testing Knowledge Validation:\")\n",
    "mvp_agent.validate_new_knowledge('dog', 'IsA', 'mammal', confidence=0.9)\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Testing Learning:\")\n",
    "mvp_agent.learn_new_triple('dog', 'CapableOf', 'barking', confidence=0.95)\n",
    "\n",
    "print(f\"\\nüéâ MVP Agent is ready! All core features working.\")\n",
    "print(f\"üìä Current knowledge: {mvp_agent.graph.number_of_nodes():,} concepts, {mvp_agent.graph.number_of_edges():,} relationships\")\n",
    "print(f\"üß† Learning events: {len(mvp_agent.learning_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44b4c1",
   "metadata": {},
   "source": [
    "# Phase 6: Full learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a5d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Training Loop for Self-Improving Knowledge Graph Agent\n",
    "class AdaptiveTrainingLoop:\n",
    "    \"\"\"\n",
    "    Self-improving training loop that processes the entire dataset until convergence\n",
    "    Uses adaptive batching, quality thresholds, and convergence detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mvp_agent, full_dataset, config=None):\n",
    "        self.agent = mvp_agent\n",
    "        self.full_dataset = full_dataset.copy()\n",
    "        self.training_history = []\n",
    "        self.convergence_metrics = deque(maxlen=10)  # Rolling window for convergence\n",
    "        \n",
    "        # Training configuration\n",
    "        self.config = config or {\n",
    "            'initial_batch_size': 1000,\n",
    "            'max_batch_size': 10000,\n",
    "            'min_batch_size': 100,\n",
    "            'quality_threshold_start': 0.3,\n",
    "            'quality_threshold_end': 0.7,\n",
    "            'convergence_patience': 3,\n",
    "            'max_epochs': 20,\n",
    "            'validation_sample_size': 500,\n",
    "            'adaptive_threshold': True,\n",
    "            'learning_rate_decay': 0.95\n",
    "        }\n",
    "        \n",
    "        self.current_epoch = 0\n",
    "        self.current_batch_size = self.config['initial_batch_size']\n",
    "        self.current_quality_threshold = self.config['quality_threshold_start']\n",
    "        \n",
    "        print(\"üéØ Adaptive Training Loop Initialized!\")\n",
    "        print(f\"   Dataset size: {len(self.full_dataset):,} triples\")\n",
    "        print(f\"   Starting batch size: {self.current_batch_size:,}\")\n",
    "        print(f\"   Quality threshold: {self.current_quality_threshold:.3f}\")\n",
    "    \n",
    "    def calculate_graph_quality_metrics(self):\n",
    "        \"\"\"Calculate comprehensive quality metrics for the current graph state\"\"\"\n",
    "        graph = self.agent.graph\n",
    "        \n",
    "        metrics = {\n",
    "            'total_nodes': graph.number_of_nodes(),\n",
    "            'total_edges': graph.number_of_edges(),\n",
    "            'density': nx.density(graph),\n",
    "            'avg_degree': np.mean([d for n, d in graph.degree()]) if graph.number_of_nodes() > 0 else 0,\n",
    "            'connected_components': nx.number_weakly_connected_components(graph),\n",
    "            'avg_weight': 0,\n",
    "            'high_quality_edges': 0,\n",
    "            'relation_diversity': 0\n",
    "        }\n",
    "        \n",
    "        # Weight-based metrics\n",
    "        weights = [data.get('weight', 1.0) for _, _, data in graph.edges(data=True)]\n",
    "        if weights:\n",
    "            metrics['avg_weight'] = np.mean(weights)\n",
    "            metrics['high_quality_edges'] = sum(1 for w in weights if w > 0.7)\n",
    "        \n",
    "        # Relation diversity\n",
    "        relations = [data.get('relation', 'Unknown') for _, _, data in graph.edges(data=True)]\n",
    "        metrics['relation_diversity'] = len(set(relations))\n",
    "        \n",
    "        # Graph connectivity score (higher is better)\n",
    "        if metrics['total_nodes'] > 0:\n",
    "            metrics['connectivity_score'] = (metrics['avg_degree'] * metrics['density'] * \n",
    "                                           (1 / max(1, metrics['connected_components'])))\n",
    "        else:\n",
    "            metrics['connectivity_score'] = 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def create_intelligent_batch(self, remaining_data, batch_size):\n",
    "        \"\"\"\n",
    "        Create an intelligent batch prioritizing:\n",
    "        1. High-weight triples\n",
    "        2. Concepts already in the graph (for better connectivity)\n",
    "        3. Diverse relation types\n",
    "        4. Novel concepts (for expansion)\n",
    "        \"\"\"\n",
    "        if len(remaining_data) <= batch_size:\n",
    "            return remaining_data.copy(), pd.DataFrame()\n",
    "        \n",
    "        print(f\"   üìä Creating intelligent batch of {batch_size:,} from {len(remaining_data):,} remaining...\")\n",
    "        \n",
    "        # Score each triple for training value\n",
    "        scores = []\n",
    "        existing_concepts = set(self.agent.graph.nodes())\n",
    "        \n",
    "        for idx, row in remaining_data.iterrows():\n",
    "            score = 0\n",
    "            \n",
    "            # Weight component (40%)\n",
    "            score += row['edge_weight'] * 0.4\n",
    "            \n",
    "            # Connectivity component (30%) - prefer triples connecting to existing graph\n",
    "            start_known = row['start_concept'] in existing_concepts\n",
    "            end_known = row['end_concept'] in existing_concepts\n",
    "            \n",
    "            if start_known and end_known:\n",
    "                score += 0.3  # Best: connects existing concepts\n",
    "            elif start_known or end_known:\n",
    "                score += 0.2  # Good: expands from existing\n",
    "            else:\n",
    "                score += 0.1  # Novel: completely new concepts\n",
    "            \n",
    "            # Relation diversity component (20%) - prefer underrepresented relations\n",
    "            current_relations = [data.get('relation') for _, _, data in self.agent.graph.edges(data=True)]\n",
    "            relation_counts = pd.Series(current_relations).value_counts()\n",
    "            current_relation = row['relation_type']\n",
    "            \n",
    "            if current_relation not in relation_counts.index:\n",
    "                score += 0.2  # New relation type\n",
    "            else:\n",
    "                # Prefer less common relations\n",
    "                relation_freq = relation_counts[current_relation] / len(current_relations)\n",
    "                score += 0.2 * (1 - relation_freq)\n",
    "            \n",
    "            # Validation prediction component (10%)\n",
    "            # Simple heuristic: triples with common relation types score higher\n",
    "            common_relations = ['IsA', 'RelatedTo', 'PartOf', 'UsedFor', 'CapableOf']\n",
    "            if current_relation in common_relations:\n",
    "                score += 0.1\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        # Add scores to dataframe and sort by score (descending)\n",
    "        remaining_data = remaining_data.copy()\n",
    "        remaining_data['training_score'] = scores\n",
    "        remaining_data = remaining_data.sort_values('training_score', ascending=False)\n",
    "        \n",
    "        # Take top-scored triples for batch\n",
    "        batch = remaining_data.head(batch_size).drop('training_score', axis=1)\n",
    "        remaining = remaining_data.tail(len(remaining_data) - batch_size).drop('training_score', axis=1)\n",
    "        \n",
    "        return batch, remaining\n",
    "    \n",
    "    def validate_batch_quality(self, batch_sample_size=100):\n",
    "        \"\"\"\n",
    "        Validate a sample of the remaining data to estimate quality\n",
    "        Used for adaptive threshold adjustment\n",
    "        \"\"\"\n",
    "        if len(self.full_dataset) <= batch_sample_size:\n",
    "            sample = self.full_dataset\n",
    "        else:\n",
    "            sample = self.full_dataset.sample(n=batch_sample_size, random_state=42)\n",
    "        \n",
    "        validation_scores = []\n",
    "        \n",
    "        for _, row in sample.iterrows():\n",
    "            validation = self.agent.validate_new_knowledge(\n",
    "                row['start_concept'], \n",
    "                row['relation_type'], \n",
    "                row['end_concept'], \n",
    "                row['edge_weight']\n",
    "            )\n",
    "            validation_scores.append(validation['validation_score'])\n",
    "        \n",
    "        return {\n",
    "            'mean_quality': np.mean(validation_scores),\n",
    "            'std_quality': np.std(validation_scores),\n",
    "            'high_quality_ratio': sum(1 for s in validation_scores if s > 0.7) / len(validation_scores)\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a batch of triples with detailed tracking\"\"\"\n",
    "        batch_stats = {\n",
    "            'attempted': len(batch),\n",
    "            'accepted': 0,\n",
    "            'rejected_duplicate': 0,\n",
    "            'rejected_validation': 0,\n",
    "            'rejected_contradiction': 0,\n",
    "            'avg_validation_score': 0,\n",
    "            'processing_time': 0\n",
    "        }\n",
    "        \n",
    "        validation_scores = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"   üîÑ Processing batch of {len(batch):,} triples...\")\n",
    "        \n",
    "        for _, row in tqdm(batch.iterrows(), total=len(batch), desc=\"Processing batch\", leave=False):\n",
    "            # Validate first\n",
    "            validation = self.agent.validate_new_knowledge(\n",
    "                row['start_concept'], \n",
    "                row['relation_type'], \n",
    "                row['end_concept'], \n",
    "                row['edge_weight']\n",
    "            )\n",
    "            \n",
    "            validation_scores.append(validation['validation_score'])\n",
    "            \n",
    "            # Decide whether to learn based on current quality threshold\n",
    "            if validation['validation_score'] >= self.current_quality_threshold:\n",
    "                success = self.agent.learn_new_triple(\n",
    "                    row['start_concept'], \n",
    "                    row['relation_type'], \n",
    "                    row['end_concept'], \n",
    "                    row['edge_weight'],\n",
    "                    force=False  # Use validation\n",
    "                )\n",
    "                \n",
    "                if success:\n",
    "                    batch_stats['accepted'] += 1\n",
    "                else:\n",
    "                    batch_stats['rejected_duplicate'] += 1\n",
    "            else:\n",
    "                batch_stats['rejected_validation'] += 1\n",
    "        \n",
    "        batch_stats['avg_validation_score'] = np.mean(validation_scores)\n",
    "        batch_stats['processing_time'] = time.time() - start_time\n",
    "        \n",
    "        return batch_stats\n",
    "    \n",
    "    def update_training_parameters(self, epoch_stats):\n",
    "        \"\"\"\n",
    "        Adaptively update training parameters based on performance\n",
    "        \"\"\"\n",
    "        print(f\"   üéõÔ∏è  Updating training parameters...\")\n",
    "        \n",
    "        # Adaptive batch size\n",
    "        acceptance_rate = epoch_stats['total_accepted'] / max(1, epoch_stats['total_attempted'])\n",
    "        \n",
    "        if acceptance_rate > 0.8:  # High acceptance, increase batch size\n",
    "            self.current_batch_size = min(\n",
    "                self.config['max_batch_size'],\n",
    "                int(self.current_batch_size * 1.2)\n",
    "            )\n",
    "        elif acceptance_rate < 0.3:  # Low acceptance, decrease batch size for quality\n",
    "            self.current_batch_size = max(\n",
    "                self.config['min_batch_size'],\n",
    "                int(self.current_batch_size * 0.8)\n",
    "            )\n",
    "        \n",
    "        # Adaptive quality threshold\n",
    "        if self.config['adaptive_threshold']:\n",
    "            # Gradually increase threshold as graph improves\n",
    "            progress = self.current_epoch / self.config['max_epochs']\n",
    "            self.current_quality_threshold = (\n",
    "                self.config['quality_threshold_start'] + \n",
    "                progress * (self.config['quality_threshold_end'] - self.config['quality_threshold_start'])\n",
    "            )\n",
    "        \n",
    "        print(f\"      Batch size: {self.current_batch_size:,}\")\n",
    "        print(f\"      Quality threshold: {self.current_quality_threshold:.3f}\")\n",
    "    \n",
    "    def check_convergence(self, epoch_stats):\n",
    "        \"\"\"\n",
    "        Check if the training has converged (no more meaningful improvements)\n",
    "        \"\"\"\n",
    "        # Add current metrics to rolling window\n",
    "        improvement_score = epoch_stats['total_accepted'] / max(1, epoch_stats['total_attempted'])\n",
    "        self.convergence_metrics.append(improvement_score)\n",
    "        \n",
    "        if len(self.convergence_metrics) < self.config['convergence_patience']:\n",
    "            return False\n",
    "        \n",
    "        # Check if improvement has plateaued\n",
    "        recent_improvements = list(self.convergence_metrics)\n",
    "        trend = np.polyfit(range(len(recent_improvements)), recent_improvements, 1)[0]\n",
    "        \n",
    "        # Convergence criteria:\n",
    "        # 1. Very low acceptance rate (< 5%)\n",
    "        # 2. Declining or flat trend in improvements\n",
    "        # 3. Multiple epochs with similar performance\n",
    "        \n",
    "        avg_recent_improvement = np.mean(recent_improvements)\n",
    "        improvement_std = np.std(recent_improvements)\n",
    "        \n",
    "        has_converged = (\n",
    "            avg_recent_improvement < 0.05 or  # Very low acceptance\n",
    "            (trend <= 0 and improvement_std < 0.02)  # Flat/declining trend with low variance\n",
    "        )\n",
    "        \n",
    "        if has_converged:\n",
    "            print(f\"   üéØ CONVERGENCE DETECTED:\")\n",
    "            print(f\"      Recent improvement: {avg_recent_improvement:.4f}\")\n",
    "            print(f\"      Trend: {trend:.6f}\")\n",
    "            print(f\"      Stability: {improvement_std:.4f}\")\n",
    "        \n",
    "        return has_converged\n",
    "    \n",
    "    def train_until_convergence(self):\n",
    "        \"\"\"\n",
    "        Main training loop - processes entire dataset until convergence\n",
    "        \"\"\"\n",
    "        print(\"üöÄ STARTING ADAPTIVE TRAINING LOOP\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        remaining_data = self.full_dataset.copy()\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        while self.current_epoch < self.config['max_epochs']:\n",
    "            self.current_epoch += 1\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            print(f\"\\nüìÖ EPOCH {self.current_epoch}/{self.config['max_epochs']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Calculate pre-epoch metrics\n",
    "            pre_metrics = self.calculate_graph_quality_metrics()\n",
    "            print(f\"   Graph state: {pre_metrics['total_nodes']:,} nodes, {pre_metrics['total_edges']:,} edges\")\n",
    "            \n",
    "            # Process data in batches\n",
    "            epoch_stats = {\n",
    "                'total_attempted': 0,\n",
    "                'total_accepted': 0,\n",
    "                'total_rejected': 0,\n",
    "                'batches_processed': 0,\n",
    "                'avg_validation_score': 0\n",
    "            }\n",
    "            \n",
    "            validation_scores = []\n",
    "            \n",
    "            while len(remaining_data) > 0:\n",
    "                # Create intelligent batch\n",
    "                batch, remaining_data = self.create_intelligent_batch(\n",
    "                    remaining_data, \n",
    "                    self.current_batch_size\n",
    "                )\n",
    "                \n",
    "                # Process batch\n",
    "                batch_stats = self.process_batch(batch)\n",
    "                \n",
    "                # Update epoch statistics\n",
    "                epoch_stats['total_attempted'] += batch_stats['attempted']\n",
    "                epoch_stats['total_accepted'] += batch_stats['accepted']\n",
    "                epoch_stats['total_rejected'] += (batch_stats['rejected_duplicate'] + \n",
    "                                                 batch_stats['rejected_validation'] + \n",
    "                                                 batch_stats['rejected_contradiction'])\n",
    "                epoch_stats['batches_processed'] += 1\n",
    "                validation_scores.append(batch_stats['avg_validation_score'])\n",
    "                \n",
    "                print(f\"      Batch {epoch_stats['batches_processed']}: \"\n",
    "                      f\"{batch_stats['accepted']}/{batch_stats['attempted']} accepted \"\n",
    "                      f\"(score: {batch_stats['avg_validation_score']:.3f})\")\n",
    "            \n",
    "            # Calculate epoch-level metrics\n",
    "            epoch_stats['avg_validation_score'] = np.mean(validation_scores)\n",
    "            post_metrics = self.calculate_graph_quality_metrics()\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            \n",
    "            # Calculate improvements\n",
    "            node_growth = post_metrics['total_nodes'] - pre_metrics['total_nodes']\n",
    "            edge_growth = post_metrics['total_edges'] - pre_metrics['total_edges']\n",
    "            \n",
    "            epoch_summary = {\n",
    "                'epoch': self.current_epoch,\n",
    "                'duration': epoch_duration,\n",
    "                'acceptance_rate': epoch_stats['total_accepted'] / max(1, epoch_stats['total_attempted']),\n",
    "                'node_growth': node_growth,\n",
    "                'edge_growth': edge_growth,\n",
    "                'final_nodes': post_metrics['total_nodes'],\n",
    "                'final_edges': post_metrics['total_edges'],\n",
    "                'avg_validation_score': epoch_stats['avg_validation_score'],\n",
    "                'connectivity_score': post_metrics['connectivity_score']\n",
    "            }\n",
    "            \n",
    "            self.training_history.append(epoch_summary)\n",
    "            \n",
    "            print(f\"\\n   üìä EPOCH {self.current_epoch} SUMMARY:\")\n",
    "            print(f\"      Duration: {epoch_duration:.1f}s\")\n",
    "            print(f\"      Acceptance rate: {epoch_summary['acceptance_rate']:.3f}\")\n",
    "            print(f\"      Growth: +{node_growth:,} nodes, +{edge_growth:,} edges\")\n",
    "            print(f\"      Total: {post_metrics['total_nodes']:,} nodes, {post_metrics['total_edges']:,} edges\")\n",
    "            print(f\"      Avg validation score: {epoch_stats['avg_validation_score']:.3f}\")\n",
    "            \n",
    "            # Update parameters for next epoch\n",
    "            self.update_training_parameters(epoch_stats)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if self.check_convergence(epoch_stats):\n",
    "                print(f\"\\nüéØ CONVERGENCE ACHIEVED after {self.current_epoch} epochs!\")\n",
    "                break\n",
    "            \n",
    "            # Reset remaining data for next epoch (re-process entire dataset)\n",
    "            remaining_data = self.full_dataset.copy()\n",
    "        \n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(f\"\\nüéâ TRAINING COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total training time: {total_training_time/3600:.2f} hours\")\n",
    "        print(f\"Epochs completed: {self.current_epoch}\")\n",
    "        print(f\"Final graph: {post_metrics['total_nodes']:,} nodes, {post_metrics['total_edges']:,} edges\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Visualize training progress\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = [h['epoch'] for h in self.training_history]\n",
    "        \n",
    "        # Plot 1: Acceptance rate over time\n",
    "        acceptance_rates = [h['acceptance_rate'] for h in self.training_history]\n",
    "        axes[0,0].plot(epochs, acceptance_rates, 'b-o')\n",
    "        axes[0,0].set_title('Acceptance Rate Over Time')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('Acceptance Rate')\n",
    "        axes[0,0].grid(True)\n",
    "        \n",
    "        # Plot 2: Graph growth\n",
    "        total_edges = [h['final_edges'] for h in self.training_history]\n",
    "        axes[0,1].plot(epochs, total_edges, 'g-o')\n",
    "        axes[0,1].set_title('Knowledge Graph Growth')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Total Edges')\n",
    "        axes[0,1].grid(True)\n",
    "        \n",
    "        # Plot 3: Validation scores\n",
    "        val_scores = [h['avg_validation_score'] for h in self.training_history]\n",
    "        axes[1,0].plot(epochs, val_scores, 'r-o')\n",
    "        axes[1,0].set_title('Average Validation Score')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('Validation Score')\n",
    "        axes[1,0].grid(True)\n",
    "        \n",
    "        # Plot 4: Edge growth per epoch\n",
    "        edge_growth = [h['edge_growth'] for h in self.training_history]\n",
    "        axes[1,1].bar(epochs, edge_growth, alpha=0.7)\n",
    "        axes[1,1].set_title('New Edges Per Epoch')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('New Edges Added')\n",
    "        axes[1,1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3979801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Setting up Adaptive Training Loop...\n",
      "üéØ Adaptive Training Loop Initialized!\n",
      "   Dataset size: 1,655,522 triples\n",
      "   Starting batch size: 2,000\n",
      "   Quality threshold: 0.300\n",
      "\n",
      "üöÄ Ready to train on 1,655,522 triples!\n",
      "‚ö° This will take several hours but creates a self-improving agent\n",
      "üéØ Uncomment the next line to start training:\n",
      "# training_history = trainer.train_until_convergence()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example - Add this to your notebook\n",
    "print(\"üéØ Setting up Adaptive Training Loop...\")\n",
    "\n",
    "# Configure training parameters\n",
    "training_config = {\n",
    "    'initial_batch_size': 2000,      # Start with 2K triples per batch\n",
    "    'max_batch_size': 10000,         # Max 10K triples per batch\n",
    "    'min_batch_size': 500,           # Min 500 triples per batch\n",
    "    'quality_threshold_start': 0.3,  # Start accepting medium-quality triples\n",
    "    'quality_threshold_end': 0.7,    # End by only accepting high-quality triples\n",
    "    'convergence_patience': 3,       # Wait 3 epochs for convergence confirmation\n",
    "    'max_epochs': 15,                # Maximum 15 full passes through data\n",
    "    'adaptive_threshold': True,      # Gradually increase quality standards\n",
    "    'learning_rate_decay': 0.95      # Decrease batch size if needed\n",
    "}\n",
    "\n",
    "# Initialize training loop\n",
    "\n",
    "prepped_english_triples = cleaned_english_triples.copy()\n",
    "prepped_english_triples = prepped_english_triples.rename(columns={\n",
    "    'relation_cleaned': 'relation_type',\n",
    "    'start_cleaned': 'start_concept',\n",
    "    'end_cleaned': 'end_concept',\n",
    "    'weight_cleaned': 'edge_weight'\n",
    "})\n",
    "\n",
    "\n",
    "trainer = AdaptiveTrainingLoop(mvp_agent, prepped_english_triples, training_config)\n",
    "\n",
    "print(f\"\\nüöÄ Ready to train on {len(prepped_english_triples):,} triples!\")\n",
    "print(f\"‚ö° This will take several hours but creates a self-improving agent\")\n",
    "print(f\"üéØ Uncomment the next line to start training:\")\n",
    "print(f\"# training_history = trainer.train_until_convergence()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING ADAPTIVE TRAINING LOOP\n",
      "============================================================\n",
      "\n",
      "üìÖ EPOCH 1/15\n",
      "----------------------------------------\n",
      "   Graph state: 4,641 nodes, 4,957 edges\n",
      "   üìä Creating intelligent batch of 2,000 from 1,655,522 remaining...\n"
     ]
    }
   ],
   "source": [
    "training_history = trainer.train_until_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749bdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
