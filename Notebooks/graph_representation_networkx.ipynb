{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce0a31a",
   "metadata": {},
   "source": [
    "# Building a Scalable Semantic Graph from ConceptNet with NetworkX\n",
    "This notebook demonstrates how to construct a scalable semantic graph database from cleaned ConceptNet triples using NetworkX. It covers loading the data, building a graph, performing basic queries, and saving/loading the graph in a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c998847",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned ConceptNet Data\n",
    "We use the preprocessed ConceptNet triples stored in Parquet format. Adjust the path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39228ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the cleaned ConceptNet data\n",
    "conceptnet_path = os.path.join('..', 'Data', 'Input', 'conceptnet_en_processed_for_graph.parquet.gzip')\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_parquet(conceptnet_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f348",
   "metadata": {},
   "source": [
    "## 2. Build a NetworkX MultiDiGraph from the Edge List\n",
    "Each ConceptNet triple becomes a directed edge with attributes (relation, weight, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultiDiGraph (allows multiple edges between nodes)\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# Add edges from the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(\n",
    "        row['start_concept'],\n",
    "        row['end_concept'],\n",
    "        relation=row['relation_type'],\n",
    "        weight=row.get('edge_weight', 1.0)\n",
    "    )\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babc0e0",
   "metadata": {},
   "source": [
    "## 3. Basic Graph Database Operations\n",
    "Query neighbors, edge attributes, and extract subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query neighbors of a concept\n",
    "concept = 'dog'\n",
    "neighbors = list(G.neighbors(concept))\n",
    "print(f\"Neighbors of '{concept}':\", neighbors)\n",
    "\n",
    "# Example: Get all edges and their attributes for a concept\n",
    "edges = G.out_edges(concept, data=True)\n",
    "for u, v, attr in edges:\n",
    "    print(f\"{u} --[{attr['relation']}]--> {v} (weight={attr['weight']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract a subgraph of a concept and its immediate neighbors\n",
    "sub_nodes = [concept] + neighbors\n",
    "subgraph = G.subgraph(sub_nodes)\n",
    "print(f\"Subgraph has {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb0028",
   "metadata": {},
   "source": [
    "## 4. Save and Load the Graph in GraphML Format\n",
    "GraphML is a standard, portable format for graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2edabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph\n",
    "output_path = os.path.join('..', 'Data', 'Output', 'conceptnet_graph.graphml')\n",
    "# nx.write_graphml(G, output_path)\n",
    "print(f\"Graph saved to {output_path}\")\n",
    "\n",
    "# Load the graph back (demonstration)\n",
    "G_loaded = nx.read_graphml(output_path)\n",
    "print(f\"Loaded graph has {G_loaded.number_of_nodes()} nodes and {G_loaded.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a823a",
   "metadata": {},
   "source": [
    "## 5. Next Steps and Advanced Queries\n",
    "- Expand to larger datasets by chunking or streaming edge additions.\n",
    "- Integrate with graph databases like Neo4j for even larger-scale applications.\n",
    "- Perform advanced queries (e.g., shortest paths, community detection, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f4741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d510fe69",
   "metadata": {},
   "source": [
    "## 6. Semantic Enrichment: Adding Custom Logic\n",
    "Now that the graph is built, you can add your own semantic logic. For example, you might:\n",
    "- Add new semantic relationships based on custom rules or external data.\n",
    "- Reweight or filter edges based on your own criteria.\n",
    "- Annotate nodes or edges with additional semantic metadata.\n",
    "- Implement reasoning or inference over the graph structure.\n",
    "\n",
    "Below is an example of how to add a new semantic property to edges based on a custom rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add a semantic property to edges based on a custom rule\n",
    "# Here, we flag edges as 'is_animal_relation' if either node is an animal (simple demo)\n",
    "animal_concepts = {'dog', 'cat', 'horse', 'cow', 'sheep'}  # Replace with your own logic or list\n",
    "\n",
    "for u, v, k, data in G.edges(keys=True, data=True):\n",
    "    if u in animal_concepts or v in animal_concepts:\n",
    "        data['is_animal_relation'] = True\n",
    "    else:\n",
    "        data['is_animal_relation'] = False\n",
    "\n",
    "# Show a few example edges with the new property\n",
    "count = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if 'is_animal_relation' in data and data['is_animal_relation']:\n",
    "        print(f\"{u} --[{data['relation']}]--> {v} | is_animal_relation: {data['is_animal_relation']}\")\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0b298",
   "metadata": {},
   "source": [
    "You can now build on this pattern to add more advanced semantic logic, such as inference, clustering, or integration with external knowledge sources. Just add new cells below to continue your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80853cd6",
   "metadata": {},
   "source": [
    "# 1. NetworkX Hyper-Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0116e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_hyper_train(data_path, iterations=3, save_prefix=\"nx_hyper_trained\"):\n",
    "    \"\"\"\n",
    "    Hyper-train a knowledge graph using NetworkX - much faster than matrix-based approaches\n",
    "    while preserving the iterative quality improvements.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "    import os\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(f\"üöÄ Starting NetworkX Hyper-Training ({iterations} iterations)\")\n",
    "    \n",
    "    # Load the data once\n",
    "    full_df = pd.read_parquet(data_path)\n",
    "    print(f\"üìä Loaded {len(full_df):,} triples\")\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = []\n",
    "    \n",
    "    # Initial params\n",
    "    current_quality_threshold = 0.5  # Start higher with NetworkX (faster)\n",
    "    current_concept_limit = 10000\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(os.path.join('..', 'Data', 'Output'), exist_ok=True)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìå ITERATION {iteration+1}/{iterations}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 1. Filter concepts by popularity (memory efficiency)\n",
    "        concept_counts = pd.concat([\n",
    "            full_df['start_concept'].value_counts(),\n",
    "            full_df['end_concept'].value_counts()\n",
    "        ]).groupby(level=0).sum().sort_values(ascending=False)\n",
    "        \n",
    "        top_concepts = set(concept_counts.head(current_concept_limit).index)\n",
    "        filtered_df = full_df[\n",
    "            full_df['start_concept'].isin(top_concepts) & \n",
    "            full_df['end_concept'].isin(top_concepts)\n",
    "        ]\n",
    "        \n",
    "        # 2. Apply quality threshold (weight-based)\n",
    "        quality_df = filtered_df[filtered_df['edge_weight'] >= current_quality_threshold]\n",
    "        \n",
    "        print(f\"üìä Using {len(top_concepts):,} concepts\")\n",
    "        print(f\"üìä Quality threshold: {current_quality_threshold:.2f}\")\n",
    "        print(f\"üìä Filtered to {len(quality_df):,} high-quality triples\")\n",
    "        \n",
    "        # 3. Build graph (much faster with NetworkX)\n",
    "        print(f\"üîÑ Building graph...\")\n",
    "        G = nx.MultiDiGraph()\n",
    "        \n",
    "        # Add edges with progress bar\n",
    "        for _, row in tqdm(quality_df.iterrows(), total=len(quality_df), desc=\"Adding edges\"):\n",
    "            G.add_edge(\n",
    "                row['start_concept'],\n",
    "                row['end_concept'],\n",
    "                relation=row['relation_type'],\n",
    "                weight=row['edge_weight']\n",
    "            )\n",
    "        \n",
    "        # 4. Apply semantic enrichment for this iteration\n",
    "        print(f\"‚ú® Adding semantic enrichment (iteration-specific)...\")\n",
    "        \n",
    "        # Simple example: mark high-confidence edges\n",
    "        high_confidence_threshold = 0.7 + (iteration * 0.05)  # Increases each iteration\n",
    "        edge_count = 0\n",
    "        \n",
    "        for u, v, k, data in G.edges(keys=True, data=True):\n",
    "            # Add iteration-specific attributes\n",
    "            data['iteration_added'] = iteration + 1\n",
    "            data['high_confidence'] = data['weight'] >= high_confidence_threshold\n",
    "            edge_count += 1\n",
    "            \n",
    "            # Add custom logic (just examples - modify for your needs)\n",
    "            if iteration == 1:\n",
    "                # Second iteration: add transitivity scores\n",
    "                data['transitive_potential'] = len(list(G.neighbors(v))) / 100\n",
    "            elif iteration == 2:\n",
    "                # Third iteration: add centrality-based importance\n",
    "                try:\n",
    "                    data['centrality_score'] = G.degree(u) * G.degree(v) / 1000\n",
    "                except:\n",
    "                    data['centrality_score'] = 0\n",
    "        \n",
    "        # 5. Save this iteration\n",
    "        output_path = os.path.join('..', 'Data', 'Output', f\"{save_prefix}_iter{iteration+1}.graphml\")\n",
    "        nx.write_graphml(G, output_path)\n",
    "        \n",
    "        # 6. Calculate and store metrics\n",
    "        iter_time = time.time() - start_time\n",
    "        metrics.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'quality_threshold': current_quality_threshold,\n",
    "            'concept_limit': current_concept_limit,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'processing_time': iter_time,\n",
    "            'edges_per_second': G.number_of_edges() / max(1, iter_time)\n",
    "        })\n",
    "        \n",
    "        print(f\"üìä Graph has {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\")\n",
    "        print(f\"‚è±Ô∏è  Processing time: {iter_time:.1f} seconds\")\n",
    "        print(f\"üíæ Saved to: {output_path}\")\n",
    "        \n",
    "        # 7. Update parameters for next iteration\n",
    "        current_quality_threshold = min(0.9, current_quality_threshold + 0.1)\n",
    "        current_concept_limit = min(50000, current_concept_limit + 10000)  # Increase concept count\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéâ HYPER-TRAINING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for m in metrics:\n",
    "        print(f\"Iteration {m['iteration']}: {m['nodes']:,} nodes, \"\n",
    "              f\"{m['edges']:,} edges, {m['processing_time']:.1f}s, \"\n",
    "              f\"threshold={m['quality_threshold']:.2f}\")\n",
    "    \n",
    "    final_path = os.path.join('..', 'Data', 'Output', f\"{save_prefix}_final.graphml\")\n",
    "    nx.write_graphml(G, final_path)\n",
    "    print(f\"üíæ Final model saved to: {final_path}\")\n",
    "    \n",
    "    return G, metrics, final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa84bda",
   "metadata": {},
   "source": [
    "# 2. Call The Function To Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the NetworkX hyper-training function\n",
    "# G_trained, training_metrics, final_model_path = networkx_hyper_train(\n",
    "#     data_path=conceptnet_path,  # Path to your conceptnet data\n",
    "#     iterations=3,               # Number of training iterations\n",
    "#     save_prefix=\"nx_semantic\"   # Prefix for saved files\n",
    "# )\n",
    "\n",
    "# # The result is already a NetworkX graph ready for querying!\n",
    "# print(f\"\\nüîç Testing final model...\")\n",
    "# concept = 'dog'\n",
    "# for u, v, data in G_trained.out_edges(concept, data=True):\n",
    "#     if data.get('high_confidence', False):\n",
    "#         print(f\"{u} --[{data['relation']}]--> {v} (weight={data['weight']}, added in iteration {data['iteration_added']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769328f1",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Hyper-Training for Scaling\n",
    "To achieve our goal of eventually including the full ConceptNet dataset, we need a more memory-efficient approach that:\n",
    "1. Processes data in chunks rather than loading everything at once\n",
    "2. Uses more efficient storage formats\n",
    "3. Implements checkpointing for recovery\n",
    "4. Has configurable parameters for gradual scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68069e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö°Ô∏è Fix: Chunked Parquet Reading with pyarrow\n",
    "# The scalable training function is now updated to use a custom `parquet_chunk_reader` utility based on `pyarrow` for chunked reading of Parquet files. This avoids the `TypeError` from pandas' `read_parquet()` and enables true memory-efficient processing of large datasets.\n",
    "\n",
    "# Utility: Chunked Parquet Reader using pyarrow\n",
    "def parquet_chunk_reader(parquet_path, chunk_size=100000, columns=None):\n",
    "    \"\"\"\n",
    "    Efficiently read a Parquet file in chunks using pyarrow, yielding pandas DataFrames.\n",
    "    \"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pandas as pd\n",
    "\n",
    "    parquet_file = pq.ParquetFile(parquet_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "    rows_read = 0\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "    # Read in row groups, but yield DataFrames in chunks of chunk_size\n",
    "    batch = []\n",
    "    batch_rows = 0\n",
    "    for rg in range(num_row_groups):\n",
    "        table = parquet_file.read_row_group(rg, columns=columns)\n",
    "        df = table.to_pandas()\n",
    "        batch.append(df)\n",
    "        batch_rows += len(df)\n",
    "        rows_read += len(df)\n",
    "        # If enough rows for a chunk, yield\n",
    "        while batch_rows >= chunk_size:\n",
    "            concat_df = pd.concat(batch)\n",
    "            yield concat_df.iloc[:chunk_size]\n",
    "            # Prepare for next batch\n",
    "            if batch_rows > chunk_size:\n",
    "                # Keep the remainder for next chunk\n",
    "                batch = [concat_df.iloc[chunk_size:]]\n",
    "                batch_rows = len(batch[0])\n",
    "            else:\n",
    "                batch = []\n",
    "                batch_rows = 0\n",
    "    # Yield any remaining rows\n",
    "    if batch_rows > 0:\n",
    "        yield pd.concat(batch)\n",
    "\n",
    "# Refactor: networkx_scalable_hyper_train now uses parquet_chunk_reader for chunked reading\n",
    "def networkx_scalable_hyper_train(\n",
    "    data_path,\n",
    "    iterations=3,\n",
    "    save_prefix=\"nx_scalable\",\n",
    "    config=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-efficient scalable hyper-training for knowledge graphs.\n",
    "    Can handle much larger datasets by processing in chunks and implementing checkpointing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the parquet file with ConceptNet data\n",
    "    iterations : int\n",
    "        Number of training iterations\n",
    "    save_prefix : str\n",
    "        Prefix for saved files\n",
    "    config : dict\n",
    "        Configuration parameters (if None, defaults will be used)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    G : networkx.MultiDiGraph\n",
    "        Final trained graph\n",
    "    metrics : list\n",
    "        Training metrics for each iteration\n",
    "    final_path : str\n",
    "        Path to the final saved graph\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "    import os\n",
    "    import time\n",
    "    import pickle\n",
    "    import json\n",
    "    import gc\n",
    "    from datetime import datetime\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        # Data filtering\n",
    "        'initial_quality_threshold': 0.3,      # Start with this threshold\n",
    "        'quality_threshold_step': 0.1,         # Increase by this amount each iteration\n",
    "        'max_quality_threshold': 0.9,          # Maximum quality threshold\n",
    "        \n",
    "        # Concept limits\n",
    "        'initial_concept_limit': 20000,        # Start with this many concepts\n",
    "        'concept_limit_step': 20000,           # Increase by this amount each iteration\n",
    "        'max_concept_limit': 100000,           # Maximum number of concepts\n",
    "        \n",
    "        # Memory optimization\n",
    "        'chunk_size': 100000,                  # Process this many rows at a time\n",
    "        'gc_frequency': 2,                     # Run garbage collection every N chunks\n",
    "        \n",
    "        # Storage options\n",
    "        'save_format': 'pickle',               # 'graphml' or 'pickle'\n",
    "        'compression': True,                   # Compress saved files\n",
    "        \n",
    "        # Checkpointing\n",
    "        'enable_checkpoints': True,            # Save checkpoint after each iteration\n",
    "        'resume_from_checkpoint': True,        # Try to resume from checkpoint if available\n",
    "        \n",
    "        # Enrichment options\n",
    "        'add_high_confidence_flag': True,      # Add high_confidence flag to edges\n",
    "        'add_transitivity': True,              # Add transitivity scores in iteration 1\n",
    "        'add_centrality': True,                # Add centrality scores in iteration 2\n",
    "        \n",
    "        # Performance profiling\n",
    "        'profile': False                       # Track detailed performance metrics\n",
    "    }\n",
    "    \n",
    "    # Update with user config\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    for key, value in config.items():\n",
    "        if key in default_config:\n",
    "            default_config[key] = value\n",
    "    config = default_config\n",
    "    \n",
    "    print(f\"üöÄ Starting Scalable Hyper-Training ({iterations} iterations)\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join('..', 'Data', 'Output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize metrics tracking\n",
    "    metrics = []\n",
    "    \n",
    "    # Current parameters\n",
    "    current_quality_threshold = config['initial_quality_threshold']\n",
    "    current_concept_limit = config['initial_concept_limit']\n",
    "    \n",
    "    # Try to resume from checkpoint if enabled\n",
    "    checkpoint_path = os.path.join(output_dir, f\"{save_prefix}_checkpoint.pkl\")\n",
    "    if config['enable_checkpoints'] and config['resume_from_checkpoint'] and os.path.exists(checkpoint_path):\n",
    "        print(f\"üìÇ Resuming from checkpoint: {checkpoint_path}\")\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                checkpoint = pickle.load(f)\n",
    "                \n",
    "            # Restore state\n",
    "            metrics = checkpoint['metrics']\n",
    "            current_quality_threshold = checkpoint['next_quality_threshold']\n",
    "            current_concept_limit = checkpoint['next_concept_limit']\n",
    "            last_iteration = checkpoint['last_completed_iteration']\n",
    "            \n",
    "            print(f\"‚úÖ Restored from checkpoint (completed {last_iteration+1}/{iterations} iterations)\")\n",
    "            start_iteration = last_iteration + 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load checkpoint: {e}\")\n",
    "            start_iteration = 0\n",
    "    else:\n",
    "        start_iteration = 0\n",
    "    \n",
    "    # Process only needed iterations\n",
    "    for iteration in range(start_iteration, iterations):\n",
    "        iteration_start_time = time.time()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìå ITERATION {iteration+1}/{iterations}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"üìä Quality threshold: {current_quality_threshold:.2f}\")\n",
    "        print(f\"üìä Concept limit: {current_concept_limit:,}\")\n",
    "        \n",
    "        # Step 1: Initial scan - count concepts to identify top ones\n",
    "        print(f\"üîç Scanning data to identify top concepts...\")\n",
    "        \n",
    "        # Initialize concept counter\n",
    "        concept_counter = None\n",
    "        for i, chunk in enumerate(parquet_chunk_reader(data_path, chunk_size=config['chunk_size'])):\n",
    "            chunk_counts = pd.concat([\n",
    "                chunk['start_concept'].value_counts(),\n",
    "                chunk['end_concept'].value_counts()\n",
    "            ]).groupby(level=0).sum()\n",
    "            if concept_counter is None:\n",
    "                concept_counter = chunk_counts\n",
    "            else:\n",
    "                concept_counter = concept_counter.add(chunk_counts, fill_value=0)\n",
    "            if i % config['gc_frequency'] == 0:\n",
    "                import gc; gc.collect()\n",
    "        \n",
    "        # Get top concepts\n",
    "        top_concepts = set(concept_counter.nlargest(current_concept_limit).index)\n",
    "        print(f\"‚úÖ Identified {len(top_concepts):,} most frequent concepts\")\n",
    "        \n",
    "        # Step 2: Build graph in chunks\n",
    "        print(f\"üîÑ Building graph with quality threshold {current_quality_threshold:.2f}...\")\n",
    "        \n",
    "        # Initialize the graph\n",
    "        G = nx.MultiDiGraph()\n",
    "        \n",
    "        # Process in chunks\n",
    "        total_edges_processed = 0\n",
    "        total_edges_added = 0\n",
    "        \n",
    "        # Reset reader for second pass\n",
    "        chunk_reader = parquet_chunk_reader(data_path, chunk_size=config['chunk_size'])\n",
    "        for i, chunk in enumerate(tqdm(chunk_reader, desc=\"Building graph\")):\n",
    "            # Filter by concepts\n",
    "            filtered_chunk = chunk[\n",
    "                chunk['start_concept'].isin(top_concepts) & \n",
    "                chunk['end_concept'].isin(top_concepts)\n",
    "            ]\n",
    "            # Filter by quality\n",
    "            quality_chunk = filtered_chunk[filtered_chunk['edge_weight'] >= current_quality_threshold]\n",
    "\n",
    "            # Progress bar for edges in this chunk\n",
    "            for _, row in tqdm(quality_chunk.iterrows(), total=len(quality_chunk), desc=f\"Adding edges (chunk {i+1})\", leave=False):\n",
    "                G.add_edge(\n",
    "                    row['start_concept'],\n",
    "                    row['end_concept'],\n",
    "                    relation=row['relation_type'],\n",
    "                    weight=row['edge_weight'],\n",
    "                    iteration_added=iteration + 1\n",
    "                )\n",
    "            total_edges_processed += len(filtered_chunk)\n",
    "            total_edges_added += len(quality_chunk)\n",
    "            if i % config['gc_frequency'] == 0:\n",
    "                import gc; gc.collect()\n",
    "        \n",
    "        print(f\"‚úÖ Processed {total_edges_processed:,} edges, added {total_edges_added:,} to graph\")\n",
    "        print(f\"üìä Graph has {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\")\n",
    "        \n",
    "        # Step 3: Apply semantic enrichment\n",
    "        if config['add_high_confidence_flag'] or config['add_transitivity'] or config['add_centrality']:\n",
    "            print(f\"‚ú® Adding semantic enrichment...\")\n",
    "            \n",
    "            enrichment_start = time.time()\n",
    "            \n",
    "            # High confidence threshold increases with each iteration\n",
    "            high_confidence_threshold = 0.7 + (iteration * 0.05)\n",
    "            \n",
    "            if config['add_high_confidence_flag']:\n",
    "                # Mark high-confidence edges\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True), \n",
    "                                         desc=\"Adding high confidence flags\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    data['high_confidence'] = data['weight'] >= high_confidence_threshold\n",
    "            \n",
    "            # Add iteration-specific enrichment\n",
    "            if iteration == 1 and config['add_transitivity']:\n",
    "                # Second iteration: add transitivity scores for a subset of nodes\n",
    "                # This is expensive, so we limit to higher-degree nodes\n",
    "                print(\"üîÑ Adding transitivity scores...\")\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True),\n",
    "                                         desc=\"Adding transitivity scores\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    try:\n",
    "                        # More efficient calculation - only count if needed\n",
    "                        if G.out_degree(v) > 0:  # Only if target has outgoing connections\n",
    "                            data['transitive_potential'] = min(G.out_degree(v) / 100, 1.0)\n",
    "                    except:\n",
    "                        data['transitive_potential'] = 0\n",
    "            \n",
    "            elif iteration == 2 and config['add_centrality']:\n",
    "                # Third iteration: add simplified centrality scores\n",
    "                # Full betweenness centrality is too expensive, so we use degree as proxy\n",
    "                print(\"üîÑ Adding centrality scores...\")\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True),\n",
    "                                         desc=\"Adding centrality scores\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    try:\n",
    "                        data['centrality_score'] = min(\n",
    "                            (G.degree(u) + G.degree(v)) / 1000, \n",
    "                            1.0\n",
    "                        )\n",
    "                    except:\n",
    "                        data['centrality_score'] = 0\n",
    "            \n",
    "            print(f\"‚úÖ Enrichment completed in {time.time() - enrichment_start:.1f}s\")\n",
    "        \n",
    "        # Step 4: Save this iteration\n",
    "        save_start_time = time.time()\n",
    "        \n",
    "        if config['save_format'] == 'graphml':\n",
    "            # Save as GraphML (standard but larger files)\n",
    "            output_path = os.path.join(output_dir, f\"{save_prefix}_iter{iteration+1}.graphml\")\n",
    "            nx.write_graphml(G, output_path)\n",
    "        else:\n",
    "            # Save as pickle (faster, smaller)\n",
    "            output_path = os.path.join(output_dir, f\"{save_prefix}_iter{iteration+1}.pkl\")\n",
    "            with open(output_path, 'wb') as f:\n",
    "                pickle.dump(G, f, protocol=4)  # Protocol 4 for better performance\n",
    "        \n",
    "        print(f\"üíæ Saved to: {output_path} in {time.time() - save_start_time:.1f}s\")\n",
    "        \n",
    "        # Step 5: Calculate and store metrics\n",
    "        iter_time = time.time() - iteration_start_time\n",
    "        iter_metrics = {\n",
    "            'iteration': iteration + 1,\n",
    "            'quality_threshold': current_quality_threshold,\n",
    "            'concept_limit': current_concept_limit,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'processing_time': iter_time,\n",
    "            'edges_per_second': total_edges_processed / max(1, iter_time),\n",
    "            'added_edges_ratio': total_edges_added / max(1, total_edges_processed),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        metrics.append(iter_metrics)\n",
    "        \n",
    "        print(f\"üìä Iteration stats:\")\n",
    "        print(f\"   Nodes: {iter_metrics['nodes']:,}\")\n",
    "        print(f\"   Edges: {iter_metrics['edges']:,}\")\n",
    "        print(f\"   Processing time: {iter_metrics['processing_time']:.1f}s\")\n",
    "        print(f\"   Edges/second: {iter_metrics['edges_per_second']:.1f}\")\n",
    "        \n",
    "        # Step 6: Save checkpoint if enabled\n",
    "        if config['enable_checkpoints']:\n",
    "            # Calculate next thresholds\n",
    "            next_quality_threshold = min(\n",
    "                config['max_quality_threshold'],\n",
    "                current_quality_threshold + config['quality_threshold_step']\n",
    "            )\n",
    "            next_concept_limit = min(\n",
    "                config['max_concept_limit'],\n",
    "                current_concept_limit + config['concept_limit_step']\n",
    "            )\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'metrics': metrics,\n",
    "                'last_completed_iteration': iteration,\n",
    "                'next_quality_threshold': next_quality_threshold,\n",
    "                'next_concept_limit': next_concept_limit,\n",
    "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "            print(f\"‚úÖ Checkpoint saved to: {checkpoint_path}\")\n",
    "        \n",
    "        # Step 7: Update parameters for next iteration\n",
    "        current_quality_threshold = min(\n",
    "            config['max_quality_threshold'],\n",
    "            current_quality_threshold + config['quality_threshold_step']\n",
    "        )\n",
    "        current_concept_limit = min(\n",
    "            config['max_concept_limit'],\n",
    "            current_concept_limit + config['concept_limit_step']\n",
    "        )\n",
    "        \n",
    "        # Explicitly run garbage collection between iterations\n",
    "        gc.collect()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéâ HYPER-TRAINING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for m in metrics:\n",
    "        print(f\"Iteration {m['iteration']}: {m['nodes']:,} nodes, \"\n",
    "              f\"{m['edges']:,} edges, {m['processing_time']:.1f}s, \"\n",
    "              f\"threshold={m['quality_threshold']:.2f}\")\n",
    "    \n",
    "    # Save final version (use the same format as iterations for consistency)\n",
    "    if config['save_format'] == 'graphml':\n",
    "        final_path = os.path.join(output_dir, f\"{save_prefix}_final.graphml\")\n",
    "        nx.write_graphml(G, final_path)\n",
    "    else:\n",
    "        final_path = os.path.join(output_dir, f\"{save_prefix}_final.pkl\")\n",
    "        with open(final_path, 'wb') as f:\n",
    "            pickle.dump(G, f, protocol=4)\n",
    "    \n",
    "    print(f\"üíæ Final model saved to: {final_path}\")\n",
    "    \n",
    "    # Save metrics as JSON for analysis\n",
    "    metrics_path = os.path.join(output_dir, f\"{save_prefix}_metrics.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"üìä Training metrics saved to: {metrics_path}\")\n",
    "    \n",
    "    return G, metrics, final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8f409",
   "metadata": {},
   "source": [
    "# 3. Run the Scalable Hyper-Training\n",
    "\n",
    "This version can handle much larger datasets by using chunked processing, efficient storage, and checkpointing. It's designed to be interruptible and resumable, making it practical for training on the full ConceptNet dataset in stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecdfa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Scalable Hyper-Training (4 iterations)\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 1/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.30\n",
      "üìä Concept limit: 15,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "‚úÖ Identified 15,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 17it [00:14,  1.19it/s]\n",
      "Building graph: 17it [00:14,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 267,699 edges, added 212,073 to graph\n",
      "üìä Graph has 14,990 nodes and 212,073 edges\n",
      "‚ú® Adding semantic enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding high confidence flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212073/212073 [00:00<00:00, 1576663.98it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enrichment completed in 0.2s\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter1.pkl in 0.2süíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter1.pkl in 0.2s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 14,990\n",
      "   Edges: 212,073\n",
      "   Processing time: 27.2s\n",
      "   Edges/second: 9847.9\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "\n",
      "üìä Iteration stats:\n",
      "   Nodes: 14,990\n",
      "   Edges: 212,073\n",
      "   Processing time: 27.2s\n",
      "   Edges/second: 9847.9\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 2/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.40\n",
      "üìä Concept limit: 30,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 2/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.40\n",
      "üìä Concept limit: 30,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "‚úÖ Identified 30,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.40...\n",
      "‚úÖ Identified 30,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 17it [00:16,  1.03it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 383,276 edges, added 309,247 to graph\n",
      "üìä Graph has 29,975 nodes and 309,247 edges\n",
      "‚ú® Adding semantic enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding high confidence flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 309247/309247 [00:00<00:00, 1543945.02it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Adding transitivity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding transitivity scores: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 309247/309247 [00:29<00:00, 10394.86it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enrichment completed in 30.1s\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter2.pkl in 0.3s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 29,975\n",
      "   Edges: 309,247\n",
      "   Processing time: 59.9s\n",
      "   Edges/second: 6394.5\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter2.pkl in 0.3s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 29,975\n",
      "   Edges: 309,247\n",
      "   Processing time: 59.9s\n",
      "   Edges/second: 6394.5\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 3/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.50\n",
      "üìä Concept limit: 45,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 3/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.50\n",
      "üìä Concept limit: 45,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "‚úÖ Identified 45,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.50...\n",
      "‚úÖ Identified 45,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 17it [00:18,  1.11s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 471,082 edges, added 386,340 to graph\n",
      "üìä Graph has 44,945 nodes and 386,340 edges\n",
      "‚ú® Adding semantic enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding high confidence flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386340/386340 [00:00<00:00, 1491353.73it/s]\n",
      "Adding high confidence flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386340/386340 [00:00<00:00, 1491353.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Adding centrality scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding centrality scores: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386340/386340 [05:15<00:00, 1224.05it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enrichment completed in 316.1s\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter3.pkl in 0.4s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 44,945\n",
      "   Edges: 386,340\n",
      "   Processing time: 348.4s\n",
      "   Edges/second: 1352.3\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter3.pkl in 0.4s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 44,945\n",
      "   Edges: 386,340\n",
      "   Processing time: 348.4s\n",
      "   Edges/second: 1352.3\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 4/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.60\n",
      "üìä Concept limit: 60,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "\n",
      "============================================================\n",
      "üìå ITERATION 4/4\n",
      "============================================================\n",
      "üìä Quality threshold: 0.60\n",
      "üìä Concept limit: 60,000\n",
      "üîç Scanning data to identify top concepts...\n",
      "‚úÖ Identified 60,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.60...\n",
      "‚úÖ Identified 60,000 most frequent concepts\n",
      "üîÑ Building graph with quality threshold 0.60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 17it [00:19,  1.17s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 545,269 edges, added 413,502 to graph\n",
      "üìä Graph has 58,986 nodes and 413,502 edges\n",
      "‚ú® Adding semantic enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding high confidence flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 413502/413502 [00:00<00:00, 1514660.65it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enrichment completed in 0.4s\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter4.pkl in 0.5s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 58,986\n",
      "   Edges: 413,502\n",
      "   Processing time: 34.7s\n",
      "   Edges/second: 15735.8\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "üíæ Saved to: ..\\Data\\Output\\nx_scaled_kg_iter4.pkl in 0.5s\n",
      "üìä Iteration stats:\n",
      "   Nodes: 58,986\n",
      "   Edges: 413,502\n",
      "   Processing time: 34.7s\n",
      "   Edges/second: 15735.8\n",
      "‚úÖ Checkpoint saved to: ..\\Data\\Output\\nx_scaled_kg_checkpoint.pkl\n",
      "\n",
      "============================================================\n",
      "üéâ HYPER-TRAINING COMPLETE\n",
      "============================================================\n",
      "Iteration 1: 14,990 nodes, 212,073 edges, 27.2s, threshold=0.30\n",
      "Iteration 2: 29,975 nodes, 309,247 edges, 59.9s, threshold=0.40\n",
      "Iteration 3: 44,945 nodes, 386,340 edges, 348.4s, threshold=0.50\n",
      "Iteration 4: 58,986 nodes, 413,502 edges, 34.7s, threshold=0.60\n",
      "\n",
      "============================================================\n",
      "üéâ HYPER-TRAINING COMPLETE\n",
      "============================================================\n",
      "Iteration 1: 14,990 nodes, 212,073 edges, 27.2s, threshold=0.30\n",
      "Iteration 2: 29,975 nodes, 309,247 edges, 59.9s, threshold=0.40\n",
      "Iteration 3: 44,945 nodes, 386,340 edges, 348.4s, threshold=0.50\n",
      "Iteration 4: 58,986 nodes, 413,502 edges, 34.7s, threshold=0.60\n",
      "üíæ Final model saved to: ..\\Data\\Output\\nx_scaled_kg_final.pkl\n",
      "üìä Training metrics saved to: ..\\Data\\Output\\nx_scaled_kg_metrics.json\n",
      "\n",
      "üîç Testing scalable model...\n",
      "computer --[AtLocation]--> apartment (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> backpack (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> box (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> building (weight=4.472, added in iteration 4)\n",
      "computer --[AtLocation]--> car (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> classroom (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> computer_store (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> demonstration (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> desktop (weight=4.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> desktop (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> house (weight=8.0, added in iteration 4)\n",
      "computer --[AtLocation]--> library (weight=4.0, added in iteration 4)\n",
      "computer --[AtLocation]--> office (weight=4.899, added in iteration 4)\n",
      "computer --[AtLocation]--> office_building (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> school (weight=6.633, added in iteration 4)\n",
      "computer --[AtLocation]--> space_shuttle (weight=2.0, added in iteration 4)\n",
      "computer --[AtLocation]--> table (weight=4.899, added in iteration 4)\n",
      "computer --[CapableOf]--> arithmetic (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> arithmetic (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> believe_in_god (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> calculate (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> cast_shadow (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> count (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> crash (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> do (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> heat_room (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> make_decisions (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> record_music (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> think (weight=1.0, added in iteration 4)\n",
      "computer --[DerivedFrom]--> compute (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> compute (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> keyboard (weight=2.0, added in iteration 4)\n",
      "computer --[HasA]--> monitor (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> mouse (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> software (weight=1.0, added in iteration 4)\n",
      "computer --[HasContext]--> computer_science (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> computer_science (weight=2.0, added in iteration 4)\n",
      "computer --[HasContext]--> location (weight=2.0, added in iteration 4)\n",
      "computer --[HasPrerequisite]--> electricity (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> expensive (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> machine (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> machine (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> ugly (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> cognition (weight=2.0, added in iteration 4)\n",
      "computer --[PartOf]--> cognition (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> dumb (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> electronic_appliance (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> junk (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> musical_instrument (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> tool (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> useful (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> artifact (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> communication (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> communication (weight=7.483, added in iteration 4)\n",
      "computer --[IsA]--> act (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> event (weight=2.0, added in iteration 4)\n",
      "computer --[MadeOf]--> hardware (weight=2.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> hardware (weight=1.0, added in iteration 4)\n",
      "computer --[MadeOf]--> silicon (weight=1.0, added in iteration 4)\n",
      "computer --[MannerOf]--> creation (weight=2.0, added in iteration 4)\n",
      "computer --[PartOf]--> network (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> found_in_house (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> found_in_school (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> programmed (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> taught (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> chip (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> laptop (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> mainframe (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> microprocessor (weight=1.0, added in iteration 4)\n",
      "computer --[SimilarTo]--> wn (weight=2.0, added in iteration 4)\n",
      "computer --[Synonym]--> computer (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> computer (weight=2.828, added in iteration 4)\n",
      "computer --[UsedFor]--> automation (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> calculating (weight=4.472, added in iteration 4)\n",
      "computer --[UsedFor]--> computing (weight=2.828, added in iteration 4)\n",
      "computer --[UsedFor]--> destructive (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> doing_math (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> doing_work (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> entertain_yourself (weight=3.464, added in iteration 4)\n",
      "computer --[UsedFor]--> entertainment (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> finding_information (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> homework (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> learn (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_money (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_music (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_shopping_list (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> play (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> play_games (weight=7.211, added in iteration 4)\n",
      "computer --[UsedFor]--> playing_games (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> send_email (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> solving (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> store_information (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> storing_information (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> surfing_web (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> word_processing (weight=4.0, added in iteration 4)\n",
      "computer --[UsedFor]--> work (weight=6.633, added in iteration 4)\n",
      "computer --[UsedFor]--> working (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> writing (weight=2.0, added in iteration 4)\n",
      "üíæ Final model saved to: ..\\Data\\Output\\nx_scaled_kg_final.pkl\n",
      "üìä Training metrics saved to: ..\\Data\\Output\\nx_scaled_kg_metrics.json\n",
      "\n",
      "üîç Testing scalable model...\n",
      "computer --[AtLocation]--> apartment (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> backpack (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> box (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> building (weight=4.472, added in iteration 4)\n",
      "computer --[AtLocation]--> car (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> classroom (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> computer_store (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> demonstration (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> desktop (weight=4.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> desktop (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> house (weight=8.0, added in iteration 4)\n",
      "computer --[AtLocation]--> library (weight=4.0, added in iteration 4)\n",
      "computer --[AtLocation]--> office (weight=4.899, added in iteration 4)\n",
      "computer --[AtLocation]--> office_building (weight=1.0, added in iteration 4)\n",
      "computer --[AtLocation]--> school (weight=6.633, added in iteration 4)\n",
      "computer --[AtLocation]--> space_shuttle (weight=2.0, added in iteration 4)\n",
      "computer --[AtLocation]--> table (weight=4.899, added in iteration 4)\n",
      "computer --[CapableOf]--> arithmetic (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> arithmetic (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> believe_in_god (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> calculate (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> cast_shadow (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> count (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> crash (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> do (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> heat_room (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> make_decisions (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> record_music (weight=1.0, added in iteration 4)\n",
      "computer --[CapableOf]--> think (weight=1.0, added in iteration 4)\n",
      "computer --[DerivedFrom]--> compute (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> compute (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> keyboard (weight=2.0, added in iteration 4)\n",
      "computer --[HasA]--> monitor (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> mouse (weight=1.0, added in iteration 4)\n",
      "computer --[HasA]--> software (weight=1.0, added in iteration 4)\n",
      "computer --[HasContext]--> computer_science (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> computer_science (weight=2.0, added in iteration 4)\n",
      "computer --[HasContext]--> location (weight=2.0, added in iteration 4)\n",
      "computer --[HasPrerequisite]--> electricity (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> expensive (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> machine (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> machine (weight=2.0, added in iteration 4)\n",
      "computer --[HasProperty]--> ugly (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> cognition (weight=2.0, added in iteration 4)\n",
      "computer --[PartOf]--> cognition (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> dumb (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> electronic_appliance (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> junk (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> musical_instrument (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> tool (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> useful (weight=1.0, added in iteration 4)\n",
      "computer --[IsA]--> artifact (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> communication (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> communication (weight=7.483, added in iteration 4)\n",
      "computer --[IsA]--> act (weight=2.0, added in iteration 4)\n",
      "computer --[IsA]--> event (weight=2.0, added in iteration 4)\n",
      "computer --[MadeOf]--> hardware (weight=2.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> hardware (weight=1.0, added in iteration 4)\n",
      "computer --[MadeOf]--> silicon (weight=1.0, added in iteration 4)\n",
      "computer --[MannerOf]--> creation (weight=2.0, added in iteration 4)\n",
      "computer --[PartOf]--> network (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> found_in_house (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> found_in_school (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> programmed (weight=1.0, added in iteration 4)\n",
      "computer --[ReceivesAction]--> taught (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> chip (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> laptop (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> mainframe (weight=1.0, added in iteration 4)\n",
      "computer --[RelatedTo]--> microprocessor (weight=1.0, added in iteration 4)\n",
      "computer --[SimilarTo]--> wn (weight=2.0, added in iteration 4)\n",
      "computer --[Synonym]--> computer (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> computer (weight=2.828, added in iteration 4)\n",
      "computer --[UsedFor]--> automation (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> calculating (weight=4.472, added in iteration 4)\n",
      "computer --[UsedFor]--> computing (weight=2.828, added in iteration 4)\n",
      "computer --[UsedFor]--> destructive (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> doing_math (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> doing_work (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> entertain_yourself (weight=3.464, added in iteration 4)\n",
      "computer --[UsedFor]--> entertainment (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> finding_information (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> homework (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> learn (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_money (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_music (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> make_shopping_list (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> play (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> play_games (weight=7.211, added in iteration 4)\n",
      "computer --[UsedFor]--> playing_games (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> send_email (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> solving (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> store_information (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> storing_information (weight=2.0, added in iteration 4)\n",
      "computer --[UsedFor]--> surfing_web (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> word_processing (weight=4.0, added in iteration 4)\n",
      "computer --[UsedFor]--> work (weight=6.633, added in iteration 4)\n",
      "computer --[UsedFor]--> working (weight=1.0, added in iteration 4)\n",
      "computer --[UsedFor]--> writing (weight=2.0, added in iteration 4)\n"
     ]
    }
   ],
   "source": [
    "# Configure the hyper-training parameters\n",
    "scalable_config = {\n",
    "    # Data filtering - start with lower threshold for broader coverage\n",
    "    'initial_quality_threshold': 0.3,\n",
    "    'quality_threshold_step': 0.1,\n",
    "    'max_quality_threshold': 0.6,  # Lower max threshold to keep more relationships\n",
    "    \n",
    "    # Concept limits - gradually scale up\n",
    "    'initial_concept_limit': 15000,\n",
    "    'concept_limit_step': 15000, \n",
    "    'max_concept_limit': 100000,\n",
    "    \n",
    "    # Memory optimization\n",
    "    'chunk_size': 100000,\n",
    "    'gc_frequency': 3,\n",
    "    \n",
    "    # Storage options - pickle is much faster and more compact\n",
    "    'save_format': 'pickle',\n",
    "    'compression': True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'enable_checkpoints': True,\n",
    "    'resume_from_checkpoint': True,\n",
    "}\n",
    "\n",
    "# Uncomment and run this cell to start the scalable training\n",
    "G_scalable, scalable_metrics, scalable_path = networkx_scalable_hyper_train(\n",
    "    data_path=conceptnet_path,\n",
    "    iterations=4,                # More iterations for gradual refinement\n",
    "    save_prefix=\"nx_scaled_kg\",  # Different prefix to avoid overwriting\n",
    "    config=scalable_config\n",
    ")\n",
    "\n",
    "# Test the final model\n",
    "print(f\"\\nüîç Testing scalable model...\")\n",
    "test_concept = 'computer'  # Try a different concept\n",
    "for u, v, data in G_scalable.out_edges(test_concept, data=True):\n",
    "    if data.get('high_confidence', False):\n",
    "        print(f\"{u} --[{data['relation']}]--> {v} (weight={data['weight']}, added in iteration {data['iteration_added']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cd7df",
   "metadata": {},
   "source": [
    "## 8. Advanced Knowledge Graph Exploration Tools (Scalable Version)\n",
    "\n",
    "The following tools are designed for advanced semantic exploration and analysis of large, semantically-enriched knowledge graphs produced by the scalable hyper-training workflow. You can use these tools to query, analyze, and visualize the graph, extract semantic paths, and perform advanced reasoning tasks.\n",
    "\n",
    "**How to use:**\n",
    "- Load your trained scalable graph (typically saved as a pickle file for efficiency).\n",
    "- Initialize the `KnowledgeGraphExplorer` with your graph.\n",
    "- Use the provided methods to explore concepts, find paths, analyze statistics, and more.\n",
    "- For very large graphs, ensure your environment has sufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28689684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphExplorer:\n",
    "    \"\"\"\n",
    "    Tools for exploring and analyzing a NetworkX-based knowledge graph.\n",
    "    Designed to work with ConceptNet-style semantic graphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, graph, relation_types=None):\n",
    "        \"\"\"Initialize the explorer with a graph\"\"\"\n",
    "        self.graph = graph\n",
    "        \n",
    "        # Filter relation types if specified, otherwise use all\n",
    "        if relation_types is None:\n",
    "            self.relation_types = self._get_all_relation_types()\n",
    "        else:\n",
    "            self.relation_types = relation_types\n",
    "            \n",
    "        # Cache for performance\n",
    "        self._node_centrality = None\n",
    "        \n",
    "    def _get_all_relation_types(self):\n",
    "        \"\"\"Get all unique relation types in the graph\"\"\"\n",
    "        relation_types = set()\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            if 'relation' in data:\n",
    "                relation_types.add(data['relation'])\n",
    "        return list(relation_types)\n",
    "    \n",
    "    def explore_concept(self, concept, relation_filter=None, min_weight=0.0, limit=10):\n",
    "        \"\"\"\n",
    "        Get the most important relationships for a concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        concept : str\n",
    "            The concept to explore\n",
    "        relation_filter : list or None\n",
    "            Filter by relation types (e.g., ['IsA', 'PartOf'])\n",
    "        min_weight : float\n",
    "            Minimum weight threshold\n",
    "        limit : int\n",
    "            Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with source, target, relation, weight\n",
    "        \"\"\"\n",
    "        if concept not in self.graph:\n",
    "            return f\"Concept '{concept}' not found in the graph.\"\n",
    "        \n",
    "        # Collect outgoing and incoming edges\n",
    "        edges = []\n",
    "        \n",
    "        # Outgoing edges\n",
    "        for _, target, data in self.graph.out_edges(concept, data=True):\n",
    "            if (relation_filter is None or data.get('relation') in relation_filter) and data.get('weight', 0) >= min_weight:\n",
    "                edges.append({\n",
    "                    'source': concept,\n",
    "                    'target': target,\n",
    "                    'relation': data.get('relation', 'unknown'),\n",
    "                    'weight': data.get('weight', 0.0),\n",
    "                    'direction': 'outgoing',\n",
    "                    'high_confidence': data.get('high_confidence', False)\n",
    "                })\n",
    "        \n",
    "        # Incoming edges\n",
    "        for source, _, data in self.graph.in_edges(concept, data=True):\n",
    "            if (relation_filter is None or data.get('relation') in relation_filter) and data.get('weight', 0) >= min_weight:\n",
    "                edges.append({\n",
    "                    'source': source,\n",
    "                    'target': concept,\n",
    "                    'relation': data.get('relation', 'unknown'),\n",
    "                    'weight': data.get('weight', 0.0),\n",
    "                    'direction': 'incoming',\n",
    "                    'high_confidence': data.get('high_confidence', False)\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame and sort by weight\n",
    "        import pandas as pd\n",
    "        edges_df = pd.DataFrame(edges)\n",
    "        \n",
    "        if len(edges_df) > 0:\n",
    "            edges_df = edges_df.sort_values('weight', ascending=False).head(limit)\n",
    "        \n",
    "        return edges_df\n",
    "    \n",
    "    def find_path(self, source, target, relation_filter=None, max_length=3):\n",
    "        \"\"\"\n",
    "        Find the shortest path between two concepts, considering only certain relations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source : str\n",
    "            Source concept\n",
    "        target : str\n",
    "            Target concept\n",
    "        relation_filter : list or None\n",
    "            Filter by relation types\n",
    "        max_length : int\n",
    "            Maximum path length\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (concept, relation, concept) tuples, or None if no path\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Check if concepts exist\n",
    "        if source not in self.graph or target not in self.graph:\n",
    "            return f\"One or both concepts not found: {source}, {target}\"\n",
    "        \n",
    "        # Create a view of the graph with only the desired relations\n",
    "        if relation_filter is not None:\n",
    "            view = nx.MultiDiGraph()\n",
    "            \n",
    "            for u, v, key, data in self.graph.edges(keys=True, data=True):\n",
    "                if data.get('relation') in relation_filter:\n",
    "                    view.add_edge(u, v, key=key, **data)\n",
    "                    \n",
    "            search_graph = view\n",
    "        else:\n",
    "            search_graph = self.graph\n",
    "        \n",
    "        # Find the shortest path\n",
    "        try:\n",
    "            path = nx.shortest_path(search_graph, source, target, weight=lambda u, v, data: 1/max(data.get('weight', 0.1), 0.1))\n",
    "            \n",
    "            # Construct the full path with relations\n",
    "            full_path = []\n",
    "            for i in range(len(path)-1):\n",
    "                edges = search_graph.get_edge_data(path[i], path[i+1])\n",
    "                if not edges:  # No edge exists (shouldn't happen given how path was found)\n",
    "                    relation = 'unknown'\n",
    "                    weight = 0.0\n",
    "                else:\n",
    "                    # Get the highest weighted edge if multiple\n",
    "                    best_key = max(edges.keys(), key=lambda k: edges[k].get('weight', 0))\n",
    "                    relation = edges[best_key].get('relation', 'unknown')\n",
    "                    weight = edges[best_key].get('weight', 0.0)\n",
    "                \n",
    "                full_path.append((path[i], relation, path[i+1], weight))\n",
    "            \n",
    "            return full_path\n",
    "            \n",
    "        except nx.NetworkXNoPath:\n",
    "            return f\"No path found between {source} and {target} with given constraints.\"\n",
    "    \n",
    "    def get_top_concepts(self, n=20, measure='degree'):\n",
    "        \"\"\"\n",
    "        Get the top N concepts by a centrality measure\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            Number of concepts to return\n",
    "        measure : str\n",
    "            Centrality measure ('degree', 'in_degree', 'out_degree', 'pagerank')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with concepts and their centrality scores\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import networkx as nx\n",
    "        \n",
    "        if measure == 'degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.degree()}\n",
    "        elif measure == 'in_degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.in_degree()}\n",
    "        elif measure == 'out_degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.out_degree()}\n",
    "        elif measure == 'pagerank':\n",
    "            # Compute pagerank (can be slow for large graphs)\n",
    "            print(\"Computing PageRank (this may take a while)...\")\n",
    "            centrality = nx.pagerank(self.graph, alpha=0.85, weight='weight')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown centrality measure: {measure}\")\n",
    "        \n",
    "        # Convert to DataFrame and get top N\n",
    "        df = pd.DataFrame(centrality.items(), columns=['concept', 'score'])\n",
    "        df = df.nlargest(n, 'score')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def analyze_graph_stats(self):\n",
    "        \"\"\"\n",
    "        Compute basic statistics about the knowledge graph\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with graph statistics\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        import numpy as np\n",
    "        \n",
    "        stats = {\n",
    "            'nodes': self.graph.number_of_nodes(),\n",
    "            'edges': self.graph.number_of_edges(),\n",
    "            'density': nx.density(self.graph),\n",
    "            'active_nodes': sum(1 for node in self.graph.nodes() if self.graph.degree(node) > 0),\n",
    "            'isolated_nodes': sum(1 for node in self.graph.nodes() if self.graph.degree(node) == 0),\n",
    "            'avg_degree': np.mean([d for n, d in self.graph.degree()]),\n",
    "            'max_degree': max([d for n, d in self.graph.degree()]) if self.graph.number_of_nodes() > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Add weight statistics if available\n",
    "        weights = [data.get('weight', 0) for _, _, data in self.graph.edges(data=True)]\n",
    "        if weights:\n",
    "            stats.update({\n",
    "                'min_weight': min(weights),\n",
    "                'max_weight': max(weights),\n",
    "                'avg_weight': np.mean(weights),\n",
    "                'median_weight': np.median(weights)\n",
    "            })\n",
    "        \n",
    "        # Add relation type counts\n",
    "        relation_counts = {}\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            relation = data.get('relation', 'unknown')\n",
    "            relation_counts[relation] = relation_counts.get(relation, 0) + 1\n",
    "        \n",
    "        stats['relation_counts'] = relation_counts\n",
    "        stats['unique_relations'] = len(relation_counts)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def visualize_concept_network(self, central_concept, depth=1, min_weight=0.5, relation_filter=None, max_nodes=30):\n",
    "        \"\"\"\n",
    "        Visualize the network around a concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        central_concept : str\n",
    "            The central concept to visualize\n",
    "        depth : int\n",
    "            How many hops to include\n",
    "        min_weight : float\n",
    "            Minimum edge weight\n",
    "        relation_filter : list\n",
    "            Only include these relation types\n",
    "        max_nodes : int\n",
    "            Maximum number of nodes to include\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        networkx.Graph\n",
    "            A subgraph centered on the concept\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        \n",
    "        if central_concept not in self.graph:\n",
    "            return f\"Concept '{central_concept}' not found in the graph.\"\n",
    "        \n",
    "        # Start with the central concept\n",
    "        nodes_to_include = {central_concept}\n",
    "        \n",
    "        # BFS to add concepts up to the specified depth\n",
    "        current_depth = 0\n",
    "        frontier = {central_concept}\n",
    "        \n",
    "        while current_depth < depth and len(nodes_to_include) < max_nodes:\n",
    "            next_frontier = set()\n",
    "            \n",
    "            for node in frontier:\n",
    "                # Add outgoing edges\n",
    "                for _, target, data in self.graph.out_edges(node, data=True):\n",
    "                    if (len(nodes_to_include) < max_nodes and \n",
    "                        target not in nodes_to_include and\n",
    "                        data.get('weight', 0) >= min_weight and\n",
    "                        (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                        nodes_to_include.add(target)\n",
    "                        next_frontier.add(target)\n",
    "                \n",
    "                # Add incoming edges\n",
    "                for source, _, data in self.graph.in_edges(node, data=True):\n",
    "                    if (len(nodes_to_include) < max_nodes and\n",
    "                        source not in nodes_to_include and\n",
    "                        data.get('weight', 0) >= min_weight and\n",
    "                        (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                        nodes_to_include.add(source)\n",
    "                        next_frontier.add(source)\n",
    "            \n",
    "            frontier = next_frontier\n",
    "            current_depth += 1\n",
    "        \n",
    "        # Extract the subgraph\n",
    "        subgraph = self.graph.subgraph(nodes_to_include)\n",
    "        \n",
    "        # Filter edges by weight and relation\n",
    "        view = nx.MultiDiGraph()\n",
    "        view.add_nodes_from(subgraph.nodes(data=True))\n",
    "        \n",
    "        for u, v, key, data in subgraph.edges(keys=True, data=True):\n",
    "            if (data.get('weight', 0) >= min_weight and \n",
    "                (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                view.add_edge(u, v, key=key, **data)\n",
    "        \n",
    "        return view\n",
    "    \n",
    "    def search_by_pattern(self, pattern, relation_types=None, limit=20):\n",
    "        \"\"\"\n",
    "        Search the graph for concepts matching a pattern\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pattern : str\n",
    "            Text pattern to search for\n",
    "        relation_types : list\n",
    "            Limit search to edges with these relations\n",
    "        limit : int\n",
    "            Maximum results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of matching concepts\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        pattern = pattern.lower()\n",
    "        regex = re.compile(pattern)\n",
    "        \n",
    "        # Search for matching nodes\n",
    "        matches = []\n",
    "        for node in self.graph.nodes():\n",
    "            if regex.search(str(node).lower()):\n",
    "                matches.append(node)\n",
    "                if len(matches) >= limit:\n",
    "                    break\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def extract_taxonomy(self, root_concept, relation_types=['IsA'], max_depth=5):\n",
    "        \"\"\"\n",
    "        Extract a taxonomy (tree) starting from a root concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        root_concept : str\n",
    "            Starting concept\n",
    "        relation_types : list\n",
    "            Relations to consider for the taxonomy (default: IsA)\n",
    "        max_depth : int\n",
    "            Maximum depth to traverse\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Nested dictionary representing the taxonomy\n",
    "        \"\"\"\n",
    "        if root_concept not in self.graph:\n",
    "            return f\"Concept '{root_concept}' not found in the graph.\"\n",
    "        \n",
    "        def build_tree(concept, depth=0):\n",
    "            if depth >= max_depth:\n",
    "                return {}\n",
    "            \n",
    "            tree = {}\n",
    "            \n",
    "            # Get all outgoing edges with the specified relation types\n",
    "            for _, target, data in self.graph.out_edges(concept, data=True):\n",
    "                if data.get('relation') in relation_types:\n",
    "                    tree[target] = build_tree(target, depth + 1)\n",
    "            \n",
    "            return tree\n",
    "        \n",
    "        taxonomy = {root_concept: build_tree(root_concept)}\n",
    "        return taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a86d4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Statistics:\n",
      "  nodes: 29769\n",
      "  edges: 279422\n",
      "  density: 0.0003153164974532152\n",
      "  active_nodes: 29769\n",
      "  isolated_nodes: 0\n",
      "  avg_degree: 18.77268299237462\n",
      "  max_degree: 67357\n",
      "  min_weight: 0.7\n",
      "  max_weight: 22.891\n",
      "  avg_weight: 1.2029343931401246\n",
      "  median_weight: 1.0\n",
      "  unique_relations: 39\n",
      "\n",
      "Exploring 'dog':\n",
      "   source         target    relation  weight direction  high_confidence\n",
      "14    dog           bark   CapableOf  16.000  outgoing             True\n",
      "20    dog            pet   RelatedTo   9.830  outgoing             True\n",
      "47    dog         animal   RelatedTo   9.410  outgoing             True\n",
      "8     dog         kennel  AtLocation   9.381  outgoing             True\n",
      "73   flea            dog   RelatedTo   9.021  incoming             True\n",
      "40    dog         canine   RelatedTo   7.626  outgoing             True\n",
      "18    dog            pet   CapableOf   7.483  outgoing             True\n",
      "62    dog  companionship     UsedFor   6.325  outgoing             True\n",
      "23    dog            run   CapableOf   6.000  outgoing             True\n",
      "19    dog            pet         IsA   6.000  outgoing             True\n",
      "\n",
      "Finding path from 'dog' to 'computer':\n",
      "[('dog', 'AtLocation', 'desk', 1.0), ('desk', 'RelatedTo', 'computer', 3.834)]\n",
      "\n",
      "Top concepts by degree:\n",
      "     concept  score\n",
      "0          n  67357\n",
      "16         a  30986\n",
      "18         v  28937\n",
      "6       en_1  16117\n",
      "230     en_2  10225\n",
      "3          r   9964\n",
      "587     en_3   3272\n",
      "1013  person   1276\n",
      "610     en_4   1086\n",
      "1803   water    452\n"
     ]
    }
   ],
   "source": [
    "# Example of using the explorer with the trained knowledge graph\n",
    "# Uncomment and run after training\n",
    "\n",
    "# Load a trained graph\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# You can load either a GraphML file or a pickle file\n",
    "trained_graph_path = os.path.join('..', 'Data', 'Output', 'nx_semantic_final.graphml')\n",
    "G_trained = nx.read_graphml(trained_graph_path)\n",
    "\n",
    "# Initialize the explorer\n",
    "explorer = KnowledgeGraphExplorer(G_trained)\n",
    "\n",
    "# Get basic graph stats\n",
    "stats = explorer.analyze_graph_stats()\n",
    "print(\"Graph Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if key != 'relation_counts':  # Skip printing the full relation counts\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Explore a concept\n",
    "print(\"\\nExploring 'dog':\")\n",
    "dog_relations = explorer.explore_concept('dog', min_weight=1.5, limit=10)\n",
    "print(dog_relations)\n",
    "\n",
    "# Find a path between concepts\n",
    "print(\"\\nFinding path from 'dog' to 'computer':\")\n",
    "path = explorer.find_path('dog', 'computer', max_length=3)\n",
    "print(path)\n",
    "\n",
    "# Get top concepts\n",
    "print(\"\\nTop concepts by degree:\")\n",
    "top_concepts = explorer.get_top_concepts(10, 'degree')\n",
    "print(top_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70916741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
