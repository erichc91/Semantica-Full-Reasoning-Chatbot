{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce0a31a",
   "metadata": {},
   "source": [
    "# Building a Scalable Semantic Graph from ConceptNet with NetworkX\n",
    "This notebook demonstrates how to construct a scalable semantic graph database from cleaned ConceptNet triples using NetworkX. It covers loading the data, building a graph, performing basic queries, and saving/loading the graph in a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b1c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c998847",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned ConceptNet Data\n",
    "We use the preprocessed ConceptNet triples stored in Parquet format. Adjust the path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39228ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "relation_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_concept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end_concept",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "edge_weight",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "07a5ae27-df46-4ca9-be6d-962e8b6652a5",
       "rows": [
        [
         "0",
         "Antonym",
         "n",
         "1",
         "1.0"
        ],
        [
         "1",
         "Antonym",
         "n",
         "24_hour_clock",
         "1.0"
        ],
        [
         "2",
         "Antonym",
         "n",
         "12_hour_clock",
         "1.0"
        ],
        [
         "3",
         "Antonym",
         "n",
         "3",
         "1.0"
        ],
        [
         "4",
         "Antonym",
         "n",
         "d.c",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation_type</th>\n",
       "      <th>start_concept</th>\n",
       "      <th>end_concept</th>\n",
       "      <th>edge_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>24_hour_clock</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>12_hour_clock</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Antonym</td>\n",
       "      <td>n</td>\n",
       "      <td>d.c</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relation_type start_concept    end_concept  edge_weight\n",
       "0       Antonym             n              1          1.0\n",
       "1       Antonym             n  24_hour_clock          1.0\n",
       "2       Antonym             n  12_hour_clock          1.0\n",
       "3       Antonym             n              3          1.0\n",
       "4       Antonym             n            d.c          1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the cleaned ConceptNet data\n",
    "conceptnet_path = os.path.join('..', 'Data', 'Input', 'conceptnet_en_processed_for_graph.parquet.gzip')\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_parquet(conceptnet_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f348",
   "metadata": {},
   "source": [
    "## 2. Build a NetworkX MultiDiGraph from the Edge List\n",
    "Each ConceptNet triple becomes a directed edge with attributes (relation, weight, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ab240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 754380 nodes and 1655522 edges.\n"
     ]
    }
   ],
   "source": [
    "# Create a MultiDiGraph (allows multiple edges between nodes)\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# Add edges from the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(\n",
    "        row['start_concept'],\n",
    "        row['end_concept'],\n",
    "        relation=row['relation_type'],\n",
    "        weight=row.get('edge_weight', 1.0)\n",
    "    )\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babc0e0",
   "metadata": {},
   "source": [
    "## 3. Basic Graph Database Operations\n",
    "Query neighbors, edge attributes, and extract subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757d5863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of 'dog': ['again', 'car', 'cat', 'cat_again', 'cat_maybe', 'computer', 'maybe', 'woof', 'back_yard', 'backyard', 'bed', 'couch', 'desk', 'dog_house', \"dog_owner's_home\", 'doghouse', 'dogpound', 'farmyard', 'front_door', 'ground', 'house', \"it's_kennel\", 'kennel', 'leash', \"neighbor's_house\", 'outside', 'park', 'pet_shop', 'pet_store', 'petshop', 'porch', 'pound', 'relatives_house', 'rug', 'table', 'act_playful', 'answer_to_master', 'appear_tired', 'bark', 'bark_at_strangers', 'barks_when_hears_steps', 'become_pet', 'belong_human', 'bite', 'breathe', 'breed_several_puppies', 'bring_master_bone', 'bury_bone_in_ground', 'calm_mind', 'cause_accident', 'cause_rabbit_to_run_away', 'chase_ball', 'chasing_ball', 'circle_sheep', 'circle_tree', 'come_home', 'come_to_master', 'come_when_call_name', 'company_man', 'corner_cat', 'course_hare', 'cover_hole_in_ground', 'cross_street', 'dig_up_bone', 'dig_up_bones', 'dream_caught_rabbit', 'drink_water', \"eat_cat_food_but_probably_shouldn't\", 'enjoy_food', 'enjoy_to_play_in_lake', 'enter_doghouse', 'eye_icecream', \"eye_person's_sandwich\", 'fancy_toy', 'father_puppy', \"feed_it's_babies\", 'fetch_ball', 'fight_another_dog', 'fight_cat', 'find_food_to_eat', \"follow_it's_owner\", 'follow_master', 'follow_person', 'follow_scent', 'following_owner', 'frighten_intruders', 'get_stick', 'guard_building', 'guard_home', 'guard_house', 'guard_premises', 'guard_property', 'guide_blind', 'guide_blind_human', 'guide_blind_person', 'guide_blind_woman', 'hear_high_pitches', 'hear_sub_sonic_sounds', 'hear_whistle', 'hear_whistling', 'hear_wolf', 'help_master', 'hunt_cat', 'jump_over_log', 'lead_police_to_dead_body', 'learn_how_to_beg', 'learn_to_do_tricks', 'learn_to_fetch_things', 'lick_privates', 'lie_down', 'lie_down_after_eating', 'like_going_for_walk', 'listen_to_master', 'live_in_dog_house', 'live_in_house', 'long_for_dead_owner', 'love_master', 'mark_territory', 'mark_territory_with_urine', 'mark_tree', 'master_new_trick', \"mother_it's_young\", 'mother_puppy', 'not_eat_bone_of_contention', 'open_door', 'pet', 'piss_on_couch', 'place_bone_in_hole', 'play_fetch', 'play_frisbee', 'play_with_bone', 'please_master', 'pull_sleigh', 'put_paw_on_knee', 'remember_basic_commands', 'roll_over', 'run', 'run_after_mailman', 'run_away_from_master', 'run_fasr', 'see_in_black_and_white', 'sense_danger', 'sense_fear', 'shake_hands', 'shake_slipper', 'sleep_long_time', 'smell_cooking_meat', 'smell_crotch', 'smell_drugs', 'smell_fear', 'smell_food', 'smell_scents', 'smell_trace_of_another_dog', 'spot_movement', 'stand_on_hindlegs', 'stand_on_two_legs', 'stay_in_house', 'stay_in_place', 'stray', 'taste_dog_food', 'tear_rug', 'tear_shirt', 'think_about_food', 'trip_over_rope', 'try_to_please_master', 'turn_over', 'understand_simple_commands', 'urinate', 'wag_tail', 'walk_man', 'walk_on_leash', 'wear_collar', 'wear_sweater', 'win_blue_ribbon', 'go_for_walk', 'jog', 'take_walk', 'affection', 'attention', 'bone', 'dig', 'eat', 'eat_lot', 'food', 'go_outside', 'lots_of_attention', 'loved_by_master', 'meat', 'petted', 'play', 'walked', 'cut', 'dogge', 'hound', 'mastiff', 'see_man_about_dog', 'big_heart', 'brains', 'fleas', 'four_legs', 'fur', 'nose', 'one_mouth', 'paws', 'penis', 'teeth', 'two_ears', 'alive', 'black', 'brown', 'fun', 'gray', 'larger_than_cat', 'mean', 'one_among_many_animals', 'playing_dead', 'running_in_dream', 'very_common_pet', 'worth_much_more_than_dime', 'pee_against_trees', 'canine', 'curious_observer_of_mankind', 'cuter_than_kids', 'domestic_animal', 'domesticated_canine', 'example_of_pet', 'faithful_companion', 'four_legged_animal', 'good_friend', 'growling_at_man', 'happy_when_wags_tail', 'loyal_friend', 'mammal', 'n', 'mans_best_friend', 'nice_friend', 'quadriped', 'shedding', 'so_big', 'straining_at_leash', 'thing', 'usually_about_3_feet_tall', 'usually_about_two_feet_tall', 'very_faithful_friend', 'flesh_and_bones', 'hair', 'abandoned', 'bath', 'eat_dinner', 'go_to_vet', 'hear_loud_noises', 'left_alone', 'left_home_alone', 'punishment', 'shooed_off_couch', 'starve', 'animal_kingdom', 'bought_at_pet_store', 'fed', 'kept_in_kennel', 'kicked', 'loved', 'heredity', 'animal', 'animal_barks', 'animal_canine', 'animal_companion', 'animal_furry', 'animal_pet', 'animal_tail', 'backwards', 'bark_animal', 'bark_bark', 'bark_bite', 'bark_woof', 'barking', 'barking_animal', 'barking_fetching', 'barking_pet', 'barks', 'beagle', 'beagle_hound', 'best', 'best_friend', 'best_pet', 'big', 'big_puppy', 'bow', 'bow_bow', 'bow_wow', 'canine_animal', 'canine_friend', 'canine_pet', 'canine_pooch', 'canines', 'cat_and', 'cat_chaser', 'cat_enemy', 'cat_hater', 'cats', 'cats_and', 'cats_enemy', 'chaser', 'collar', 'common', 'common_pet', 'companion', 'companionship', 'conformation_show', 'coyote', 'creature', 'critter', 'cur', 'cute', 'cynomorphic', 'cynomorphism', 'digital', 'dog_food', 'domestic', 'domestic_pet', 'domestic_wolf', 'domesticated', 'domesticated_animal', 'domesticated_wolf', 'ears', 'enemy', 'et', 'faithful', 'faithful_animal', 'family', 'family_pet', 'feline', 'feline_animal', 'fetching', 'flea', 'flea_animal', 'flea_bag', 'fluffy', 'four', 'four_legged', 'four_paws', 'fox', 'friend', 'friendly', 'friendly_mammal', 'furry', 'furry_animal', 'furry_creature', 'furry_pet', 'fuzzy', 'general', 'general_canine', 'god', 'god_backwards', 'good', 'good_pet', 'graphic', 'greyhound', 'grown', 'grown_puppy', 'guard', 'hairy', 'hater', 'home', 'home_animal', 'homes', 'horse', 'hound_animal', 'house_animal', 'house_pet', 'household', 'household_animal', 'household_et', 'household_pet', 'houses', 'humans', 'hunting', 'it_barks', 'k', 'k_nine', 'labrador', 'large', 'large_cat', 'large_puppy', 'lassie', 'leach', 'legged', 'legs', 'legs_tail', 'little', 'little_horse', 'loving', 'loyal', 'loyal_animal', 'loyal_pet', 'lupine', 'man', 'man_friend', 'mans', 'mans_best', 'mans_friend', 'max', 'mutt', 'muzzle', 'nine', 'old', 'old_puppy', 'on', 'parent', 'parks', 'pe', 'people', 'per', 'pet_animal', 'pet_barking', 'pet_canine', 'pet_puppy', 'pet_sorry', 'pet_woof', 'pets', 'pluto', 'pooch', 'poodle', 'pup', 'puppies', 'puppy', 'puppy_parent', 'raining', 'raining_cat', 'retriever', 'reverse', 'reverse_god', 'rover', 'schnauzer', 'screen', 'small', 'small_animal', 'small_fox', 'small_horse', 'snoopy', 'sorry', 'stupid', 'tail', 'tail_barks', 'tail_ears', 'tailed', 'tailed_animal', 'teeth_tail', 'terrier', 'tics', 'typical', 'typical_pet', 'wagging', 'wagging_tail', 'wet', 'wet_nose', 'wolf', 'wolf_family', 'wolf_wolf', 'wolves', 'woof_animal', 'woof_bark', 'woof_woof', 'woofer', 'wow', 'ðŸ•', 'dog', 'bird_hunting', 'biting_postman', 'breeding', 'chasing_cats', 'comforting_elderly', 'companionship_and_protection', 'company', 'dog_experiments', 'entertaining_people', 'fetching_sticks', 'give_comfort', 'guarding_home', 'guarding_junkyard', 'guarding_piece_of_property', 'guarding_property', 'helping_blind_person_to_navigate', 'helping_to_control_livestock', 'herding_sheep', 'keep_company', 'keep_watrm', 'keeping_company', 'love', 'playing', 'protect_belongings', 'protecting_livestock', 'providing_friendship', 'safty', 'scare_away_bad_guy', 'sniffing_out_drugs', 'sniffing_out_explosives', 'tracking_animal', 'tracking_criminal', 'watch_house', 'canis']\n",
      "dog --[Antonym]--> again (weight=0.183)\n",
      "dog --[Antonym]--> car (weight=0.122)\n",
      "dog --[Antonym]--> cat (weight=3.686)\n",
      "dog --[DistinctFrom]--> cat (weight=4.606)\n",
      "dog --[RelatedTo]--> cat (weight=3.622)\n",
      "dog --[Antonym]--> cat_again (weight=0.183)\n",
      "dog --[Antonym]--> cat_maybe (weight=0.306)\n",
      "dog --[Antonym]--> computer (weight=0.112)\n",
      "dog --[Antonym]--> maybe (weight=0.306)\n",
      "dog --[Antonym]--> woof (weight=0.196)\n",
      "dog --[RelatedTo]--> woof (weight=4.396)\n",
      "dog --[AtLocation]--> back_yard (weight=1.0)\n",
      "dog --[AtLocation]--> backyard (weight=3.464)\n",
      "dog --[AtLocation]--> bed (weight=2.0)\n",
      "dog --[AtLocation]--> couch (weight=2.0)\n",
      "dog --[AtLocation]--> desk (weight=1.0)\n",
      "dog --[AtLocation]--> dog_house (weight=4.0)\n",
      "dog --[AtLocation]--> dog_owner's_home (weight=1.0)\n",
      "dog --[AtLocation]--> doghouse (weight=4.472)\n",
      "dog --[RelatedTo]--> doghouse (weight=0.316)\n",
      "dog --[AtLocation]--> dogpound (weight=2.0)\n",
      "dog --[AtLocation]--> farmyard (weight=1.0)\n",
      "dog --[AtLocation]--> front_door (weight=1.0)\n",
      "dog --[AtLocation]--> ground (weight=1.0)\n",
      "dog --[AtLocation]--> house (weight=2.828)\n",
      "dog --[RelatedTo]--> house (weight=2.672)\n",
      "dog --[AtLocation]--> it's_kennel (weight=1.0)\n",
      "dog --[AtLocation]--> kennel (weight=9.381)\n",
      "dog --[RelatedTo]--> kennel (weight=0.274)\n",
      "dog --[AtLocation]--> leash (weight=2.0)\n",
      "dog --[RelatedTo]--> leash (weight=0.228)\n",
      "dog --[AtLocation]--> neighbor's_house (weight=1.0)\n",
      "dog --[AtLocation]--> outside (weight=1.0)\n",
      "dog --[AtLocation]--> park (weight=4.899)\n",
      "dog --[AtLocation]--> pet_shop (weight=1.0)\n",
      "dog --[AtLocation]--> pet_store (weight=1.0)\n",
      "dog --[AtLocation]--> petshop (weight=1.0)\n",
      "dog --[AtLocation]--> porch (weight=3.464)\n",
      "dog --[AtLocation]--> pound (weight=1.0)\n",
      "dog --[AtLocation]--> relatives_house (weight=1.0)\n",
      "dog --[AtLocation]--> rug (weight=2.828)\n",
      "dog --[AtLocation]--> table (weight=5.657)\n",
      "dog --[CapableOf]--> act_playful (weight=1.0)\n",
      "dog --[CapableOf]--> answer_to_master (weight=1.0)\n",
      "dog --[CapableOf]--> appear_tired (weight=1.0)\n",
      "dog --[CapableOf]--> bark (weight=16.0)\n",
      "dog --[HasSubevent]--> bark (weight=1.0)\n",
      "dog --[RelatedTo]--> bark (weight=3.977)\n",
      "dog --[CapableOf]--> bark_at_strangers (weight=2.0)\n",
      "dog --[CapableOf]--> barks_when_hears_steps (weight=1.0)\n",
      "dog --[CapableOf]--> become_pet (weight=1.0)\n",
      "dog --[CapableOf]--> belong_human (weight=1.0)\n",
      "dog --[CapableOf]--> bite (weight=2.0)\n",
      "dog --[RelatedTo]--> bite (weight=0.177)\n",
      "dog --[CapableOf]--> breathe (weight=1.0)\n",
      "dog --[CapableOf]--> breed_several_puppies (weight=1.0)\n",
      "dog --[CapableOf]--> bring_master_bone (weight=1.0)\n",
      "dog --[CapableOf]--> bury_bone_in_ground (weight=1.0)\n",
      "dog --[CapableOf]--> calm_mind (weight=1.0)\n",
      "dog --[CapableOf]--> cause_accident (weight=1.0)\n",
      "dog --[CapableOf]--> cause_rabbit_to_run_away (weight=1.0)\n",
      "dog --[CapableOf]--> chase_ball (weight=1.0)\n",
      "dog --[CapableOf]--> chasing_ball (weight=2.828)\n",
      "dog --[CapableOf]--> circle_sheep (weight=1.0)\n",
      "dog --[CapableOf]--> circle_tree (weight=1.0)\n",
      "dog --[CapableOf]--> come_home (weight=1.0)\n",
      "dog --[CapableOf]--> come_to_master (weight=1.0)\n",
      "dog --[CapableOf]--> come_when_call_name (weight=1.0)\n",
      "dog --[CapableOf]--> company_man (weight=1.0)\n",
      "dog --[CapableOf]--> corner_cat (weight=2.828)\n",
      "dog --[CapableOf]--> course_hare (weight=1.0)\n",
      "dog --[CapableOf]--> cover_hole_in_ground (weight=1.0)\n",
      "dog --[CapableOf]--> cross_street (weight=1.0)\n",
      "dog --[CapableOf]--> dig_up_bone (weight=2.828)\n",
      "dog --[CapableOf]--> dig_up_bones (weight=2.828)\n",
      "dog --[CapableOf]--> dream_caught_rabbit (weight=1.0)\n",
      "dog --[CapableOf]--> drink_water (weight=4.472)\n",
      "dog --[CapableOf]--> eat_cat_food_but_probably_shouldn't (weight=1.0)\n",
      "dog --[CapableOf]--> enjoy_food (weight=1.0)\n",
      "dog --[CapableOf]--> enjoy_to_play_in_lake (weight=1.0)\n",
      "dog --[CapableOf]--> enter_doghouse (weight=1.0)\n",
      "dog --[CapableOf]--> eye_icecream (weight=1.0)\n",
      "dog --[CapableOf]--> eye_person's_sandwich (weight=1.0)\n",
      "dog --[CapableOf]--> fancy_toy (weight=1.0)\n",
      "dog --[CapableOf]--> father_puppy (weight=1.0)\n",
      "dog --[CapableOf]--> feed_it's_babies (weight=1.0)\n",
      "dog --[CapableOf]--> fetch_ball (weight=1.0)\n",
      "dog --[CapableOf]--> fight_another_dog (weight=1.0)\n",
      "dog --[CapableOf]--> fight_cat (weight=2.828)\n",
      "dog --[CapableOf]--> find_food_to_eat (weight=1.0)\n",
      "dog --[CapableOf]--> follow_it's_owner (weight=1.0)\n",
      "dog --[CapableOf]--> follow_master (weight=1.0)\n",
      "dog --[CapableOf]--> follow_person (weight=1.0)\n",
      "dog --[CapableOf]--> follow_scent (weight=1.0)\n",
      "dog --[CapableOf]--> following_owner (weight=1.0)\n",
      "dog --[CapableOf]--> frighten_intruders (weight=2.0)\n",
      "dog --[CapableOf]--> get_stick (weight=1.0)\n",
      "dog --[CapableOf]--> guard_building (weight=1.0)\n",
      "dog --[CapableOf]--> guard_home (weight=1.0)\n",
      "dog --[CapableOf]--> guard_house (weight=10.392)\n",
      "dog --[UsedFor]--> guard_house (weight=2.0)\n",
      "dog --[CapableOf]--> guard_premises (weight=1.0)\n",
      "dog --[CapableOf]--> guard_property (weight=4.472)\n",
      "dog --[CapableOf]--> guide_blind (weight=2.828)\n",
      "dog --[CapableOf]--> guide_blind_human (weight=1.0)\n",
      "dog --[CapableOf]--> guide_blind_person (weight=5.292)\n",
      "dog --[CapableOf]--> guide_blind_woman (weight=1.0)\n",
      "dog --[CapableOf]--> hear_high_pitches (weight=1.0)\n",
      "dog --[CapableOf]--> hear_sub_sonic_sounds (weight=1.0)\n",
      "dog --[CapableOf]--> hear_whistle (weight=1.0)\n",
      "dog --[CapableOf]--> hear_whistling (weight=3.464)\n",
      "dog --[CapableOf]--> hear_wolf (weight=1.0)\n",
      "dog --[CapableOf]--> help_master (weight=1.0)\n",
      "dog --[CapableOf]--> hunt_cat (weight=1.0)\n",
      "dog --[CapableOf]--> jump_over_log (weight=1.0)\n",
      "dog --[CapableOf]--> lead_police_to_dead_body (weight=1.0)\n",
      "dog --[CapableOf]--> learn_how_to_beg (weight=1.0)\n",
      "dog --[CapableOf]--> learn_to_do_tricks (weight=2.828)\n",
      "dog --[CapableOf]--> learn_to_fetch_things (weight=1.0)\n",
      "dog --[CapableOf]--> lick_privates (weight=1.0)\n",
      "dog --[CapableOf]--> lie_down (weight=1.0)\n",
      "dog --[CapableOf]--> lie_down_after_eating (weight=1.0)\n",
      "dog --[CapableOf]--> like_going_for_walk (weight=1.0)\n",
      "dog --[CapableOf]--> listen_to_master (weight=1.0)\n",
      "dog --[CapableOf]--> live_in_dog_house (weight=1.0)\n",
      "dog --[CapableOf]--> live_in_house (weight=1.0)\n",
      "dog --[CapableOf]--> long_for_dead_owner (weight=2.0)\n",
      "dog --[CapableOf]--> love_master (weight=1.0)\n",
      "dog --[CapableOf]--> mark_territory (weight=3.464)\n",
      "dog --[CapableOf]--> mark_territory_with_urine (weight=1.0)\n",
      "dog --[CapableOf]--> mark_tree (weight=2.0)\n",
      "dog --[CapableOf]--> master_new_trick (weight=1.0)\n",
      "dog --[CapableOf]--> mother_it's_young (weight=1.0)\n",
      "dog --[CapableOf]--> mother_puppy (weight=2.828)\n",
      "dog --[CapableOf]--> not_eat_bone_of_contention (weight=1.0)\n",
      "dog --[CapableOf]--> open_door (weight=1.0)\n",
      "dog --[CapableOf]--> pet (weight=7.483)\n",
      "dog --[IsA]--> pet (weight=6.0)\n",
      "dog --[RelatedTo]--> pet (weight=9.83)\n",
      "dog --[UsedFor]--> pet (weight=2.828)\n",
      "dog --[CapableOf]--> piss_on_couch (weight=1.0)\n",
      "dog --[CapableOf]--> place_bone_in_hole (weight=1.0)\n",
      "dog --[CapableOf]--> play_fetch (weight=1.0)\n",
      "dog --[CapableOf]--> play_frisbee (weight=1.0)\n",
      "dog --[CausesDesire]--> play_frisbee (weight=2.828)\n",
      "dog --[CapableOf]--> play_with_bone (weight=1.0)\n",
      "dog --[CapableOf]--> please_master (weight=1.0)\n",
      "dog --[CapableOf]--> pull_sleigh (weight=1.0)\n",
      "dog --[CapableOf]--> put_paw_on_knee (weight=1.0)\n",
      "dog --[CapableOf]--> remember_basic_commands (weight=1.0)\n",
      "dog --[CapableOf]--> roll_over (weight=1.0)\n",
      "dog --[CapableOf]--> run (weight=6.0)\n",
      "dog --[CapableOf]--> run_after_mailman (weight=1.0)\n",
      "dog --[CapableOf]--> run_away_from_master (weight=1.0)\n",
      "dog --[CapableOf]--> run_fasr (weight=1.0)\n",
      "dog --[CapableOf]--> see_in_black_and_white (weight=1.0)\n",
      "dog --[CapableOf]--> sense_danger (weight=4.472)\n",
      "dog --[CapableOf]--> sense_fear (weight=1.0)\n",
      "dog --[CapableOf]--> shake_hands (weight=2.0)\n",
      "dog --[CapableOf]--> shake_slipper (weight=1.0)\n",
      "dog --[CapableOf]--> sleep_long_time (weight=1.0)\n",
      "dog --[CapableOf]--> smell_cooking_meat (weight=1.0)\n",
      "dog --[CapableOf]--> smell_crotch (weight=1.0)\n",
      "dog --[CapableOf]--> smell_drugs (weight=3.464)\n",
      "dog --[CapableOf]--> smell_fear (weight=2.0)\n",
      "dog --[CapableOf]--> smell_food (weight=1.0)\n",
      "dog --[CapableOf]--> smell_scents (weight=1.0)\n",
      "dog --[CapableOf]--> smell_trace_of_another_dog (weight=1.0)\n",
      "dog --[CapableOf]--> spot_movement (weight=1.0)\n",
      "dog --[CapableOf]--> stand_on_hindlegs (weight=1.0)\n",
      "dog --[CapableOf]--> stand_on_two_legs (weight=1.0)\n",
      "dog --[CapableOf]--> stay_in_house (weight=1.0)\n",
      "dog --[CapableOf]--> stay_in_place (weight=2.0)\n",
      "dog --[CapableOf]--> stray (weight=2.0)\n",
      "dog --[CapableOf]--> taste_dog_food (weight=1.0)\n",
      "dog --[CapableOf]--> tear_rug (weight=1.0)\n",
      "dog --[CapableOf]--> tear_shirt (weight=1.0)\n",
      "dog --[CapableOf]--> think_about_food (weight=1.0)\n",
      "dog --[CapableOf]--> trip_over_rope (weight=2.0)\n",
      "dog --[CapableOf]--> try_to_please_master (weight=1.0)\n",
      "dog --[CapableOf]--> turn_over (weight=1.0)\n",
      "dog --[CapableOf]--> understand_simple_commands (weight=1.0)\n",
      "dog --[CapableOf]--> urinate (weight=1.0)\n",
      "dog --[CapableOf]--> wag_tail (weight=1.0)\n",
      "dog --[CapableOf]--> walk_man (weight=1.0)\n",
      "dog --[CapableOf]--> walk_on_leash (weight=2.828)\n",
      "dog --[CapableOf]--> wear_collar (weight=2.0)\n",
      "dog --[CapableOf]--> wear_sweater (weight=1.0)\n",
      "dog --[CapableOf]--> win_blue_ribbon (weight=2.0)\n",
      "dog --[CausesDesire]--> go_for_walk (weight=1.0)\n",
      "dog --[Desires]--> go_for_walk (weight=2.0)\n",
      "dog --[CausesDesire]--> jog (weight=1.0)\n",
      "dog --[CausesDesire]--> take_walk (weight=1.0)\n",
      "dog --[Desires]--> affection (weight=2.828)\n",
      "dog --[Desires]--> attention (weight=1.0)\n",
      "dog --[Desires]--> bone (weight=5.292)\n",
      "dog --[Desires]--> dig (weight=1.0)\n",
      "dog --[Desires]--> eat (weight=1.0)\n",
      "dog --[Desires]--> eat_lot (weight=1.0)\n",
      "dog --[Desires]--> food (weight=1.0)\n",
      "dog --[Desires]--> go_outside (weight=1.0)\n",
      "dog --[Desires]--> lots_of_attention (weight=3.464)\n",
      "dog --[Desires]--> loved_by_master (weight=2.0)\n",
      "dog --[Desires]--> meat (weight=1.0)\n",
      "dog --[Desires]--> petted (weight=4.899)\n",
      "dog --[Desires]--> play (weight=1.0)\n",
      "dog --[Desires]--> walked (weight=1.0)\n",
      "dog --[DistinctFrom]--> cut (weight=0.15)\n",
      "dog --[EtymologicallyDerivedFrom]--> dogge (weight=1.0)\n",
      "dog --[EtymologicallyRelatedTo]--> hound (weight=0.25)\n",
      "dog --[RelatedTo]--> hound (weight=1.052)\n",
      "dog --[EtymologicallyRelatedTo]--> mastiff (weight=0.25)\n",
      "dog --[EtymologicallyRelatedTo]--> see_man_about_dog (weight=0.25)\n",
      "dog --[HasA]--> big_heart (weight=1.0)\n",
      "dog --[HasA]--> brains (weight=1.0)\n",
      "dog --[HasA]--> fleas (weight=4.0)\n",
      "dog --[NotDesires]--> fleas (weight=4.0)\n",
      "dog --[RelatedTo]--> fleas (weight=0.714)\n",
      "dog --[HasA]--> four_legs (weight=5.292)\n",
      "dog --[RelatedTo]--> four_legs (weight=2.224)\n",
      "dog --[HasA]--> fur (weight=1.0)\n",
      "dog --[RelatedTo]--> fur (weight=1.198)\n",
      "dog --[HasA]--> nose (weight=2.0)\n",
      "dog --[RelatedTo]--> nose (weight=0.24)\n",
      "dog --[HasA]--> one_mouth (weight=1.0)\n",
      "dog --[HasA]--> paws (weight=2.828)\n",
      "dog --[RelatedTo]--> paws (weight=1.106)\n",
      "dog --[HasA]--> penis (weight=1.0)\n",
      "dog --[HasA]--> teeth (weight=2.828)\n",
      "dog --[RelatedTo]--> teeth (weight=0.589)\n",
      "dog --[HasA]--> two_ears (weight=2.828)\n",
      "dog --[HasProperty]--> alive (weight=1.0)\n",
      "dog --[HasProperty]--> black (weight=2.0)\n",
      "dog --[HasProperty]--> brown (weight=1.0)\n",
      "dog --[HasProperty]--> fun (weight=2.828)\n",
      "dog --[RelatedTo]--> fun (weight=0.11)\n",
      "dog --[HasProperty]--> gray (weight=1.0)\n",
      "dog --[HasProperty]--> larger_than_cat (weight=1.0)\n",
      "dog --[HasProperty]--> mean (weight=1.0)\n",
      "dog --[HasProperty]--> one_among_many_animals (weight=1.0)\n",
      "dog --[HasProperty]--> playing_dead (weight=1.0)\n",
      "dog --[HasProperty]--> running_in_dream (weight=1.0)\n",
      "dog --[HasProperty]--> very_common_pet (weight=1.0)\n",
      "dog --[HasProperty]--> worth_much_more_than_dime (weight=1.0)\n",
      "dog --[HasSubevent]--> pee_against_trees (weight=1.0)\n",
      "dog --[IsA]--> canine (weight=4.899)\n",
      "dog --[RelatedTo]--> canine (weight=7.626)\n",
      "dog --[IsA]--> curious_observer_of_mankind (weight=2.0)\n",
      "dog --[IsA]--> cuter_than_kids (weight=1.0)\n",
      "dog --[IsA]--> domestic_animal (weight=1.0)\n",
      "dog --[RelatedTo]--> domestic_animal (weight=1.504)\n",
      "dog --[IsA]--> domesticated_canine (weight=1.0)\n",
      "dog --[IsA]--> example_of_pet (weight=2.0)\n",
      "dog --[IsA]--> faithful_companion (weight=2.0)\n",
      "dog --[IsA]--> four_legged_animal (weight=2.828)\n",
      "dog --[IsA]--> good_friend (weight=3.464)\n",
      "dog --[IsA]--> growling_at_man (weight=1.0)\n",
      "dog --[IsA]--> happy_when_wags_tail (weight=1.0)\n",
      "dog --[IsA]--> loyal_friend (weight=6.633)\n",
      "dog --[IsA]--> mammal (weight=5.292)\n",
      "dog --[RelatedTo]--> mammal (weight=2.038)\n",
      "dog --[IsA]--> n (weight=0.5)\n",
      "dog --[IsA]--> mans_best_friend (weight=2.0)\n",
      "dog --[IsA]--> nice_friend (weight=2.0)\n",
      "dog --[IsA]--> quadriped (weight=1.0)\n",
      "dog --[IsA]--> shedding (weight=1.0)\n",
      "dog --[IsA]--> so_big (weight=1.0)\n",
      "dog --[IsA]--> straining_at_leash (weight=1.0)\n",
      "dog --[IsA]--> thing (weight=1.0)\n",
      "dog --[IsA]--> usually_about_3_feet_tall (weight=1.0)\n",
      "dog --[IsA]--> usually_about_two_feet_tall (weight=1.0)\n",
      "dog --[IsA]--> very_faithful_friend (weight=1.0)\n",
      "dog --[MadeOf]--> flesh_and_bones (weight=1.0)\n",
      "dog --[MadeOf]--> hair (weight=1.0)\n",
      "dog --[RelatedTo]--> hair (weight=0.331)\n",
      "dog --[NotDesires]--> abandoned (weight=2.0)\n",
      "dog --[NotDesires]--> bath (weight=1.0)\n",
      "dog --[NotDesires]--> eat_dinner (weight=1.0)\n",
      "dog --[NotDesires]--> go_to_vet (weight=1.0)\n",
      "dog --[NotDesires]--> hear_loud_noises (weight=2.0)\n",
      "dog --[NotDesires]--> left_alone (weight=1.0)\n",
      "dog --[NotDesires]--> left_home_alone (weight=1.0)\n",
      "dog --[NotDesires]--> punishment (weight=1.0)\n",
      "dog --[NotDesires]--> shooed_off_couch (weight=1.0)\n",
      "dog --[NotDesires]--> starve (weight=1.0)\n",
      "dog --[PartOf]--> animal_kingdom (weight=1.0)\n",
      "dog --[ReceivesAction]--> bought_at_pet_store (weight=1.0)\n",
      "dog --[ReceivesAction]--> fed (weight=2.0)\n",
      "dog --[ReceivesAction]--> kept_in_kennel (weight=1.0)\n",
      "dog --[ReceivesAction]--> kicked (weight=3.464)\n",
      "dog --[ReceivesAction]--> loved (weight=2.0)\n",
      "dog --[RelatedTo]--> heredity (weight=0.5)\n",
      "dog --[RelatedTo]--> animal (weight=9.41)\n",
      "dog --[RelatedTo]--> animal_barks (weight=0.33)\n",
      "dog --[RelatedTo]--> animal_canine (weight=0.44)\n",
      "dog --[RelatedTo]--> animal_companion (weight=0.114)\n",
      "dog --[RelatedTo]--> animal_furry (weight=0.352)\n",
      "dog --[RelatedTo]--> animal_pet (weight=0.853)\n",
      "dog --[RelatedTo]--> animal_tail (weight=0.106)\n",
      "dog --[RelatedTo]--> backwards (weight=1.401)\n",
      "dog --[RelatedTo]--> bark_animal (weight=0.108)\n",
      "dog --[RelatedTo]--> bark_bark (weight=0.497)\n",
      "dog --[RelatedTo]--> bark_bite (weight=0.177)\n",
      "dog --[RelatedTo]--> bark_woof (weight=0.224)\n",
      "dog --[RelatedTo]--> barking (weight=3.392)\n",
      "dog --[RelatedTo]--> barking_animal (weight=1.704)\n",
      "dog --[RelatedTo]--> barking_fetching (weight=0.106)\n",
      "dog --[RelatedTo]--> barking_pet (weight=0.648)\n",
      "dog --[RelatedTo]--> barks (weight=1.656)\n",
      "dog --[RelatedTo]--> beagle (weight=0.922)\n",
      "dog --[RelatedTo]--> beagle_hound (weight=0.576)\n",
      "dog --[RelatedTo]--> best (weight=2.512)\n",
      "dog --[RelatedTo]--> best_friend (weight=2.043)\n",
      "dog --[RelatedTo]--> best_pet (weight=0.102)\n",
      "dog --[RelatedTo]--> big (weight=0.782)\n",
      "dog --[RelatedTo]--> big_puppy (weight=0.782)\n",
      "dog --[RelatedTo]--> bow (weight=1.075)\n",
      "dog --[RelatedTo]--> bow_bow (weight=0.129)\n",
      "dog --[RelatedTo]--> bow_wow (weight=0.946)\n",
      "dog --[RelatedTo]--> canine_animal (weight=2.228)\n",
      "dog --[RelatedTo]--> canine_friend (weight=0.764)\n",
      "dog --[RelatedTo]--> canine_pet (weight=2.027)\n",
      "dog --[RelatedTo]--> canine_pooch (weight=0.18)\n",
      "dog --[RelatedTo]--> canines (weight=0.158)\n",
      "dog --[RelatedTo]--> cat_and (weight=0.219)\n",
      "dog --[RelatedTo]--> cat_chaser (weight=0.279)\n",
      "dog --[RelatedTo]--> cat_enemy (weight=0.323)\n",
      "dog --[RelatedTo]--> cat_hater (weight=0.27)\n",
      "dog --[RelatedTo]--> cats (weight=0.953)\n",
      "dog --[RelatedTo]--> cats_and (weight=0.173)\n",
      "dog --[RelatedTo]--> cats_enemy (weight=0.237)\n",
      "dog --[RelatedTo]--> chaser (weight=0.279)\n",
      "dog --[RelatedTo]--> collar (weight=0.866)\n",
      "dog --[RelatedTo]--> common (weight=0.606)\n",
      "dog --[RelatedTo]--> common_pet (weight=0.606)\n",
      "dog --[RelatedTo]--> companion (weight=0.114)\n",
      "dog --[RelatedTo]--> companionship (weight=0.166)\n",
      "dog --[UsedFor]--> companionship (weight=6.325)\n",
      "dog --[RelatedTo]--> conformation_show (weight=0.5)\n",
      "dog --[RelatedTo]--> coyote (weight=0.225)\n",
      "dog --[RelatedTo]--> creature (weight=0.18)\n",
      "dog --[RelatedTo]--> critter (weight=0.108)\n",
      "dog --[RelatedTo]--> cur (weight=0.162)\n",
      "dog --[RelatedTo]--> cute (weight=0.195)\n",
      "dog --[RelatedTo]--> cynomorphic (weight=1.0)\n",
      "dog --[RelatedTo]--> cynomorphism (weight=1.0)\n",
      "dog --[RelatedTo]--> digital (weight=1.0)\n",
      "dog --[RelatedTo]--> dog_food (weight=0.5)\n",
      "dog --[RelatedTo]--> domestic (weight=2.5)\n",
      "dog --[RelatedTo]--> domestic_pet (weight=0.893)\n",
      "dog --[RelatedTo]--> domestic_wolf (weight=0.166)\n",
      "dog --[RelatedTo]--> domesticated (weight=0.803)\n",
      "dog --[RelatedTo]--> domesticated_animal (weight=0.529)\n",
      "dog --[RelatedTo]--> domesticated_wolf (weight=0.104)\n",
      "dog --[RelatedTo]--> ears (weight=0.478)\n",
      "dog --[RelatedTo]--> enemy (weight=0.56)\n",
      "dog --[RelatedTo]--> et (weight=0.109)\n",
      "dog --[RelatedTo]--> faithful (weight=0.362)\n",
      "dog --[RelatedTo]--> faithful_animal (weight=0.362)\n",
      "dog --[RelatedTo]--> family (weight=1.241)\n",
      "dog --[RelatedTo]--> family_pet (weight=1.137)\n",
      "dog --[RelatedTo]--> feline (weight=0.11)\n",
      "dog --[RelatedTo]--> feline_animal (weight=0.11)\n",
      "dog --[RelatedTo]--> fetching (weight=0.106)\n",
      "dog --[RelatedTo]--> flea (weight=0.438)\n",
      "dog --[RelatedTo]--> flea_animal (weight=0.106)\n",
      "dog --[RelatedTo]--> flea_bag (weight=1.0)\n",
      "dog --[RelatedTo]--> fluffy (weight=0.16)\n",
      "dog --[RelatedTo]--> four (weight=2.852)\n",
      "dog --[RelatedTo]--> four_legged (weight=0.349)\n",
      "dog --[RelatedTo]--> four_paws (weight=0.448)\n",
      "dog --[RelatedTo]--> fox (weight=0.382)\n",
      "dog --[RelatedTo]--> friend (weight=3.642)\n",
      "dog --[RelatedTo]--> friendly (weight=0.432)\n",
      "dog --[RelatedTo]--> friendly_mammal (weight=0.108)\n",
      "dog --[RelatedTo]--> furry (weight=1.811)\n",
      "dog --[RelatedTo]--> furry_animal (weight=0.382)\n",
      "dog --[RelatedTo]--> furry_creature (weight=0.18)\n",
      "dog --[RelatedTo]--> furry_pet (weight=0.173)\n",
      "dog --[RelatedTo]--> fuzzy (weight=0.242)\n",
      "dog --[RelatedTo]--> general (weight=0.11)\n",
      "dog --[RelatedTo]--> general_canine (weight=0.11)\n",
      "dog --[RelatedTo]--> god (weight=1.841)\n",
      "dog --[RelatedTo]--> god_backwards (weight=1.197)\n",
      "dog --[RelatedTo]--> good (weight=0.212)\n",
      "dog --[RelatedTo]--> good_pet (weight=0.212)\n",
      "dog --[RelatedTo]--> graphic (weight=1.0)\n",
      "dog --[RelatedTo]--> greyhound (weight=0.163)\n",
      "dog --[RelatedTo]--> grown (weight=0.392)\n",
      "dog --[RelatedTo]--> grown_puppy (weight=0.392)\n",
      "dog --[RelatedTo]--> guard (weight=0.19)\n",
      "dog --[RelatedTo]--> hairy (weight=0.228)\n",
      "dog --[RelatedTo]--> hater (weight=0.27)\n",
      "dog --[RelatedTo]--> home (weight=0.526)\n",
      "dog --[RelatedTo]--> home_animal (weight=0.284)\n",
      "dog --[RelatedTo]--> homes (weight=0.19)\n",
      "dog --[RelatedTo]--> horse (weight=0.333)\n",
      "dog --[RelatedTo]--> hound_animal (weight=0.106)\n",
      "dog --[RelatedTo]--> house_animal (weight=0.229)\n",
      "dog --[RelatedTo]--> house_pet (weight=1.734)\n",
      "dog --[RelatedTo]--> household (weight=1.99)\n",
      "dog --[RelatedTo]--> household_animal (weight=0.408)\n",
      "dog --[RelatedTo]--> household_et (weight=0.109)\n",
      "dog --[RelatedTo]--> household_pet (weight=1.473)\n",
      "dog --[RelatedTo]--> houses (weight=0.189)\n",
      "dog --[RelatedTo]--> humans (weight=0.19)\n",
      "dog --[RelatedTo]--> hunting (weight=0.159)\n",
      "dog --[RelatedTo]--> it_barks (weight=0.226)\n",
      "dog --[RelatedTo]--> k (weight=0.253)\n",
      "dog --[RelatedTo]--> k_nine (weight=0.253)\n",
      "dog --[RelatedTo]--> labrador (weight=0.104)\n",
      "dog --[RelatedTo]--> large (weight=0.411)\n",
      "dog --[RelatedTo]--> large_cat (weight=0.309)\n",
      "dog --[RelatedTo]--> large_puppy (weight=0.102)\n",
      "dog --[RelatedTo]--> lassie (weight=0.473)\n",
      "dog --[RelatedTo]--> leach (weight=0.141)\n",
      "dog --[RelatedTo]--> legged (weight=0.349)\n",
      "dog --[RelatedTo]--> legs (weight=2.668)\n",
      "dog --[RelatedTo]--> legs_tail (weight=0.138)\n",
      "dog --[RelatedTo]--> little (weight=0.135)\n",
      "dog --[RelatedTo]--> little_horse (weight=0.135)\n",
      "dog --[RelatedTo]--> loving (weight=0.135)\n",
      "dog --[RelatedTo]--> loyal (weight=0.946)\n",
      "dog --[RelatedTo]--> loyal_animal (weight=0.252)\n",
      "dog --[RelatedTo]--> loyal_pet (weight=0.173)\n",
      "dog --[RelatedTo]--> lupine (weight=0.324)\n",
      "dog --[RelatedTo]--> man (weight=0.474)\n",
      "dog --[RelatedTo]--> man_friend (weight=0.474)\n",
      "dog --[RelatedTo]--> mans (weight=1.522)\n",
      "dog --[RelatedTo]--> mans_best (weight=0.433)\n",
      "dog --[RelatedTo]--> mans_friend (weight=0.814)\n",
      "dog --[RelatedTo]--> max (weight=0.141)\n",
      "dog --[RelatedTo]--> mutt (weight=0.3)\n",
      "dog --[RelatedTo]--> muzzle (weight=0.267)\n",
      "dog --[RelatedTo]--> nine (weight=0.253)\n",
      "dog --[RelatedTo]--> old (weight=0.141)\n",
      "dog --[RelatedTo]--> old_puppy (weight=0.141)\n",
      "dog --[RelatedTo]--> on (weight=1.0)\n",
      "dog --[RelatedTo]--> parent (weight=0.187)\n",
      "dog --[RelatedTo]--> parks (weight=0.158)\n",
      "dog --[RelatedTo]--> pe (weight=0.448)\n",
      "dog --[RelatedTo]--> people (weight=0.385)\n",
      "dog --[RelatedTo]--> per (weight=0.265)\n",
      "dog --[RelatedTo]--> pet_animal (weight=2.234)\n",
      "dog --[RelatedTo]--> pet_barking (weight=0.228)\n",
      "dog --[RelatedTo]--> pet_canine (weight=0.284)\n",
      "dog --[RelatedTo]--> pet_puppy (weight=0.102)\n",
      "dog --[RelatedTo]--> pet_sorry (weight=0.183)\n",
      "dog --[RelatedTo]--> pet_woof (weight=0.17)\n",
      "dog --[RelatedTo]--> pets (weight=0.233)\n",
      "dog --[RelatedTo]--> pluto (weight=0.18)\n",
      "dog --[RelatedTo]--> pooch (weight=0.33)\n",
      "dog --[RelatedTo]--> poodle (weight=0.289)\n",
      "dog --[RelatedTo]--> pup (weight=0.132)\n",
      "dog --[RelatedTo]--> puppies (weight=0.382)\n",
      "dog --[RelatedTo]--> puppy (weight=3.41)\n",
      "dog --[RelatedTo]--> puppy_parent (weight=0.187)\n",
      "dog --[RelatedTo]--> raining (weight=0.102)\n",
      "dog --[RelatedTo]--> raining_cat (weight=0.102)\n",
      "dog --[RelatedTo]--> retriever (weight=0.103)\n",
      "dog --[RelatedTo]--> reverse (weight=0.211)\n",
      "dog --[RelatedTo]--> reverse_god (weight=0.211)\n",
      "dog --[RelatedTo]--> rover (weight=0.32)\n",
      "dog --[RelatedTo]--> schnauzer (weight=0.187)\n",
      "dog --[RelatedTo]--> screen (weight=1.0)\n",
      "dog --[RelatedTo]--> small (weight=0.434)\n",
      "dog --[RelatedTo]--> small_animal (weight=0.104)\n",
      "dog --[RelatedTo]--> small_fox (weight=0.132)\n",
      "dog --[RelatedTo]--> small_horse (weight=0.198)\n",
      "dog --[RelatedTo]--> snoopy (weight=0.179)\n",
      "dog --[RelatedTo]--> sorry (weight=0.183)\n",
      "dog --[RelatedTo]--> stupid (weight=0.187)\n",
      "dog --[RelatedTo]--> tail (weight=3.891)\n",
      "dog --[RelatedTo]--> tail_barks (weight=0.147)\n",
      "dog --[RelatedTo]--> tail_ears (weight=0.146)\n",
      "dog --[RelatedTo]--> tailed (weight=0.106)\n",
      "dog --[RelatedTo]--> tailed_animal (weight=0.106)\n",
      "dog --[RelatedTo]--> teeth_tail (weight=0.206)\n",
      "dog --[RelatedTo]--> terrier (weight=0.108)\n",
      "dog --[RelatedTo]--> tics (weight=0.225)\n",
      "dog --[RelatedTo]--> typical (weight=0.187)\n",
      "dog --[RelatedTo]--> typical_pet (weight=0.187)\n",
      "dog --[RelatedTo]--> wagging (weight=0.755)\n",
      "dog --[RelatedTo]--> wagging_tail (weight=0.755)\n",
      "dog --[RelatedTo]--> wet (weight=0.24)\n",
      "dog --[RelatedTo]--> wet_nose (weight=0.24)\n",
      "dog --[RelatedTo]--> wolf (weight=3.879)\n",
      "dog --[RelatedTo]--> wolf_family (weight=0.104)\n",
      "dog --[RelatedTo]--> wolf_wolf (weight=0.419)\n",
      "dog --[RelatedTo]--> wolves (weight=0.366)\n",
      "dog --[RelatedTo]--> woof_animal (weight=0.233)\n",
      "dog --[RelatedTo]--> woof_bark (weight=0.15)\n",
      "dog --[RelatedTo]--> woof_woof (weight=2.447)\n",
      "dog --[RelatedTo]--> woofer (weight=0.147)\n",
      "dog --[RelatedTo]--> wow (weight=0.946)\n",
      "dog --[RelatedTo]--> ðŸ• (weight=1.0)\n",
      "dog --[Synonym]--> dog (weight=0.5)\n",
      "dog --[UsedFor]--> bird_hunting (weight=1.0)\n",
      "dog --[UsedFor]--> biting_postman (weight=1.0)\n",
      "dog --[UsedFor]--> breeding (weight=1.0)\n",
      "dog --[UsedFor]--> chasing_cats (weight=1.0)\n",
      "dog --[UsedFor]--> comforting_elderly (weight=1.0)\n",
      "dog --[UsedFor]--> companionship_and_protection (weight=1.0)\n",
      "dog --[UsedFor]--> company (weight=1.0)\n",
      "dog --[UsedFor]--> dog_experiments (weight=1.0)\n",
      "dog --[UsedFor]--> entertaining_people (weight=1.0)\n",
      "dog --[UsedFor]--> fetching_sticks (weight=1.0)\n",
      "dog --[UsedFor]--> give_comfort (weight=1.0)\n",
      "dog --[UsedFor]--> guarding_home (weight=1.0)\n",
      "dog --[UsedFor]--> guarding_junkyard (weight=1.0)\n",
      "dog --[UsedFor]--> guarding_piece_of_property (weight=1.0)\n",
      "dog --[UsedFor]--> guarding_property (weight=2.828)\n",
      "dog --[UsedFor]--> helping_blind_person_to_navigate (weight=1.0)\n",
      "dog --[UsedFor]--> helping_to_control_livestock (weight=1.0)\n",
      "dog --[UsedFor]--> herding_sheep (weight=1.0)\n",
      "dog --[UsedFor]--> keep_company (weight=2.0)\n",
      "dog --[UsedFor]--> keep_watrm (weight=1.0)\n",
      "dog --[UsedFor]--> keeping_company (weight=2.0)\n",
      "dog --[UsedFor]--> love (weight=1.0)\n",
      "dog --[UsedFor]--> playing (weight=1.0)\n",
      "dog --[UsedFor]--> protect_belongings (weight=1.0)\n",
      "dog --[UsedFor]--> protecting_livestock (weight=1.0)\n",
      "dog --[UsedFor]--> providing_friendship (weight=1.0)\n",
      "dog --[UsedFor]--> safty (weight=1.0)\n",
      "dog --[UsedFor]--> scare_away_bad_guy (weight=1.0)\n",
      "dog --[UsedFor]--> sniffing_out_drugs (weight=1.0)\n",
      "dog --[UsedFor]--> sniffing_out_explosives (weight=1.0)\n",
      "dog --[UsedFor]--> tracking_animal (weight=1.0)\n",
      "dog --[UsedFor]--> tracking_criminal (weight=1.0)\n",
      "dog --[UsedFor]--> watch_house (weight=1.0)\n",
      "dog --[genus]--> canis (weight=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Example: Query neighbors of a concept\n",
    "concept = 'dog'\n",
    "neighbors = list(G.neighbors(concept))\n",
    "print(f\"Neighbors of '{concept}':\", neighbors)\n",
    "\n",
    "# Example: Get all edges and their attributes for a concept\n",
    "edges = G.out_edges(concept, data=True)\n",
    "for u, v, attr in edges:\n",
    "    print(f\"{u} --[{attr['relation']}]--> {v} (weight={attr['weight']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e84780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph has 501 nodes and 2052 edges.\n"
     ]
    }
   ],
   "source": [
    "# Example: Extract a subgraph of a concept and its immediate neighbors\n",
    "sub_nodes = [concept] + neighbors\n",
    "subgraph = G.subgraph(sub_nodes)\n",
    "print(f\"Subgraph has {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb0028",
   "metadata": {},
   "source": [
    "## 4. Save and Load the Graph in GraphML Format\n",
    "GraphML is a standard, portable format for graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2edabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to ..\\Data\\Output\\conceptnet_graph.graphml\n",
      "Loaded graph has 754380 nodes and 1655522 edges.\n"
     ]
    }
   ],
   "source": [
    "# Save the graph\n",
    "output_path = os.path.join('..', 'Data', 'Output', 'conceptnet_graph.graphml')\n",
    "nx.write_graphml(G, output_path)\n",
    "print(f\"Graph saved to {output_path}\")\n",
    "\n",
    "# Load the graph back (demonstration)\n",
    "G_loaded = nx.read_graphml(output_path)\n",
    "print(f\"Loaded graph has {G_loaded.number_of_nodes()} nodes and {G_loaded.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a823a",
   "metadata": {},
   "source": [
    "## 5. Next Steps and Advanced Queries\n",
    "- Expand to larger datasets by chunking or streaming edge additions.\n",
    "- Integrate with graph databases like Neo4j for even larger-scale applications.\n",
    "- Perform advanced queries (e.g., shortest paths, community detection, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f4741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d510fe69",
   "metadata": {},
   "source": [
    "## 6. Semantic Enrichment: Adding Custom Logic\n",
    "Now that the graph is built, you can add your own semantic logic. For example, you might:\n",
    "- Add new semantic relationships based on custom rules or external data.\n",
    "- Reweight or filter edges based on your own criteria.\n",
    "- Annotate nodes or edges with additional semantic metadata.\n",
    "- Implement reasoning or inference over the graph structure.\n",
    "\n",
    "Below is an example of how to add a new semantic property to edges based on a custom rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b88d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n --[Antonym]--> dog | is_animal_relation: True\n",
      "n --[FormOf]--> dog | is_animal_relation: True\n",
      "n --[HasContext]--> dog | is_animal_relation: True\n",
      "n --[IsA]--> dog | is_animal_relation: True\n",
      "n --[RelatedTo]--> dog | is_animal_relation: True\n"
     ]
    }
   ],
   "source": [
    "# Example: Add a semantic property to edges based on a custom rule\n",
    "# Here, we flag edges as 'is_animal_relation' if either node is an animal (simple demo)\n",
    "animal_concepts = {'dog', 'cat', 'horse', 'cow', 'sheep'}  # Replace with your own logic or list\n",
    "\n",
    "for u, v, k, data in G.edges(keys=True, data=True):\n",
    "    if u in animal_concepts or v in animal_concepts:\n",
    "        data['is_animal_relation'] = True\n",
    "    else:\n",
    "        data['is_animal_relation'] = False\n",
    "\n",
    "# Show a few example edges with the new property\n",
    "count = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if 'is_animal_relation' in data and data['is_animal_relation']:\n",
    "        print(f\"{u} --[{data['relation']}]--> {v} | is_animal_relation: {data['is_animal_relation']}\")\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0b298",
   "metadata": {},
   "source": [
    "You can now build on this pattern to add more advanced semantic logic, such as inference, clustering, or integration with external knowledge sources. Just add new cells below to continue your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80853cd6",
   "metadata": {},
   "source": [
    "# 1. NetworkX Hyper-Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0116e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_hyper_train(data_path, iterations=3, save_prefix=\"nx_hyper_trained\"):\n",
    "    \"\"\"\n",
    "    Hyper-train a knowledge graph using NetworkX - much faster than matrix-based approaches\n",
    "    while preserving the iterative quality improvements.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "    import os\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(f\"ðŸš€ Starting NetworkX Hyper-Training ({iterations} iterations)\")\n",
    "    \n",
    "    # Load the data once\n",
    "    full_df = pd.read_parquet(data_path)\n",
    "    print(f\"ðŸ“Š Loaded {len(full_df):,} triples\")\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = []\n",
    "    \n",
    "    # Initial params\n",
    "    current_quality_threshold = 0.5  # Start higher with NetworkX (faster)\n",
    "    current_concept_limit = 10000\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(os.path.join('..', 'Data', 'Output'), exist_ok=True)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ“Œ ITERATION {iteration+1}/{iterations}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 1. Filter concepts by popularity (memory efficiency)\n",
    "        concept_counts = pd.concat([\n",
    "            full_df['start_concept'].value_counts(),\n",
    "            full_df['end_concept'].value_counts()\n",
    "        ]).groupby(level=0).sum().sort_values(ascending=False)\n",
    "        \n",
    "        top_concepts = set(concept_counts.head(current_concept_limit).index)\n",
    "        filtered_df = full_df[\n",
    "            full_df['start_concept'].isin(top_concepts) & \n",
    "            full_df['end_concept'].isin(top_concepts)\n",
    "        ]\n",
    "        \n",
    "        # 2. Apply quality threshold (weight-based)\n",
    "        quality_df = filtered_df[filtered_df['edge_weight'] >= current_quality_threshold]\n",
    "        \n",
    "        print(f\"ðŸ“Š Using {len(top_concepts):,} concepts\")\n",
    "        print(f\"ðŸ“Š Quality threshold: {current_quality_threshold:.2f}\")\n",
    "        print(f\"ðŸ“Š Filtered to {len(quality_df):,} high-quality triples\")\n",
    "        \n",
    "        # 3. Build graph (much faster with NetworkX)\n",
    "        print(f\"ðŸ”„ Building graph...\")\n",
    "        G = nx.MultiDiGraph()\n",
    "        \n",
    "        # Add edges with progress bar\n",
    "        for _, row in tqdm(quality_df.iterrows(), total=len(quality_df), desc=\"Adding edges\"):\n",
    "            G.add_edge(\n",
    "                row['start_concept'],\n",
    "                row['end_concept'],\n",
    "                relation=row['relation_type'],\n",
    "                weight=row['edge_weight']\n",
    "            )\n",
    "        \n",
    "        # 4. Apply semantic enrichment for this iteration\n",
    "        print(f\"âœ¨ Adding semantic enrichment (iteration-specific)...\")\n",
    "        \n",
    "        # Simple example: mark high-confidence edges\n",
    "        high_confidence_threshold = 0.7 + (iteration * 0.05)  # Increases each iteration\n",
    "        edge_count = 0\n",
    "        \n",
    "        for u, v, k, data in G.edges(keys=True, data=True):\n",
    "            # Add iteration-specific attributes\n",
    "            data['iteration_added'] = iteration + 1\n",
    "            data['high_confidence'] = data['weight'] >= high_confidence_threshold\n",
    "            edge_count += 1\n",
    "            \n",
    "            # Add custom logic (just examples - modify for your needs)\n",
    "            if iteration == 1:\n",
    "                # Second iteration: add transitivity scores\n",
    "                data['transitive_potential'] = len(list(G.neighbors(v))) / 100\n",
    "            elif iteration == 2:\n",
    "                # Third iteration: add centrality-based importance\n",
    "                try:\n",
    "                    data['centrality_score'] = G.degree(u) * G.degree(v) / 1000\n",
    "                except:\n",
    "                    data['centrality_score'] = 0\n",
    "        \n",
    "        # 5. Save this iteration\n",
    "        output_path = os.path.join('..', 'Data', 'Output', f\"{save_prefix}_iter{iteration+1}.graphml\")\n",
    "        nx.write_graphml(G, output_path)\n",
    "        \n",
    "        # 6. Calculate and store metrics\n",
    "        iter_time = time.time() - start_time\n",
    "        metrics.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'quality_threshold': current_quality_threshold,\n",
    "            'concept_limit': current_concept_limit,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'processing_time': iter_time,\n",
    "            'edges_per_second': G.number_of_edges() / max(1, iter_time)\n",
    "        })\n",
    "        \n",
    "        print(f\"ðŸ“Š Graph has {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\")\n",
    "        print(f\"â±ï¸  Processing time: {iter_time:.1f} seconds\")\n",
    "        print(f\"ðŸ’¾ Saved to: {output_path}\")\n",
    "        \n",
    "        # 7. Update parameters for next iteration\n",
    "        current_quality_threshold = min(0.9, current_quality_threshold + 0.1)\n",
    "        current_concept_limit = min(50000, current_concept_limit + 10000)  # Increase concept count\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸŽ‰ HYPER-TRAINING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for m in metrics:\n",
    "        print(f\"Iteration {m['iteration']}: {m['nodes']:,} nodes, \"\n",
    "              f\"{m['edges']:,} edges, {m['processing_time']:.1f}s, \"\n",
    "              f\"threshold={m['quality_threshold']:.2f}\")\n",
    "    \n",
    "    final_path = os.path.join('..', 'Data', 'Output', f\"{save_prefix}_final.graphml\")\n",
    "    nx.write_graphml(G, final_path)\n",
    "    print(f\"ðŸ’¾ Final model saved to: {final_path}\")\n",
    "    \n",
    "    return G, metrics, final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa84bda",
   "metadata": {},
   "source": [
    "# 2. Call The Function To Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the NetworkX hyper-training function\n",
    "# G_trained, training_metrics, final_model_path = networkx_hyper_train(\n",
    "#     data_path=conceptnet_path,  # Path to your conceptnet data\n",
    "#     iterations=3,               # Number of training iterations\n",
    "#     save_prefix=\"nx_semantic\"   # Prefix for saved files\n",
    "# )\n",
    "\n",
    "# # The result is already a NetworkX graph ready for querying!\n",
    "# print(f\"\\nðŸ” Testing final model...\")\n",
    "# concept = 'dog'\n",
    "# for u, v, data in G_trained.out_edges(concept, data=True):\n",
    "#     if data.get('high_confidence', False):\n",
    "#         print(f\"{u} --[{data['relation']}]--> {v} (weight={data['weight']}, added in iteration {data['iteration_added']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769328f1",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Hyper-Training for Scaling\n",
    "To achieve our goal of eventually including the full ConceptNet dataset, we need a more memory-efficient approach that:\n",
    "1. Processes data in chunks rather than loading everything at once\n",
    "2. Uses more efficient storage formats\n",
    "3. Implements checkpointing for recovery\n",
    "4. Has configurable parameters for gradual scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68069e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ï¸ Fix: Chunked Parquet Reading with pyarrow\n",
    "# The scalable training function is now updated to use a custom `parquet_chunk_reader` utility based on `pyarrow` for chunked reading of Parquet files. This avoids the `TypeError` from pandas' `read_parquet()` and enables true memory-efficient processing of large datasets.\n",
    "\n",
    "# Utility: Chunked Parquet Reader using pyarrow\n",
    "def parquet_chunk_reader(parquet_path, chunk_size=100000, columns=None):\n",
    "    \"\"\"\n",
    "    Efficiently read a Parquet file in chunks using pyarrow, yielding pandas DataFrames.\n",
    "    \"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pandas as pd\n",
    "\n",
    "    parquet_file = pq.ParquetFile(parquet_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "    rows_read = 0\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "    # Read in row groups, but yield DataFrames in chunks of chunk_size\n",
    "    batch = []\n",
    "    batch_rows = 0\n",
    "    for rg in range(num_row_groups):\n",
    "        table = parquet_file.read_row_group(rg, columns=columns)\n",
    "        df = table.to_pandas()\n",
    "        batch.append(df)\n",
    "        batch_rows += len(df)\n",
    "        rows_read += len(df)\n",
    "        # If enough rows for a chunk, yield\n",
    "        while batch_rows >= chunk_size:\n",
    "            concat_df = pd.concat(batch)\n",
    "            yield concat_df.iloc[:chunk_size]\n",
    "            # Prepare for next batch\n",
    "            if batch_rows > chunk_size:\n",
    "                # Keep the remainder for next chunk\n",
    "                batch = [concat_df.iloc[chunk_size:]]\n",
    "                batch_rows = len(batch[0])\n",
    "            else:\n",
    "                batch = []\n",
    "                batch_rows = 0\n",
    "    # Yield any remaining rows\n",
    "    if batch_rows > 0:\n",
    "        yield pd.concat(batch)\n",
    "\n",
    "# Refactor: networkx_scalable_hyper_train now uses parquet_chunk_reader for chunked reading\n",
    "def networkx_scalable_hyper_train(\n",
    "    data_path,\n",
    "    iterations=3,\n",
    "    save_prefix=\"nx_scalable\",\n",
    "    config=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-efficient scalable hyper-training for knowledge graphs.\n",
    "    Now saves a graph snapshot after every chunk for smooth animation.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "    import os\n",
    "    import time\n",
    "    import pickle\n",
    "    import json\n",
    "    import gc\n",
    "    from datetime import datetime\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        # Data filtering\n",
    "        'initial_quality_threshold': 0.3,\n",
    "        'quality_threshold_step': 0.1,\n",
    "        'max_quality_threshold': 0.9,\n",
    "        # Concept limits\n",
    "        'initial_concept_limit': 20000,\n",
    "        'concept_limit_step': 20000,\n",
    "        'max_concept_limit': 100000,\n",
    "        # Memory optimization\n",
    "        'chunk_size': 100000,\n",
    "        'gc_frequency': 2,\n",
    "        # Storage options\n",
    "        'save_format': 'graphml',          # Save as 'graphml' (best for web), can change to 'pickle'\n",
    "        'compression': False,              # Not used for graphml\n",
    "        # Checkpointing\n",
    "        'enable_checkpoints': True,\n",
    "        'resume_from_checkpoint': True,\n",
    "        # Enrichment options\n",
    "        'add_high_confidence_flag': True,\n",
    "        'add_transitivity': True,\n",
    "        'add_centrality': True,\n",
    "        # Animation snapshots\n",
    "        'save_snapshots_per_chunk': True,   # << NEW: Save a snapshot after each chunk\n",
    "        # Performance profiling\n",
    "        'profile': False\n",
    "    }\n",
    "\n",
    "    # Update with user config\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    for key, value in config.items():\n",
    "        if key in default_config:\n",
    "            default_config[key] = value\n",
    "    config = default_config\n",
    "\n",
    "    print(f\"ðŸš€ Starting Scalable Hyper-Training ({iterations} iterations)\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = os.path.join('..', 'Data', 'Output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    metrics = []\n",
    "\n",
    "    # Current parameters\n",
    "    current_quality_threshold = config['initial_quality_threshold']\n",
    "    current_concept_limit = config['initial_concept_limit']\n",
    "\n",
    "    # Try to resume from checkpoint if enabled\n",
    "    checkpoint_path = os.path.join(output_dir, f\"{save_prefix}_checkpoint.pkl\")\n",
    "    if config['enable_checkpoints'] and config['resume_from_checkpoint'] and os.path.exists(checkpoint_path):\n",
    "        print(f\"ðŸ“‚ Resuming from checkpoint: {checkpoint_path}\")\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                checkpoint = pickle.load(f)\n",
    "            metrics = checkpoint['metrics']\n",
    "            current_quality_threshold = checkpoint['next_quality_threshold']\n",
    "            current_concept_limit = checkpoint['next_concept_limit']\n",
    "            last_iteration = checkpoint['last_completed_iteration']\n",
    "            print(f\"âœ… Restored from checkpoint (completed {last_iteration+1}/{iterations} iterations)\")\n",
    "            start_iteration = last_iteration + 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load checkpoint: {e}\")\n",
    "            start_iteration = 0\n",
    "    else:\n",
    "        start_iteration = 0\n",
    "\n",
    "    # Process only needed iterations\n",
    "    for iteration in range(start_iteration, iterations):\n",
    "        iteration_start_time = time.time()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ“Œ ITERATION {iteration+1}/{iterations}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        print(f\"ðŸ“Š Quality threshold: {current_quality_threshold:.2f}\")\n",
    "        print(f\"ðŸ“Š Concept limit: {current_concept_limit:,}\")\n",
    "\n",
    "        # Step 1: Initial scan - count concepts to identify top ones\n",
    "        print(f\"ðŸ” Scanning data to identify top concepts...\")\n",
    "        concept_counter = None\n",
    "        for i, chunk in enumerate(parquet_chunk_reader(data_path, chunk_size=config['chunk_size'])):\n",
    "            chunk_counts = pd.concat([\n",
    "                chunk['start_concept'].value_counts(),\n",
    "                chunk['end_concept'].value_counts()\n",
    "            ]).groupby(level=0).sum()\n",
    "            if concept_counter is None:\n",
    "                concept_counter = chunk_counts\n",
    "            else:\n",
    "                concept_counter = concept_counter.add(chunk_counts, fill_value=0)\n",
    "            if i % config['gc_frequency'] == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        # Get top concepts\n",
    "        top_concepts = set(concept_counter.nlargest(current_concept_limit).index)\n",
    "        print(f\"âœ… Identified {len(top_concepts):,} most frequent concepts\")\n",
    "\n",
    "        # Step 2: Build graph in chunks\n",
    "        print(f\"ðŸ”„ Building graph with quality threshold {current_quality_threshold:.2f}...\")\n",
    "\n",
    "        G = nx.MultiDiGraph()\n",
    "        total_edges_processed = 0\n",
    "        total_edges_added = 0\n",
    "\n",
    "        chunk_reader = parquet_chunk_reader(data_path, chunk_size=config['chunk_size'])\n",
    "        for i, chunk in enumerate(tqdm(chunk_reader, desc=\"Building graph\")):\n",
    "            filtered_chunk = chunk[\n",
    "                chunk['start_concept'].isin(top_concepts) &\n",
    "                chunk['end_concept'].isin(top_concepts)\n",
    "            ]\n",
    "            quality_chunk = filtered_chunk[filtered_chunk['edge_weight'] >= current_quality_threshold]\n",
    "\n",
    "            for _, row in tqdm(quality_chunk.iterrows(), total=len(quality_chunk), desc=f\"Adding edges (chunk {i+1})\", leave=False):\n",
    "                G.add_edge(\n",
    "                    row['start_concept'],\n",
    "                    row['end_concept'],\n",
    "                    relation=row['relation_type'],\n",
    "                    weight=row['edge_weight'],\n",
    "                    iteration_added=iteration + 1\n",
    "                )\n",
    "            total_edges_processed += len(filtered_chunk)\n",
    "            total_edges_added += len(quality_chunk)\n",
    "            if i % config['gc_frequency'] == 0:\n",
    "                gc.collect()\n",
    "\n",
    "            # **NEW: Save snapshot after every chunk**\n",
    "            if config.get('save_snapshots_per_chunk', False):\n",
    "                chunk_snap_path = os.path.join(\n",
    "                    output_dir, f\"{save_prefix}_iter{iteration+1}_chunk{i+1}.graphml\")\n",
    "                nx.write_graphml(G, chunk_snap_path)\n",
    "                print(f\"ðŸ’¾ Saved chunk snapshot: {chunk_snap_path}\")\n",
    "\n",
    "        print(f\"âœ… Processed {total_edges_processed:,} edges, added {total_edges_added:,} to graph\")\n",
    "        print(f\"ðŸ“Š Graph has {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\")\n",
    "        \n",
    "        # Step 3: Apply semantic enrichment\n",
    "        if config['add_high_confidence_flag'] or config['add_transitivity'] or config['add_centrality']:\n",
    "            print(f\"âœ¨ Adding semantic enrichment...\")\n",
    "            \n",
    "            enrichment_start = time.time()\n",
    "            \n",
    "            # High confidence threshold increases with each iteration\n",
    "            high_confidence_threshold = 0.7 + (iteration * 0.05)\n",
    "            \n",
    "            if config['add_high_confidence_flag']:\n",
    "                # Mark high-confidence edges\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True), \n",
    "                                         desc=\"Adding high confidence flags\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    data['high_confidence'] = data['weight'] >= high_confidence_threshold\n",
    "            \n",
    "            # Add iteration-specific enrichment\n",
    "            if iteration == 1 and config['add_transitivity']:\n",
    "                # Second iteration: add transitivity scores for a subset of nodes\n",
    "                # This is expensive, so we limit to higher-degree nodes\n",
    "                print(\"ðŸ”„ Adding transitivity scores...\")\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True),\n",
    "                                         desc=\"Adding transitivity scores\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    try:\n",
    "                        # More efficient calculation - only count if needed\n",
    "                        if G.out_degree(v) > 0:  # Only if target has outgoing connections\n",
    "                            data['transitive_potential'] = min(G.out_degree(v) / 100, 1.0)\n",
    "                    except:\n",
    "                        data['transitive_potential'] = 0\n",
    "            \n",
    "            elif iteration == 2 and config['add_centrality']:\n",
    "                # Third iteration: add simplified centrality scores\n",
    "                # Full betweenness centrality is too expensive, so we use degree as proxy\n",
    "                print(\"ðŸ”„ Adding centrality scores...\")\n",
    "                for u, v, k, data in tqdm(G.edges(keys=True, data=True),\n",
    "                                         desc=\"Adding centrality scores\",\n",
    "                                         total=G.number_of_edges()):\n",
    "                    try:\n",
    "                        data['centrality_score'] = min(\n",
    "                            (G.degree(u) + G.degree(v)) / 1000, \n",
    "                            1.0\n",
    "                        )\n",
    "                    except:\n",
    "                        data['centrality_score'] = 0\n",
    "            \n",
    "            print(f\"âœ… Enrichment completed in {time.time() - enrichment_start:.1f}s\")\n",
    "        \n",
    "        # Step 4: Save this iteration\n",
    "        save_start_time = time.time()\n",
    "        \n",
    "        if config['save_format'] == 'graphml':\n",
    "            # Save as GraphML (standard but larger files)\n",
    "            output_path = os.path.join(output_dir, f\"{save_prefix}_iter{iteration+1}.graphml\")\n",
    "            nx.write_graphml(G, output_path)\n",
    "        else:\n",
    "            # Save as pickle (faster, smaller)\n",
    "            output_path = os.path.join(output_dir, f\"{save_prefix}_iter{iteration+1}.pkl\")\n",
    "            with open(output_path, 'wb') as f:\n",
    "                pickle.dump(G, f, protocol=4)  # Protocol 4 for better performance\n",
    "        \n",
    "        print(f\"ðŸ’¾ Saved to: {output_path} in {time.time() - save_start_time:.1f}s\")\n",
    "        \n",
    "        # Step 5: Calculate and store metrics\n",
    "        iter_time = time.time() - iteration_start_time\n",
    "        iter_metrics = {\n",
    "            'iteration': iteration + 1,\n",
    "            'quality_threshold': current_quality_threshold,\n",
    "            'concept_limit': current_concept_limit,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'processing_time': iter_time,\n",
    "            'edges_per_second': total_edges_processed / max(1, iter_time),\n",
    "            'added_edges_ratio': total_edges_added / max(1, total_edges_processed),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        metrics.append(iter_metrics)\n",
    "        \n",
    "        print(f\"ðŸ“Š Iteration stats:\")\n",
    "        print(f\"   Nodes: {iter_metrics['nodes']:,}\")\n",
    "        print(f\"   Edges: {iter_metrics['edges']:,}\")\n",
    "        print(f\"   Processing time: {iter_metrics['processing_time']:.1f}s\")\n",
    "        print(f\"   Edges/second: {iter_metrics['edges_per_second']:.1f}\")\n",
    "        \n",
    "        # Step 6: Save checkpoint if enabled\n",
    "        if config['enable_checkpoints']:\n",
    "            # Calculate next thresholds\n",
    "            next_quality_threshold = min(\n",
    "                config['max_quality_threshold'],\n",
    "                current_quality_threshold + config['quality_threshold_step']\n",
    "            )\n",
    "            next_concept_limit = min(\n",
    "                config['max_concept_limit'],\n",
    "                current_concept_limit + config['concept_limit_step']\n",
    "            )\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'metrics': metrics,\n",
    "                'last_completed_iteration': iteration,\n",
    "                'next_quality_threshold': next_quality_threshold,\n",
    "                'next_concept_limit': next_concept_limit,\n",
    "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "            print(f\"âœ… Checkpoint saved to: {checkpoint_path}\")\n",
    "        \n",
    "        # Step 7: Update parameters for next iteration\n",
    "        current_quality_threshold = min(\n",
    "            config['max_quality_threshold'],\n",
    "            current_quality_threshold + config['quality_threshold_step']\n",
    "        )\n",
    "        current_concept_limit = min(\n",
    "            config['max_concept_limit'],\n",
    "            current_concept_limit + config['concept_limit_step']\n",
    "        )\n",
    "        \n",
    "        # Explicitly run garbage collection between iterations\n",
    "        gc.collect()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸŽ‰ HYPER-TRAINING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for m in metrics:\n",
    "        print(f\"Iteration {m['iteration']}: {m['nodes']:,} nodes, \"\n",
    "              f\"{m['edges']:,} edges, {m['processing_time']:.1f}s, \"\n",
    "              f\"threshold={m['quality_threshold']:.2f}\")\n",
    "    \n",
    "    # Save final version (use the same format as iterations for consistency)\n",
    "    if config['save_format'] == 'graphml':\n",
    "        final_path = os.path.join(output_dir, f\"{save_prefix}_final.graphml\")\n",
    "        nx.write_graphml(G, final_path)\n",
    "    else:\n",
    "        final_path = os.path.join(output_dir, f\"{save_prefix}_final.pkl\")\n",
    "        with open(final_path, 'wb') as f:\n",
    "            pickle.dump(G, f, protocol=4)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Final model saved to: {final_path}\")\n",
    "    \n",
    "    # Save metrics as JSON for analysis\n",
    "    metrics_path = os.path.join(output_dir, f\"{save_prefix}_metrics.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ“Š Training metrics saved to: {metrics_path}\")\n",
    "    \n",
    "    return G, metrics, final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8f409",
   "metadata": {},
   "source": [
    "# 3. Run the Scalable Hyper-Training\n",
    "\n",
    "This version can handle much larger datasets by using chunked processing, efficient storage, and checkpointing. It's designed to be interruptible and resumable, making it practical for training on the full ConceptNet dataset in stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecdfa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Scalable Hyper-Training (4 iterations)\n",
      "\n",
      "============================================================\n",
      "ðŸ“Œ ITERATION 1/4\n",
      "============================================================\n",
      "ðŸ“Š Quality threshold: 0.30\n",
      "ðŸ“Š Concept limit: 15,000\n",
      "ðŸ” Scanning data to identify top concepts...\n",
      "âœ… Identified 15,000 most frequent concepts\n",
      "ðŸ”„ Building graph with quality threshold 0.30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 1it [00:02,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk1.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 2it [00:02,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk2.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 3it [00:03,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk3.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 4it [00:05,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk4.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 5it [00:05,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk5.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 6it [00:06,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk6.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 7it [00:08,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk7.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 8it [00:09,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk8.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 9it [00:12,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk9.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 10it [00:15,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk10.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 11it [00:19,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk11.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 12it [00:23,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk12.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 13it [00:26,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk13.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 14it [00:32,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk14.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 15it [00:35,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved chunk snapshot: ..\\Data\\Output\\nx_scaled_kg_iter1_chunk15.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph: 15it [00:39,  2.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Erich Curtis\\Desktop\\All Python\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:165\u001b[39m, in \u001b[36mwrite_graphml_lxml\u001b[39m\u001b[34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01metree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxmletree\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lxml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m      2\u001b[39m scalable_config = {\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Data filtering - start with lower threshold for broader coverage\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minitial_quality_threshold\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresume_from_checkpoint\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     27\u001b[39m }\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Uncomment and run this cell to start the scalable training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m G_scalable, scalable_metrics, scalable_path = \u001b[43mnetworkx_scalable_hyper_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconceptnet_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# More iterations for gradual refinement\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnx_scaled_kg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Different prefix to avoid overwriting\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalable_config\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Test the final model\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ” Testing scalable model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mnetworkx_scalable_hyper_train\u001b[39m\u001b[34m(data_path, iterations, save_prefix, config)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.get(\u001b[33m'\u001b[39m\u001b[33msave_snapshots_per_chunk\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    191\u001b[39m         chunk_snap_path = os.path.join(\n\u001b[32m    192\u001b[39m             output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_iter\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_chunk\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.graphml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_graphml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_snap_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ’¾ Saved chunk snapshot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_snap_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_edges_processed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m edges, added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_edges_added\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to graph\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 11:5\u001b[39m, in \u001b[36margmap_write_graphml_lxml_7\u001b[39m\u001b[34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Erich Curtis\\Desktop\\All Python\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:167\u001b[39m, in \u001b[36mwrite_graphml_lxml\u001b[39m\u001b[34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001b[39m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01metree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxmletree\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite_graphml_xml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprettyprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_numeric_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnamed_key_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_id_from_attribute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m writer = GraphMLWriterLxml(\n\u001b[32m    178\u001b[39m     path,\n\u001b[32m    179\u001b[39m     graph=G,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m     edge_id_from_attribute=edge_id_from_attribute,\n\u001b[32m    185\u001b[39m )\n\u001b[32m    186\u001b[39m writer.dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 16:5\u001b[39m, in \u001b[36margmap_write_graphml_xml_12\u001b[39m\u001b[34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Erich Curtis\\Desktop\\All Python\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:114\u001b[39m, in \u001b[36mwrite_graphml_xml\u001b[39m\u001b[34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001b[39m\n\u001b[32m    106\u001b[39m writer = GraphMLWriter(\n\u001b[32m    107\u001b[39m     encoding=encoding,\n\u001b[32m    108\u001b[39m     prettyprint=prettyprint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     edge_id_from_attribute=edge_id_from_attribute,\n\u001b[32m    112\u001b[39m )\n\u001b[32m    113\u001b[39m writer.add_graph_element(G)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Erich Curtis\\Desktop\\All Python\\Semantica-Full-Reasoning-Chatbot\\.venv\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:668\u001b[39m, in \u001b[36mGraphMLWriter.dump\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28mself\u001b[39m.indent(\u001b[38;5;28mself\u001b[39m.xml)\n\u001b[32m    667\u001b[39m document = ElementTree(\u001b[38;5;28mself\u001b[39m.xml)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m \u001b[43mdocument\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_declaration\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\xml\\etree\\ElementTree.py:729\u001b[39m, in \u001b[36mElementTree.write\u001b[39m\u001b[34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method, short_empty_elements)\u001b[39m\n\u001b[32m    727\u001b[39m qnames, namespaces = _namespaces(\u001b[38;5;28mself\u001b[39m._root, default_namespace)\n\u001b[32m    728\u001b[39m serialize = _serialize[method]\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m          \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\xml\\etree\\ElementTree.py:892\u001b[39m, in \u001b[36m_serialize_xml\u001b[39m\u001b[34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[39m\n\u001b[32m    890\u001b[39m         write(_escape_cdata(text))\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m         \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m     write(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m + tag + \u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\xml\\etree\\ElementTree.py:892\u001b[39m, in \u001b[36m_serialize_xml\u001b[39m\u001b[34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[39m\n\u001b[32m    890\u001b[39m         write(_escape_cdata(text))\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m         \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m     write(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m + tag + \u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\xml\\etree\\ElementTree.py:892\u001b[39m, in \u001b[36m_serialize_xml\u001b[39m\u001b[34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[39m\n\u001b[32m    890\u001b[39m         write(_escape_cdata(text))\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m         \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m     write(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m + tag + \u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\xml\\etree\\ElementTree.py:898\u001b[39m, in \u001b[36m_serialize_xml\u001b[39m\u001b[34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[39m\n\u001b[32m    896\u001b[39m             write(\u001b[33m\"\u001b[39m\u001b[33m />\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m elem.tail:\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_escape_cdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configure the hyper-training parameters\n",
    "scalable_config = {\n",
    "    # Data filtering - start with lower threshold for broader coverage\n",
    "    'initial_quality_threshold': 0.3,\n",
    "    'quality_threshold_step': 0.1,\n",
    "    'max_quality_threshold': 0.6,   # Lower max threshold to keep more relationships\n",
    "\n",
    "    # Concept limits - gradually scale up\n",
    "    'initial_concept_limit': 15000,\n",
    "    'concept_limit_step': 15000, \n",
    "    'max_concept_limit': 100000,\n",
    "\n",
    "    # Memory optimization\n",
    "    'chunk_size': 100000,\n",
    "    'gc_frequency': 3,\n",
    "\n",
    "    # Storage options\n",
    "    'save_format': 'graphml',               # <--- CHANGE THIS!\n",
    "    'compression': False,                   # Not used for graphml, but can leave as is\n",
    "\n",
    "    # Animation!\n",
    "    'save_snapshots_per_chunk': True,       # <--- ADD THIS!\n",
    "    \n",
    "    # Checkpointing\n",
    "    'enable_checkpoints': True,\n",
    "    'resume_from_checkpoint': False,\n",
    "}\n",
    "# Uncomment and run this cell to start the scalable training\n",
    "G_scalable, scalable_metrics, scalable_path = networkx_scalable_hyper_train(\n",
    "    data_path=conceptnet_path,\n",
    "    iterations=4,                # More iterations for gradual refinement\n",
    "    save_prefix=\"nx_scaled_kg\",  # Different prefix to avoid overwriting\n",
    "    config=scalable_config\n",
    ")\n",
    "\n",
    "# Test the final model\n",
    "print(f\"\\nðŸ” Testing scalable model...\")\n",
    "test_concept = 'computer'  # Try a different concept\n",
    "for u, v, data in G_scalable.out_edges(test_concept, data=True):\n",
    "    if data.get('high_confidence', False):\n",
    "        print(f\"{u} --[{data['relation']}]--> {v} (weight={data['weight']}, added in iteration {data['iteration_added']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f145238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Downloading notebook-7.4.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from jupyter) (6.29.5)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Downloading jupyterlab-4.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from ipykernel->jupyter) (6.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (310)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab->jupyter)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyterlab->jupyter)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting setuptools>=41.1.0 (from jupyterlab->jupyter)\n",
      "  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading prometheus_client-0.22.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached pywinpty-2.0.15-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests>=2.31 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting certifi (from httpx>=0.25.0->jupyterlab->jupyter)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->jupyter)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyterlab->jupyter)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached rpds_py-0.25.1-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pyyaml>=5.3 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
      "  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\erich curtis\\desktop\\all python\\semantica-full-reasoning-chatbot\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading jupyterlab-4.4.2-py3-none-any.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.1/12.3 MB 84.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 46.1 MB/s eta 0:00:00\n",
      "Downloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading prometheus_client-0.22.0-py3-none-any.whl (62 kB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pywinpty-2.0.15-cp313-cp313-win_amd64.whl (1.4 MB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rpds_py-0.25.1-cp313-cp313-win_amd64.whl (234 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Downloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 19.7 MB/s eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Using cached cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading notebook-7.4.2-py3-none-any.whl (14.3 MB)\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  14.2/14.3 MB 90.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.3/14.3 MB 57.4 MB/s eta 0:00:00\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, fastjsonschema, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, typing-extensions, types-python-dateutil, tinycss2, soupsieve, sniffio, setuptools, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pyyaml, pywinpty, python-json-logger, pycparser, prometheus-client, pandocfilters, overrides, mistune, MarkupSafe, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, idna, h11, fqdn, defusedxml, charset-normalizer, certifi, bleach, babel, attrs, async-lru, terminado, requests, referencing, jinja2, httpcore, cffi, beautifulsoup4, arrow, anyio, jupyter-server-terminals, jsonschema-specifications, isoduration, ipywidgets, httpx, argon2-cffi-bindings, jupyter-console, jsonschema, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\n",
      "    ---------------------------------------  1/69 [fastjsonschema]\n",
      "   - --------------------------------------  3/69 [websocket-client]\n",
      "   -- -------------------------------------  4/69 [webcolors]\n",
      "   -- -------------------------------------  5/69 [urllib3]\n",
      "   -- -------------------------------------  5/69 [urllib3]\n",
      "   --- ------------------------------------  6/69 [uri-template]\n",
      "   ----- ----------------------------------  9/69 [tinycss2]\n",
      "   ----- ---------------------------------- 10/69 [soupsieve]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------ --------------------------------- 12/69 [setuptools]\n",
      "   ------- -------------------------------- 13/69 [send2trash]\n",
      "   --------- ------------------------------ 16/69 [rfc3339-validator]\n",
      "   --------- ------------------------------ 17/69 [pyyaml]\n",
      "   ----------- ---------------------------- 19/69 [python-json-logger]\n",
      "   ----------- ---------------------------- 20/69 [pycparser]\n",
      "   ------------ --------------------------- 21/69 [prometheus-client]\n",
      "   ------------ --------------------------- 21/69 [prometheus-client]\n",
      "   ------------- -------------------------- 24/69 [mistune]\n",
      "   ------------- -------------------------- 24/69 [mistune]\n",
      "   ------------- -------------------------- 24/69 [mistune]\n",
      "   --------------- ------------------------ 27/69 [jupyterlab-pygments]\n",
      "   ---------------- ----------------------- 29/69 [json5]\n",
      "   ----------------- ---------------------- 30/69 [idna]\n",
      "   ------------------ --------------------- 32/69 [fqdn]\n",
      "   ------------------- -------------------- 33/69 [defusedxml]\n",
      "   ------------------- -------------------- 34/69 [charset-normalizer]\n",
      "   -------------------- ------------------- 36/69 [bleach]\n",
      "   -------------------- ------------------- 36/69 [bleach]\n",
      "   -------------------- ------------------- 36/69 [bleach]\n",
      "   --------------------- ------------------ 37/69 [babel]\n",
      "   --------------------- ------------------ 37/69 [babel]\n",
      "   --------------------- ------------------ 37/69 [babel]\n",
      "   --------------------- ------------------ 37/69 [babel]\n",
      "   --------------------- ------------------ 37/69 [babel]\n",
      "   ---------------------- ----------------- 38/69 [attrs]\n",
      "   ----------------------- ---------------- 40/69 [terminado]\n",
      "   ----------------------- ---------------- 41/69 [requests]\n",
      "   ------------------------ --------------- 42/69 [referencing]\n",
      "   ------------------------ --------------- 43/69 [jinja2]\n",
      "   ------------------------ --------------- 43/69 [jinja2]\n",
      "   ------------------------- -------------- 44/69 [httpcore]\n",
      "   -------------------------- ------------- 45/69 [cffi]\n",
      "   -------------------------- ------------- 45/69 [cffi]\n",
      "   -------------------------- ------------- 46/69 [beautifulsoup4]\n",
      "   -------------------------- ------------- 46/69 [beautifulsoup4]\n",
      "   --------------------------- ------------ 47/69 [arrow]\n",
      "   --------------------------- ------------ 48/69 [anyio]\n",
      "   --------------------------- ------------ 48/69 [anyio]\n",
      "   ---------------------------- ----------- 49/69 [jupyter-server-terminals]\n",
      "   ----------------------------- ---------- 51/69 [isoduration]\n",
      "   ------------------------------ --------- 52/69 [ipywidgets]\n",
      "   ------------------------------ --------- 52/69 [ipywidgets]\n",
      "   ------------------------------ --------- 52/69 [ipywidgets]\n",
      "   ------------------------------ --------- 53/69 [httpx]\n",
      "   ------------------------------- -------- 54/69 [argon2-cffi-bindings]\n",
      "   ------------------------------- -------- 55/69 [jupyter-console]\n",
      "   -------------------------------- ------- 56/69 [jsonschema]\n",
      "   -------------------------------- ------- 56/69 [jsonschema]\n",
      "   --------------------------------- ------ 58/69 [nbformat]\n",
      "   --------------------------------- ------ 58/69 [nbformat]\n",
      "   --------------------------------- ------ 58/69 [nbformat]\n",
      "   ---------------------------------- ----- 59/69 [nbclient]\n",
      "   ----------------------------------- ---- 61/69 [nbconvert]\n",
      "   ----------------------------------- ---- 61/69 [nbconvert]\n",
      "   ----------------------------------- ---- 61/69 [nbconvert]\n",
      "   ----------------------------------- ---- 61/69 [nbconvert]\n",
      "   ----------------------------------- ---- 62/69 [jupyter-server]\n",
      "   ----------------------------------- ---- 62/69 [jupyter-server]\n",
      "   ----------------------------------- ---- 62/69 [jupyter-server]\n",
      "   ----------------------------------- ---- 62/69 [jupyter-server]\n",
      "   ----------------------------------- ---- 62/69 [jupyter-server]\n",
      "   ------------------------------------- -- 64/69 [jupyterlab-server]\n",
      "   ------------------------------------- -- 64/69 [jupyterlab-server]\n",
      "   ------------------------------------- -- 65/69 [jupyter-lsp]\n",
      "   ------------------------------------- -- 65/69 [jupyter-lsp]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 66/69 [jupyterlab]\n",
      "   -------------------------------------- - 67/69 [notebook]\n",
      "   -------------------------------------- - 67/69 [notebook]\n",
      "   -------------------------------------- - 67/69 [notebook]\n",
      "   ---------------------------------------- 69/69 [jupyter]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 anyio-4.9.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 certifi-2025.4.26 cffi-1.17.1 charset-normalizer-3.4.2 defusedxml-0.7.1 fastjsonschema-2.21.1 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ipywidgets-8.1.7 isoduration-20.11.0 jinja2-3.1.6 json5-0.12.0 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.2 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.4.2 notebook-shim-0.2.4 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.22.0 pycparser-2.22 python-json-logger-3.3.0 pywinpty-2.0.15 pyyaml-6.0.2 referencing-0.36.2 requests-2.32.3 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.25.1 send2trash-1.8.3 setuptools-80.8.0 sniffio-1.3.1 soupsieve-2.7 terminado-0.18.1 tinycss2-1.4.0 types-python-dateutil-2.9.0.20250516 typing-extensions-4.13.2 uri-template-1.3.0 urllib3-2.4.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# upgrade jupyter and ipywidgets\n",
    "%pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cd7df",
   "metadata": {},
   "source": [
    "## 8. Advanced Knowledge Graph Exploration Tools (Scalable Version)\n",
    "\n",
    "The following tools are designed for advanced semantic exploration and analysis of large, semantically-enriched knowledge graphs produced by the scalable hyper-training workflow. You can use these tools to query, analyze, and visualize the graph, extract semantic paths, and perform advanced reasoning tasks.\n",
    "\n",
    "**How to use:**\n",
    "- Load your trained scalable graph (typically saved as a pickle file for efficiency).\n",
    "- Initialize the `KnowledgeGraphExplorer` with your graph.\n",
    "- Use the provided methods to explore concepts, find paths, analyze statistics, and more.\n",
    "- For very large graphs, ensure your environment has sufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28689684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphExplorer:\n",
    "    \"\"\"\n",
    "    Tools for exploring and analyzing a NetworkX-based knowledge graph.\n",
    "    Designed to work with ConceptNet-style semantic graphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, graph, relation_types=None):\n",
    "        \"\"\"Initialize the explorer with a graph\"\"\"\n",
    "        self.graph = graph\n",
    "        \n",
    "        # Filter relation types if specified, otherwise use all\n",
    "        if relation_types is None:\n",
    "            self.relation_types = self._get_all_relation_types()\n",
    "        else:\n",
    "            self.relation_types = relation_types\n",
    "            \n",
    "        # Cache for performance\n",
    "        self._node_centrality = None\n",
    "        \n",
    "    def _get_all_relation_types(self):\n",
    "        \"\"\"Get all unique relation types in the graph\"\"\"\n",
    "        relation_types = set()\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            if 'relation' in data:\n",
    "                relation_types.add(data['relation'])\n",
    "        return list(relation_types)\n",
    "    \n",
    "    def explore_concept(self, concept, relation_filter=None, min_weight=0.0, limit=10):\n",
    "        \"\"\"\n",
    "        Get the most important relationships for a concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        concept : str\n",
    "            The concept to explore\n",
    "        relation_filter : list or None\n",
    "            Filter by relation types (e.g., ['IsA', 'PartOf'])\n",
    "        min_weight : float\n",
    "            Minimum weight threshold\n",
    "        limit : int\n",
    "            Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with source, target, relation, weight\n",
    "        \"\"\"\n",
    "        if concept not in self.graph:\n",
    "            return f\"Concept '{concept}' not found in the graph.\"\n",
    "        \n",
    "        # Collect outgoing and incoming edges\n",
    "        edges = []\n",
    "        \n",
    "        # Outgoing edges\n",
    "        for _, target, data in self.graph.out_edges(concept, data=True):\n",
    "            if (relation_filter is None or data.get('relation') in relation_filter) and data.get('weight', 0) >= min_weight:\n",
    "                edges.append({\n",
    "                    'source': concept,\n",
    "                    'target': target,\n",
    "                    'relation': data.get('relation', 'unknown'),\n",
    "                    'weight': data.get('weight', 0.0),\n",
    "                    'direction': 'outgoing',\n",
    "                    'high_confidence': data.get('high_confidence', False)\n",
    "                })\n",
    "        \n",
    "        # Incoming edges\n",
    "        for source, _, data in self.graph.in_edges(concept, data=True):\n",
    "            if (relation_filter is None or data.get('relation') in relation_filter) and data.get('weight', 0) >= min_weight:\n",
    "                edges.append({\n",
    "                    'source': source,\n",
    "                    'target': concept,\n",
    "                    'relation': data.get('relation', 'unknown'),\n",
    "                    'weight': data.get('weight', 0.0),\n",
    "                    'direction': 'incoming',\n",
    "                    'high_confidence': data.get('high_confidence', False)\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame and sort by weight\n",
    "        import pandas as pd\n",
    "        edges_df = pd.DataFrame(edges)\n",
    "        \n",
    "        if len(edges_df) > 0:\n",
    "            edges_df = edges_df.sort_values('weight', ascending=False).head(limit)\n",
    "        \n",
    "        return edges_df\n",
    "    \n",
    "    def find_path(self, source, target, relation_filter=None, max_length=3):\n",
    "        \"\"\"\n",
    "        Find the shortest path between two concepts, considering only certain relations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source : str\n",
    "            Source concept\n",
    "        target : str\n",
    "            Target concept\n",
    "        relation_filter : list or None\n",
    "            Filter by relation types\n",
    "        max_length : int\n",
    "            Maximum path length\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (concept, relation, concept) tuples, or None if no path\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Check if concepts exist\n",
    "        if source not in self.graph or target not in self.graph:\n",
    "            return f\"One or both concepts not found: {source}, {target}\"\n",
    "        \n",
    "        # Create a view of the graph with only the desired relations\n",
    "        if relation_filter is not None:\n",
    "            view = nx.MultiDiGraph()\n",
    "            \n",
    "            for u, v, key, data in self.graph.edges(keys=True, data=True):\n",
    "                if data.get('relation') in relation_filter:\n",
    "                    view.add_edge(u, v, key=key, **data)\n",
    "                    \n",
    "            search_graph = view\n",
    "        else:\n",
    "            search_graph = self.graph\n",
    "        \n",
    "        # Find the shortest path\n",
    "        try:\n",
    "            path = nx.shortest_path(search_graph, source, target, weight=lambda u, v, data: 1/max(data.get('weight', 0.1), 0.1))\n",
    "            \n",
    "            # Construct the full path with relations\n",
    "            full_path = []\n",
    "            for i in range(len(path)-1):\n",
    "                edges = search_graph.get_edge_data(path[i], path[i+1])\n",
    "                if not edges:  # No edge exists (shouldn't happen given how path was found)\n",
    "                    relation = 'unknown'\n",
    "                    weight = 0.0\n",
    "                else:\n",
    "                    # Get the highest weighted edge if multiple\n",
    "                    best_key = max(edges.keys(), key=lambda k: edges[k].get('weight', 0))\n",
    "                    relation = edges[best_key].get('relation', 'unknown')\n",
    "                    weight = edges[best_key].get('weight', 0.0)\n",
    "                \n",
    "                full_path.append((path[i], relation, path[i+1], weight))\n",
    "            \n",
    "            return full_path\n",
    "            \n",
    "        except nx.NetworkXNoPath:\n",
    "            return f\"No path found between {source} and {target} with given constraints.\"\n",
    "    \n",
    "    def get_top_concepts(self, n=20, measure='degree'):\n",
    "        \"\"\"\n",
    "        Get the top N concepts by a centrality measure\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            Number of concepts to return\n",
    "        measure : str\n",
    "            Centrality measure ('degree', 'in_degree', 'out_degree', 'pagerank')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with concepts and their centrality scores\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import networkx as nx\n",
    "        \n",
    "        if measure == 'degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.degree()}\n",
    "        elif measure == 'in_degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.in_degree()}\n",
    "        elif measure == 'out_degree':\n",
    "            centrality = {node: degree for node, degree in self.graph.out_degree()}\n",
    "        elif measure == 'pagerank':\n",
    "            # Compute pagerank (can be slow for large graphs)\n",
    "            print(\"Computing PageRank (this may take a while)...\")\n",
    "            centrality = nx.pagerank(self.graph, alpha=0.85, weight='weight')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown centrality measure: {measure}\")\n",
    "        \n",
    "        # Convert to DataFrame and get top N\n",
    "        df = pd.DataFrame(centrality.items(), columns=['concept', 'score'])\n",
    "        df = df.nlargest(n, 'score')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def analyze_graph_stats(self):\n",
    "        \"\"\"\n",
    "        Compute basic statistics about the knowledge graph\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with graph statistics\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        import numpy as np\n",
    "        \n",
    "        stats = {\n",
    "            'nodes': self.graph.number_of_nodes(),\n",
    "            'edges': self.graph.number_of_edges(),\n",
    "            'density': nx.density(self.graph),\n",
    "            'active_nodes': sum(1 for node in self.graph.nodes() if self.graph.degree(node) > 0),\n",
    "            'isolated_nodes': sum(1 for node in self.graph.nodes() if self.graph.degree(node) == 0),\n",
    "            'avg_degree': np.mean([d for n, d in self.graph.degree()]),\n",
    "            'max_degree': max([d for n, d in self.graph.degree()]) if self.graph.number_of_nodes() > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Add weight statistics if available\n",
    "        weights = [data.get('weight', 0) for _, _, data in self.graph.edges(data=True)]\n",
    "        if weights:\n",
    "            stats.update({\n",
    "                'min_weight': min(weights),\n",
    "                'max_weight': max(weights),\n",
    "                'avg_weight': np.mean(weights),\n",
    "                'median_weight': np.median(weights)\n",
    "            })\n",
    "        \n",
    "        # Add relation type counts\n",
    "        relation_counts = {}\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            relation = data.get('relation', 'unknown')\n",
    "            relation_counts[relation] = relation_counts.get(relation, 0) + 1\n",
    "        \n",
    "        stats['relation_counts'] = relation_counts\n",
    "        stats['unique_relations'] = len(relation_counts)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def visualize_concept_network(self, central_concept, depth=1, min_weight=0.5, relation_filter=None, max_nodes=30):\n",
    "        \"\"\"\n",
    "        Visualize the network around a concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        central_concept : str\n",
    "            The central concept to visualize\n",
    "        depth : int\n",
    "            How many hops to include\n",
    "        min_weight : float\n",
    "            Minimum edge weight\n",
    "        relation_filter : list\n",
    "            Only include these relation types\n",
    "        max_nodes : int\n",
    "            Maximum number of nodes to include\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        networkx.Graph\n",
    "            A subgraph centered on the concept\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        \n",
    "        if central_concept not in self.graph:\n",
    "            return f\"Concept '{central_concept}' not found in the graph.\"\n",
    "        \n",
    "        # Start with the central concept\n",
    "        nodes_to_include = {central_concept}\n",
    "        \n",
    "        # BFS to add concepts up to the specified depth\n",
    "        current_depth = 0\n",
    "        frontier = {central_concept}\n",
    "        \n",
    "        while current_depth < depth and len(nodes_to_include) < max_nodes:\n",
    "            next_frontier = set()\n",
    "            \n",
    "            for node in frontier:\n",
    "                # Add outgoing edges\n",
    "                for _, target, data in self.graph.out_edges(node, data=True):\n",
    "                    if (len(nodes_to_include) < max_nodes and \n",
    "                        target not in nodes_to_include and\n",
    "                        data.get('weight', 0) >= min_weight and\n",
    "                        (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                        nodes_to_include.add(target)\n",
    "                        next_frontier.add(target)\n",
    "                \n",
    "                # Add incoming edges\n",
    "                for source, _, data in self.graph.in_edges(node, data=True):\n",
    "                    if (len(nodes_to_include) < max_nodes and\n",
    "                        source not in nodes_to_include and\n",
    "                        data.get('weight', 0) >= min_weight and\n",
    "                        (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                        nodes_to_include.add(source)\n",
    "                        next_frontier.add(source)\n",
    "            \n",
    "            frontier = next_frontier\n",
    "            current_depth += 1\n",
    "        \n",
    "        # Extract the subgraph\n",
    "        subgraph = self.graph.subgraph(nodes_to_include)\n",
    "        \n",
    "        # Filter edges by weight and relation\n",
    "        view = nx.MultiDiGraph()\n",
    "        view.add_nodes_from(subgraph.nodes(data=True))\n",
    "        \n",
    "        for u, v, key, data in subgraph.edges(keys=True, data=True):\n",
    "            if (data.get('weight', 0) >= min_weight and \n",
    "                (relation_filter is None or data.get('relation') in relation_filter)):\n",
    "                view.add_edge(u, v, key=key, **data)\n",
    "        \n",
    "        return view\n",
    "    \n",
    "    def search_by_pattern(self, pattern, relation_types=None, limit=20):\n",
    "        \"\"\"\n",
    "        Search the graph for concepts matching a pattern\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pattern : str\n",
    "            Text pattern to search for\n",
    "        relation_types : list\n",
    "            Limit search to edges with these relations\n",
    "        limit : int\n",
    "            Maximum results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of matching concepts\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        pattern = pattern.lower()\n",
    "        regex = re.compile(pattern)\n",
    "        \n",
    "        # Search for matching nodes\n",
    "        matches = []\n",
    "        for node in self.graph.nodes():\n",
    "            if regex.search(str(node).lower()):\n",
    "                matches.append(node)\n",
    "                if len(matches) >= limit:\n",
    "                    break\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def extract_taxonomy(self, root_concept, relation_types=['IsA'], max_depth=5):\n",
    "        \"\"\"\n",
    "        Extract a taxonomy (tree) starting from a root concept\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        root_concept : str\n",
    "            Starting concept\n",
    "        relation_types : list\n",
    "            Relations to consider for the taxonomy (default: IsA)\n",
    "        max_depth : int\n",
    "            Maximum depth to traverse\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Nested dictionary representing the taxonomy\n",
    "        \"\"\"\n",
    "        if root_concept not in self.graph:\n",
    "            return f\"Concept '{root_concept}' not found in the graph.\"\n",
    "        \n",
    "        def build_tree(concept, depth=0):\n",
    "            if depth >= max_depth:\n",
    "                return {}\n",
    "            \n",
    "            tree = {}\n",
    "            \n",
    "            # Get all outgoing edges with the specified relation types\n",
    "            for _, target, data in self.graph.out_edges(concept, data=True):\n",
    "                if data.get('relation') in relation_types:\n",
    "                    tree[target] = build_tree(target, depth + 1)\n",
    "            \n",
    "            return tree\n",
    "        \n",
    "        taxonomy = {root_concept: build_tree(root_concept)}\n",
    "        return taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the explorer with the trained knowledge graph\n",
    "# Uncomment and run after training\n",
    "\n",
    "# Load a trained graph\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# You can load either a GraphML file or a pickle file\n",
    "trained_graph_path = os.path.join('..', 'Data', 'Output', 'nx_semantic_final.graphml')\n",
    "G_trained = nx.read_graphml(trained_graph_path)\n",
    "\n",
    "# Initialize the explorer\n",
    "explorer = KnowledgeGraphExplorer(G_trained)\n",
    "\n",
    "# Get basic graph stats\n",
    "stats = explorer.analyze_graph_stats()\n",
    "print(\"Graph Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if key != 'relation_counts':  # Skip printing the full relation counts\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Explore a concept\n",
    "print(\"\\nExploring 'dog':\")\n",
    "dog_relations = explorer.explore_concept('dog', min_weight=1.5, limit=10)\n",
    "print(dog_relations)\n",
    "\n",
    "# Find a path between concepts\n",
    "print(\"\\nFinding path from 'dog' to 'computer':\")\n",
    "path = explorer.find_path('dog', 'computer', max_length=3)\n",
    "print(path)\n",
    "\n",
    "# Get top concepts\n",
    "print(\"\\nTop concepts by degree:\")\n",
    "top_concepts = explorer.get_top_concepts(10, 'degree')\n",
    "print(top_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70916741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
